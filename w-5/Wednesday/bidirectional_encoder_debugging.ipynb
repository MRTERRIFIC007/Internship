{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Bidirectional Encoder Implementation and Debugging\n",
        "\n",
        "## Learning Objectives\n",
        "- Implement bidirectional LSTM encoders\n",
        "- Understand forward and backward processing\n",
        "- Debug bidirectional encoder errors and issues\n",
        "- Compare unidirectional vs bidirectional performance\n",
        "- Handle concatenation and dimension management\n",
        "\n",
        "## Bidirectional Encoding Benefits\n",
        "- **Forward Context**: Information from left to right\n",
        "- **Backward Context**: Information from right to left  \n",
        "- **Rich Representations**: Combined forward/backward hidden states\n",
        "- **Better Translation**: Improved understanding of sentence structure\n",
        "\n",
        "## Common Issues to Debug\n",
        "- **Dimension Mismatch**: Concatenating forward/backward states\n",
        "- **Memory Usage**: Double the parameters and computation\n",
        "- **Training Dynamics**: Different convergence patterns\n",
        "- **Attention Alignment**: How bidirectional affects attention patterns\n",
        "\n",
        "This notebook explores bidirectional encoders and solves common implementation challenges.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Ready to implement and debug bidirectional encoders!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Understanding Bidirectional Processing\n",
        "\n",
        "Let's first understand how bidirectional LSTMs work and why they're beneficial for translation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def demonstrate_bidirectional_concept():\n",
        "    \"\"\"Demonstrate the concept of bidirectional processing\"\"\"\n",
        "    \n",
        "    sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "    words = sentence.split()\n",
        "    \n",
        "    print(\"BIDIRECTIONAL PROCESSING DEMONSTRATION\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Words: {words}\")\n",
        "    print()\n",
        "    \n",
        "    # Forward processing\n",
        "    print(\"FORWARD PROCESSING (Left to Right):\")\n",
        "    print(\"-\" * 35)\n",
        "    forward_context = []\n",
        "    for i, word in enumerate(words):\n",
        "        context = \" \".join(words[:i+1])\n",
        "        forward_context.append(context)\n",
        "        print(f\"  Step {i+1}: '{word}' sees: '{context}'\")\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    # Backward processing  \n",
        "    print(\"BACKWARD PROCESSING (Right to Left):\")\n",
        "    print(\"-\" * 36)\n",
        "    backward_context = []\n",
        "    for i, word in enumerate(reversed(words)):\n",
        "        context = \" \".join(reversed(words[len(words)-i-1:]))\n",
        "        backward_context.append(context)\n",
        "        print(f\"  Step {i+1}: '{word}' sees: '{context}'\")\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    # Combined processing\n",
        "    print(\"BIDIRECTIONAL PROCESSING (Combined):\")\n",
        "    print(\"-\" * 37)\n",
        "    backward_context.reverse()  # Align with forward order\n",
        "    \n",
        "    for i, word in enumerate(words):\n",
        "        forward_ctx = forward_context[i]\n",
        "        backward_ctx = backward_context[i]\n",
        "        print(f\"  '{word}':\")\n",
        "        print(f\"    Forward context:  '{forward_ctx}'\")\n",
        "        print(f\"    Backward context: '{backward_ctx}'\")\n",
        "        print(f\"    Combined: Sees both past AND future words\")\n",
        "        print()\n",
        "    \n",
        "    # Benefits explanation\n",
        "    print(\"BENEFITS OF BIDIRECTIONAL PROCESSING:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(\"✅ Each word has access to full sentence context\")\n",
        "    print(\"✅ Better understanding of sentence structure\")\n",
        "    print(\"✅ Improved disambiguation (e.g., 'bank' of river vs money)\")\n",
        "    print(\"✅ Better translation quality, especially for complex sentences\")\n",
        "    print(\"✅ Attention can focus on more informed representations\")\n",
        "    \n",
        "    return forward_context, backward_context\n",
        "\n",
        "# Run the demonstration\n",
        "forward_ctx, backward_ctx = demonstrate_bidirectional_concept()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
