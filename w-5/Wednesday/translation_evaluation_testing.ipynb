{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Translation Evaluation and Testing\n",
        "\n",
        "## Learning Objectives\n",
        "- Evaluate trained translation models on test data\n",
        "- Implement BLEU score calculation\n",
        "- Handle out-of-vocabulary words and unknown phrases\n",
        "- Analyze translation quality and attention patterns\n",
        "- Compare model performance with and without attention\n",
        "\n",
        "## Evaluation Metrics\n",
        "- **BLEU Score**: Standard metric for translation quality\n",
        "- **Perplexity**: Language modeling evaluation\n",
        "- **Attention Visualization**: Understanding model focus\n",
        "- **Manual Evaluation**: Human assessment of translations\n",
        "\n",
        "## Testing Scenarios\n",
        "- **In-vocabulary**: Words seen during training\n",
        "- **Out-of-vocabulary**: Unknown words and phrases\n",
        "- **Long sentences**: Testing attention on complex inputs\n",
        "- **Domain transfer**: Different types of text\n",
        "\n",
        "This notebook focuses on comprehensive evaluation of our attention-based translation system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Ready to evaluate translation models!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## BLEU Score Implementation\n",
        "\n",
        "Let's implement BLEU score calculation to evaluate translation quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_bleu_score(reference, candidate):\n",
        "    \"\"\"\n",
        "    Calculate BLEU score for a single translation\n",
        "    \n",
        "    Args:\n",
        "        reference: List of reference words\n",
        "        candidate: List of candidate words\n",
        "    \n",
        "    Returns:\n",
        "        BLEU score (0-1)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use smoothing to handle short sentences\n",
        "        smoothing = SmoothingFunction().method1\n",
        "        score = sentence_bleu([reference], candidate, smoothing_function=smoothing)\n",
        "        return score\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def evaluate_translations(translations):\n",
        "    \"\"\"\n",
        "    Evaluate a list of translation pairs\n",
        "    \n",
        "    Args:\n",
        "        translations: List of (reference, candidate, attention_weights) tuples\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with evaluation metrics\n",
        "    \"\"\"\n",
        "    bleu_scores = []\n",
        "    total_length_diff = 0\n",
        "    perfect_matches = 0\n",
        "    \n",
        "    print(\"TRANSLATION EVALUATION RESULTS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    for i, (reference, candidate, attention) in enumerate(translations):\n",
        "        # Calculate BLEU score\n",
        "        ref_words = reference.split()\n",
        "        cand_words = candidate.split()\n",
        "        \n",
        "        bleu = calculate_bleu_score(ref_words, cand_words)\n",
        "        bleu_scores.append(bleu)\n",
        "        \n",
        "        # Calculate length difference\n",
        "        length_diff = abs(len(ref_words) - len(cand_words))\n",
        "        total_length_diff += length_diff\n",
        "        \n",
        "        # Check for perfect matches\n",
        "        if reference.lower() == candidate.lower():\n",
        "            perfect_matches += 1\n",
        "        \n",
        "        # Print individual results\n",
        "        print(f\"\\nTranslation {i+1}:\")\n",
        "        print(f\"  Reference: {reference}\")\n",
        "        print(f\"  Candidate: {candidate}\")\n",
        "        print(f\"  BLEU Score: {bleu:.3f}\")\n",
        "        print(f\"  Length Diff: {length_diff} words\")\n",
        "        \n",
        "        if bleu > 0.8:\n",
        "            print(\"  Quality: Excellent ‚úÖ\")\n",
        "        elif bleu > 0.5:\n",
        "            print(\"  Quality: Good üëç\")\n",
        "        elif bleu > 0.3:\n",
        "            print(\"  Quality: Fair üòê\")\n",
        "        else:\n",
        "            print(\"  Quality: Poor ‚ùå\")\n",
        "    \n",
        "    # Calculate overall metrics\n",
        "    avg_bleu = np.mean(bleu_scores)\n",
        "    avg_length_diff = total_length_diff / len(translations)\n",
        "    perfect_rate = perfect_matches / len(translations)\n",
        "    \n",
        "    print(f\"\\nOVERALL EVALUATION METRICS:\")\n",
        "    print(f\"  Average BLEU Score: {avg_bleu:.3f}\")\n",
        "    print(f\"  Average Length Difference: {avg_length_diff:.1f} words\")\n",
        "    print(f\"  Perfect Match Rate: {perfect_rate:.1%}\")\n",
        "    print(f\"  Total Translations: {len(translations)}\")\n",
        "    \n",
        "    return {\n",
        "        'avg_bleu': avg_bleu,\n",
        "        'bleu_scores': bleu_scores,\n",
        "        'avg_length_diff': avg_length_diff,\n",
        "        'perfect_rate': perfect_rate\n",
        "    }\n",
        "\n",
        "# Test with sample translations\n",
        "sample_translations = [\n",
        "    (\"Hello\", \"Bonjour\", None),\n",
        "    (\"How are you\", \"Comment allez-vous\", None),\n",
        "    (\"I love you\", \"Je t'aime\", None),\n",
        "    (\"The cat is sleeping\", \"Le chat dort\", None),\n",
        "    (\"What time is it\", \"Quelle heure est-il\", None),\n",
        "    (\"Good morning my friend\", \"Bonjour mon ami\", None),\n",
        "    (\"I am learning French\", \"J'apprends le fran√ßais\", None),\n",
        "    (\"The weather is beautiful today\", \"Il fait beau aujourd'hui\", None),\n",
        "]\n",
        "\n",
        "# Evaluate the sample translations\n",
        "evaluation_results = evaluate_translations(sample_translations)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
