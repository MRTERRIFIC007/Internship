{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Training Translation Models with Attention\n",
        "\n",
        "## Learning Objectives\n",
        "- Implement complete encoder-decoder with attention for translation\n",
        "- Train the model on English-French translation pairs\n",
        "- Handle convergence issues and training dynamics\n",
        "- Monitor attention patterns during training\n",
        "- Debug memory usage and computational efficiency\n",
        "\n",
        "## Model Architecture\n",
        "Today we'll build and train a complete translation system:\n",
        "- **Encoder**: Bidirectional LSTM that processes source sentences\n",
        "- **Attention**: Scaled dot-product attention mechanism\n",
        "- **Decoder**: LSTM with attention that generates target sentences\n",
        "- **Training**: Teacher forcing with attention visualization\n",
        "\n",
        "## Training Challenges\n",
        "- **Memory Usage**: Attention requires O(n²) memory\n",
        "- **Convergence**: Different dynamics compared to basic seq2seq\n",
        "- **Attention Patterns**: Monitoring alignment quality\n",
        "- **Hyperparameter Tuning**: Learning rates, attention dimensions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Ready to train attention-based translation models!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Training Translation Models with Attention\n",
        "\n",
        "## Learning Objectives\n",
        "- Implement complete attention-based translation model\n",
        "- Learn training procedures for sequence-to-sequence with attention\n",
        "- Handle convergence issues and memory management\n",
        "- Monitor training progress and attention patterns\n",
        "- Compare with and without attention mechanisms\n",
        "\n",
        "## Training Components\n",
        "- **Encoder-Decoder with Attention**: Complete translation architecture\n",
        "- **Training Loop**: Teacher forcing with attention mechanism\n",
        "- **Loss Computation**: Masked cross-entropy for variable lengths\n",
        "- **Optimization**: Learning rate scheduling and gradient clipping\n",
        "- **Monitoring**: Loss curves, attention visualization, sample translations\n",
        "\n",
        "## Key Training Challenges\n",
        "- Memory usage scaling with attention (O(n²))\n",
        "- Gradient flow through attention layers\n",
        "- Balancing encoder and decoder learning rates\n",
        "- Preventing attention collapse or diffusion\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
