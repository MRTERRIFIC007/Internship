{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Translation Dataset Preparation\n",
        "\n",
        "## Learning Objectives\n",
        "- Prepare English-French translation datasets\n",
        "- Create vocabulary mappings for source and target languages\n",
        "- Handle sentence tokenization and preprocessing\n",
        "- Implement data loading for variable-length sequences\n",
        "- Create training batches with proper padding\n",
        "\n",
        "## Dataset Overview\n",
        "Today we'll work with English-French sentence pairs to build a complete translation system. We'll focus on:\n",
        "- **Data Loading**: Reading parallel translation corpora\n",
        "- **Preprocessing**: Tokenization, normalization, vocabulary creation\n",
        "- **Batching**: Handling variable-length sequences efficiently\n",
        "- **Evaluation Setup**: Preparing test sets for BLEU scoring\n",
        "\n",
        "This notebook sets up the foundation for our attention-based translation system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "import random\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Ready to prepare translation datasets!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 1: Creating Translation Data\n",
        "\n",
        "Let's start by creating a sample English-French translation dataset for our experiments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample English-French translation pairs\n",
        "def create_translation_data():\n",
        "    \"\"\"Create a sample dataset of English-French translation pairs\"\"\"\n",
        "    \n",
        "    translation_pairs = [\n",
        "        # Basic greetings and common phrases\n",
        "        (\"Hello\", \"Bonjour\"),\n",
        "        (\"Good morning\", \"Bonjour\"),\n",
        "        (\"Good evening\", \"Bonsoir\"),\n",
        "        (\"How are you\", \"Comment allez-vous\"),\n",
        "        (\"I am fine\", \"Je vais bien\"),\n",
        "        (\"Thank you\", \"Merci\"),\n",
        "        (\"You are welcome\", \"De rien\"),\n",
        "        (\"Goodbye\", \"Au revoir\"),\n",
        "        \n",
        "        # Simple sentences\n",
        "        (\"I love you\", \"Je t'aime\"),\n",
        "        (\"What is your name\", \"Comment vous appelez-vous\"),\n",
        "        (\"My name is John\", \"Je m'appelle John\"),\n",
        "        (\"Where are you from\", \"D'où venez-vous\"),\n",
        "        (\"I am from France\", \"Je viens de France\"),\n",
        "        (\"How old are you\", \"Quel âge avez-vous\"),\n",
        "        (\"I am twenty years old\", \"J'ai vingt ans\"),\n",
        "        \n",
        "        # More complex sentences\n",
        "        (\"The weather is nice today\", \"Il fait beau aujourd'hui\"),\n",
        "        (\"I would like to eat something\", \"Je voudrais manger quelque chose\"),\n",
        "        (\"Can you help me please\", \"Pouvez-vous m'aider s'il vous plaît\"),\n",
        "        (\"Where is the train station\", \"Où est la gare\"),\n",
        "        (\"I don't understand French\", \"Je ne comprends pas le français\"),\n",
        "        (\"Do you speak English\", \"Parlez-vous anglais\"),\n",
        "        \n",
        "        # Longer sentences for testing attention\n",
        "        (\"The quick brown fox jumps over the lazy dog\", \"Le renard brun rapide saute par-dessus le chien paresseux\"),\n",
        "        (\"I am learning machine learning and artificial intelligence\", \"J'apprends l'apprentissage automatique et l'intelligence artificielle\"),\n",
        "        (\"The attention mechanism revolutionized natural language processing\", \"Le mécanisme d'attention a révolutionné le traitement du langage naturel\"),\n",
        "        (\"Neural networks can learn complex patterns in data\", \"Les réseaux de neurones peuvent apprendre des motifs complexes dans les données\"),\n",
        "        \n",
        "        # Questions and responses\n",
        "        (\"What time is it\", \"Quelle heure est-il\"),\n",
        "        (\"It is three o'clock\", \"Il est trois heures\"),\n",
        "        (\"What day is today\", \"Quel jour sommes-nous\"),\n",
        "        (\"Today is Monday\", \"Aujourd'hui c'est lundi\"),\n",
        "        (\"What is the weather like\", \"Quel temps fait-il\"),\n",
        "        (\"It is raining\", \"Il pleut\"),\n",
        "        \n",
        "        # Travel and directions\n",
        "        (\"I need a taxi\", \"J'ai besoin d'un taxi\"),\n",
        "        (\"How much does it cost\", \"Combien ça coûte\"),\n",
        "        (\"Turn left at the corner\", \"Tournez à gauche au coin\"),\n",
        "        (\"Go straight ahead\", \"Allez tout droit\"),\n",
        "        (\"The hotel is near the park\", \"L'hôtel est près du parc\"),\n",
        "        \n",
        "        # Food and dining\n",
        "        (\"I am hungry\", \"J'ai faim\"),\n",
        "        (\"I am thirsty\", \"J'ai soif\"),\n",
        "        (\"The food is delicious\", \"La nourriture est délicieuse\"),\n",
        "        (\"Can I have the menu please\", \"Puis-je avoir le menu s'il vous plaît\"),\n",
        "        (\"I would like a coffee\", \"Je voudrais un café\"),\n",
        "        \n",
        "        # Numbers and time\n",
        "        (\"One two three four five\", \"Un deux trois quatre cinq\"),\n",
        "        (\"Six seven eight nine ten\", \"Six sept huit neuf dix\"),\n",
        "        (\"What is your phone number\", \"Quel est votre numéro de téléphone\"),\n",
        "        (\"My phone number is five five five\", \"Mon numéro de téléphone est cinq cinq cinq\"),\n",
        "    ]\n",
        "    \n",
        "    return translation_pairs\n",
        "\n",
        "# Load and analyze the dataset\n",
        "translation_data = create_translation_data()\n",
        "\n",
        "print(\"TRANSLATION DATASET OVERVIEW\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Total translation pairs: {len(translation_data)}\")\n",
        "print()\n",
        "\n",
        "# Analyze sentence lengths\n",
        "en_lengths = [len(pair[0].split()) for pair in translation_data]\n",
        "fr_lengths = [len(pair[1].split()) for pair in translation_data]\n",
        "\n",
        "print(f\"English sentence statistics:\")\n",
        "print(f\"  Average length: {np.mean(en_lengths):.1f} words\")\n",
        "print(f\"  Min length: {min(en_lengths)} words\")\n",
        "print(f\"  Max length: {max(en_lengths)} words\")\n",
        "print()\n",
        "\n",
        "print(f\"French sentence statistics:\")\n",
        "print(f\"  Average length: {np.mean(fr_lengths):.1f} words\")\n",
        "print(f\"  Min length: {min(fr_lengths)} words\")\n",
        "print(f\"  Max length: {max(fr_lengths)} words\")\n",
        "print()\n",
        "\n",
        "# Show some examples\n",
        "print(\"Sample translation pairs:\")\n",
        "for i, (en, fr) in enumerate(translation_data[:10]):\n",
        "    print(f\"  {i+1:2d}. EN: {en}\")\n",
        "    print(f\"      FR: {fr}\")\n",
        "    print()\n",
        "\n",
        "# Visualize length distributions\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "ax1.hist(en_lengths, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "ax1.set_xlabel('Sentence Length (words)')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.set_title('English Sentence Length Distribution')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.hist(fr_lengths, bins=10, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "ax2.set_xlabel('Sentence Length (words)')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.set_title('French Sentence Length Distribution')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Translation Dataset Preparation\n",
        "\n",
        "## Learning Objectives\n",
        "- Learn how to prepare parallel translation datasets\n",
        "- Understand text preprocessing for machine translation\n",
        "- Implement vocabulary building for source and target languages\n",
        "- Handle variable-length sequences and padding\n",
        "- Create efficient data loaders for translation training\n",
        "\n",
        "## Dataset Components\n",
        "- **English-French Sentence Pairs**: Parallel corpus for translation\n",
        "- **Vocabulary Creation**: Token-to-index mapping for both languages\n",
        "- **Text Preprocessing**: Cleaning, tokenization, and normalization\n",
        "- **Sequence Padding**: Handling variable-length sequences efficiently\n",
        "\n",
        "## Data Pipeline\n",
        "1. Load and clean parallel text data\n",
        "2. Tokenize source and target sentences\n",
        "3. Build vocabularies with special tokens\n",
        "4. Convert text to numerical sequences\n",
        "5. Create batched data loaders with padding\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
