{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Transformer Attention Mechanism Implementation\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand core Transformer attention components\n",
        "- Implement multi-head attention from scratch\n",
        "- Add positional encoding to handle sequence order\n",
        "- Build complete Transformer attention layers\n",
        "- Compare with previous attention mechanisms\n",
        "\n",
        "## Transformer Innovations\n",
        "- **Multi-Head Attention**: Multiple attention heads in parallel\n",
        "- **Positional Encoding**: Inject position information into embeddings\n",
        "- **Layer Normalization**: Stabilize training\n",
        "- **Residual Connections**: Enable deeper networks\n",
        "\n",
        "## Core Components\n",
        "1. **Positional Encoding**: Sin/Cos encoding for position awareness\n",
        "2. **Multi-Head Attention**: Parallel attention heads\n",
        "3. **Feed-Forward Networks**: Position-wise transformations\n",
        "4. **Layer Normalization**: Normalize activations\n",
        "\n",
        "This notebook implements the attention mechanism that powers modern Transformers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Ready to implement Transformer attention components!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Positional Encoding Implementation\n",
        "\n",
        "Transformers need positional encoding since attention has no inherent notion of sequence order.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding using sin/cos functions\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        \n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        \n",
        "        # Create division term for frequencies\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
        "                           (-math.log(10000.0) / d_model))\n",
        "        \n",
        "        # Apply sin to even indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        \n",
        "        # Apply cos to odd indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        # Add batch dimension and register as buffer\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"Add positional encoding to input embeddings\"\"\"\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "def visualize_positional_encoding():\n",
        "    \"\"\"Visualize the positional encoding patterns\"\"\"\n",
        "    \n",
        "    d_model = 128\n",
        "    max_len = 100\n",
        "    \n",
        "    # Create positional encoding\n",
        "    pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "    \n",
        "    # Get the encoding matrix\n",
        "    pe_matrix = pos_encoding.pe.squeeze().numpy()\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10))\n",
        "    \n",
        "    # Plot full positional encoding heatmap\n",
        "    im1 = ax1.imshow(pe_matrix.T, cmap='RdBu', aspect='auto')\n",
        "    ax1.set_title('Positional Encoding Matrix\\n(Position vs Embedding Dimension)')\n",
        "    ax1.set_xlabel('Position')\n",
        "    ax1.set_ylabel('Embedding Dimension')\n",
        "    plt.colorbar(im1, ax=ax1, shrink=0.8)\n",
        "    \n",
        "    # Plot specific dimensions over positions\n",
        "    dimensions_to_plot = [0, 1, 4, 5, 8, 9]\n",
        "    for dim in dimensions_to_plot:\n",
        "        ax2.plot(pe_matrix[:50, dim], label=f'Dim {dim}')\n",
        "    ax2.set_title('Positional Encoding for Different Dimensions')\n",
        "    ax2.set_xlabel('Position')\n",
        "    ax2.set_ylabel('Encoding Value')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot specific positions across dimensions\n",
        "    positions_to_plot = [0, 5, 10, 20, 30]\n",
        "    for pos in positions_to_plot:\n",
        "        ax3.plot(pe_matrix[pos, :30], label=f'Pos {pos}')\n",
        "    ax3.set_title('Encoding Patterns for Different Positions')\n",
        "    ax3.set_xlabel('Embedding Dimension')\n",
        "    ax3.set_ylabel('Encoding Value')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"POSITIONAL ENCODING PROPERTIES:\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"✅ Each position has a unique encoding pattern\")\n",
        "    print(\"✅ Sine/cosine allows model to learn relative positions\")\n",
        "    print(\"✅ Different frequencies for different dimensions\")\n",
        "    print(\"✅ Smooth patterns enable generalization to longer sequences\")\n",
        "    \n",
        "    return pe_matrix\n",
        "\n",
        "# Visualize positional encoding\n",
        "pe_matrix = visualize_positional_encoding()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
