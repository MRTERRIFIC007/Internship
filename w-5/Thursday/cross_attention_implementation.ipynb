{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Cross-Attention Implementation\n",
        "\n",
        "## Learning Objectives\n",
        "- Implement cross-attention layers from scratch\n",
        "- Build input and output gates for attention control\n",
        "- Create cross-attention models for translation\n",
        "- Test cross-attention on translation tasks\n",
        "\n",
        "This notebook provides hands-on implementation of cross-attention mechanisms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "print(\"Ready to implement cross-attention!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Cross-Attention Layer Implementation\n",
        "\n",
        "Let's implement a cross-attention layer that can handle encoder-decoder communication.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"Cross-attention layer for encoder-decoder communication\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, num_heads=8):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        \n",
        "        # Linear projections for Q, K, V\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        \"\"\"Scaled dot-product attention computation\"\"\"\n",
        "        \n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        \n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        \n",
        "        # Apply softmax\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        return output, attention_weights\n",
        "    \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Cross-attention forward pass\n",
        "        \n",
        "        Args:\n",
        "            query: [batch_size, tgt_len, d_model] - from decoder\n",
        "            key: [batch_size, src_len, d_model] - from encoder\n",
        "            value: [batch_size, src_len, d_model] - from encoder\n",
        "            mask: [batch_size, tgt_len, src_len] - attention mask\n",
        "        \n",
        "        Returns:\n",
        "            output: [batch_size, tgt_len, d_model]\n",
        "            attention_weights: [batch_size, num_heads, tgt_len, src_len]\n",
        "        \"\"\"\n",
        "        batch_size, tgt_len, _ = query.size()\n",
        "        src_len = key.size(1)\n",
        "        \n",
        "        # Linear projections\n",
        "        Q = self.W_q(query)  # [batch_size, tgt_len, d_model]\n",
        "        K = self.W_k(key)    # [batch_size, src_len, d_model]\n",
        "        V = self.W_v(value)  # [batch_size, src_len, d_model]\n",
        "        \n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.view(batch_size, tgt_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, src_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, src_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        \n",
        "        # Apply attention\n",
        "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        \n",
        "        # Concatenate heads\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
        "            batch_size, tgt_len, self.d_model)\n",
        "        \n",
        "        # Final linear projection\n",
        "        output = self.W_o(attention_output)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "# Test cross-attention layer\n",
        "def test_cross_attention():\n",
        "    \"\"\"Test the cross-attention implementation\"\"\"\n",
        "    \n",
        "    batch_size = 2\n",
        "    src_len = 6  # encoder sequence length\n",
        "    tgt_len = 4  # decoder sequence length  \n",
        "    d_model = 128\n",
        "    num_heads = 8\n",
        "    \n",
        "    # Create test tensors\n",
        "    encoder_output = torch.randn(batch_size, src_len, d_model)  # Keys and Values\n",
        "    decoder_input = torch.randn(batch_size, tgt_len, d_model)   # Queries\n",
        "    \n",
        "    # Initialize cross-attention layer\n",
        "    cross_attn = CrossAttention(d_model, num_heads)\n",
        "    \n",
        "    # Forward pass\n",
        "    output, attention_weights = cross_attn(decoder_input, encoder_output, encoder_output)\n",
        "    \n",
        "    print(\"CROSS-ATTENTION TEST\")\n",
        "    print(\"=\" * 30)\n",
        "    print(f\"Encoder output (K,V): {encoder_output.shape}\")\n",
        "    print(f\"Decoder input (Q): {decoder_input.shape}\")\n",
        "    print(f\"Cross-attention output: {output.shape}\")\n",
        "    print(f\"Attention weights: {attention_weights.shape}\")\n",
        "    print()\n",
        "    \n",
        "    # Check attention weight properties\n",
        "    attn_sum = attention_weights.sum(dim=-1)\n",
        "    print(f\"Attention weights sum (should be ~1.0):\")\n",
        "    print(f\"  Min: {attn_sum.min().item():.6f}\")\n",
        "    print(f\"  Max: {attn_sum.max().item():.6f}\")\n",
        "    print(f\"  Mean: {attn_sum.mean().item():.6f}\")\n",
        "    \n",
        "    return output, attention_weights, cross_attn\n",
        "\n",
        "# Run the test\n",
        "output, attn_weights, cross_attn_layer = test_cross_attention()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Cross-Attention Implementation\n",
        "\n",
        "## Learning Objectives\n",
        "- Implement cross-attention layers from scratch\n",
        "- Build input and output gates for attention control\n",
        "- Create cross-attention models for translation\n",
        "- Test cross-attention on translation tasks\n",
        "- Debug common cross-attention issues\n",
        "\n",
        "## Implementation Components\n",
        "- **Cross-Attention Layer**: Core cross-attention computation\n",
        "- **Input Gate**: Control information flow from encoder\n",
        "- **Output Gate**: Control attention output integration\n",
        "- **Cross-Attention Model**: Complete encoder-decoder with cross-attention\n",
        "\n",
        "## Key Differences from Basic Attention\n",
        "- Queries come from decoder states\n",
        "- Keys and Values come from encoder outputs\n",
        "- Enables true encoder-decoder communication\n",
        "- Foundation for Transformer decoder layers\n",
        "\n",
        "This notebook provides hands-on implementation of cross-attention mechanisms.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
