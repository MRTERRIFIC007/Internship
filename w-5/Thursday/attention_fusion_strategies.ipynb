{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Shallow vs Deep Attention Fusion\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand different attention fusion strategies\n",
        "- Implement Shallow Attention Module (SAM)\n",
        "- Compare shallow vs deep attention approaches\n",
        "- Analyze multi-level feature learning with attention\n",
        "- Optimize attention computation for efficiency\n",
        "\n",
        "## Attention Fusion Approaches\n",
        "- **Shallow Fusion**: Single attention layer\n",
        "- **Deep Fusion**: Multiple attention layers\n",
        "- **Hierarchical Attention**: Multi-level attention\n",
        "- **Parallel Attention**: Multiple attention heads\n",
        "\n",
        "## Shallow Attention Module (SAM)\n",
        "- Lightweight attention computation\n",
        "- Efficient for resource-constrained scenarios\n",
        "- Single-layer attention with optimized weights\n",
        "- Good balance between performance and efficiency\n",
        "\n",
        "This notebook explores different strategies for combining and organizing attention mechanisms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "print(\"Ready to explore attention fusion strategies!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Shallow Attention Module Implementation\n",
        "\n",
        "Let's implement a lightweight attention module for efficient computation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ShallowAttentionModule(nn.Module):\n",
        "    \"\"\"Shallow Attention Module (SAM) for efficient attention computation\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, reduction_factor=4):\n",
        "        super(ShallowAttentionModule, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_reduced = d_model // reduction_factor\n",
        "        \n",
        "        # Reduced dimension projections for efficiency\n",
        "        self.W_q = nn.Linear(d_model, self.d_reduced)\n",
        "        self.W_k = nn.Linear(d_model, self.d_reduced)\n",
        "        self.W_v = nn.Linear(d_model, self.d_reduced)\n",
        "        self.W_o = nn.Linear(self.d_reduced, d_model)\n",
        "        \n",
        "        # Layer normalization for stability\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "        \n",
        "    def forward(self, query, key, value):\n",
        "        \"\"\"\n",
        "        Shallow attention forward pass\n",
        "        \n",
        "        Args:\n",
        "            query: [batch_size, tgt_len, d_model]\n",
        "            key: [batch_size, src_len, d_model]\n",
        "            value: [batch_size, src_len, d_model]\n",
        "        \n",
        "        Returns:\n",
        "            output: [batch_size, tgt_len, d_model]\n",
        "            attention_weights: [batch_size, tgt_len, src_len]\n",
        "        \"\"\"\n",
        "        batch_size, tgt_len, _ = query.size()\n",
        "        src_len = key.size(1)\n",
        "        \n",
        "        # Project to reduced dimensions\n",
        "        Q = self.W_q(query)  # [batch_size, tgt_len, d_reduced]\n",
        "        K = self.W_k(key)    # [batch_size, src_len, d_reduced]\n",
        "        V = self.W_v(value)  # [batch_size, src_len, d_reduced]\n",
        "        \n",
        "        # Compute attention scores (simplified)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_reduced)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        attended = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        # Project back to original dimension\n",
        "        output = self.W_o(attended)\n",
        "        \n",
        "        # Residual connection and layer norm\n",
        "        output = self.layer_norm(output + query)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "class DeepAttentionModule(nn.Module):\n",
        "    \"\"\"Deep Attention Module with multiple attention layers\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, num_layers=3, num_heads=8):\n",
        "        super(DeepAttentionModule, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.num_heads = num_heads\n",
        "        \n",
        "        # Multiple attention layers\n",
        "        self.attention_layers = nn.ModuleList([\n",
        "            nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        # Layer normalizations\n",
        "        self.layer_norms = nn.ModuleList([\n",
        "            nn.LayerNorm(d_model) for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "    def forward(self, query, key, value):\n",
        "        \"\"\"Deep attention with multiple layers\"\"\"\n",
        "        \n",
        "        output = query\n",
        "        attention_weights_list = []\n",
        "        \n",
        "        for i, (attn_layer, layer_norm) in enumerate(zip(self.attention_layers, self.layer_norms)):\n",
        "            # Apply attention\n",
        "            attn_output, attn_weights = attn_layer(output, key, value)\n",
        "            attention_weights_list.append(attn_weights)\n",
        "            \n",
        "            # Residual connection and layer norm\n",
        "            output = layer_norm(attn_output + output)\n",
        "        \n",
        "        return output, attention_weights_list\n",
        "\n",
        "def compare_attention_efficiency():\n",
        "    \"\"\"Compare computational efficiency of different attention approaches\"\"\"\n",
        "    \n",
        "    batch_size = 8\n",
        "    seq_len = 128\n",
        "    d_model = 512\n",
        "    \n",
        "    # Create test data\n",
        "    query = torch.randn(batch_size, seq_len, d_model)\n",
        "    key = torch.randn(batch_size, seq_len, d_model)\n",
        "    value = torch.randn(batch_size, seq_len, d_model)\n",
        "    \n",
        "    # Initialize models\n",
        "    shallow_attn = ShallowAttentionModule(d_model, reduction_factor=4)\n",
        "    deep_attn = DeepAttentionModule(d_model, num_layers=3)\n",
        "    \n",
        "    print(\"ATTENTION EFFICIENCY COMPARISON\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Test shallow attention\n",
        "    start_time = time.time()\n",
        "    for _ in range(10):\n",
        "        shallow_output, shallow_weights = shallow_attn(query, key, value)\n",
        "    shallow_time = time.time() - start_time\n",
        "    \n",
        "    # Test deep attention\n",
        "    start_time = time.time()\n",
        "    for _ in range(10):\n",
        "        deep_output, deep_weights = deep_attn(query, key, value)\n",
        "    deep_time = time.time() - start_time\n",
        "    \n",
        "    # Calculate parameters\n",
        "    shallow_params = sum(p.numel() for p in shallow_attn.parameters())\n",
        "    deep_params = sum(p.numel() for p in deep_attn.parameters())\n",
        "    \n",
        "    print(f\"Shallow Attention:\")\n",
        "    print(f\"  Parameters: {shallow_params:,}\")\n",
        "    print(f\"  Time (10 iterations): {shallow_time:.4f}s\")\n",
        "    print(f\"  Output shape: {shallow_output.shape}\")\n",
        "    print()\n",
        "    \n",
        "    print(f\"Deep Attention:\")\n",
        "    print(f\"  Parameters: {deep_params:,}\")\n",
        "    print(f\"  Time (10 iterations): {deep_time:.4f}s\")\n",
        "    print(f\"  Output shape: {deep_output.shape}\")\n",
        "    print(f\"  Attention layers: {len(deep_weights)}\")\n",
        "    print()\n",
        "    \n",
        "    print(f\"Efficiency Comparison:\")\n",
        "    print(f\"  Parameter ratio (Deep/Shallow): {deep_params/shallow_params:.1f}x\")\n",
        "    print(f\"  Time ratio (Deep/Shallow): {deep_time/shallow_time:.1f}x\")\n",
        "    \n",
        "    return shallow_attn, deep_attn, shallow_output, deep_output\n",
        "\n",
        "# Run efficiency comparison\n",
        "shallow_model, deep_model, shallow_out, deep_out = compare_attention_efficiency()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
