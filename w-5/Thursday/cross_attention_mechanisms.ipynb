{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Cross-Attention Mechanisms in Transformers\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the difference between self-attention and cross-attention\n",
        "- Learn how cross-attention enables encoder-decoder communication\n",
        "- Implement cross-attention layers from scratch\n",
        "- Visualize cross-attention patterns in translation\n",
        "- Compare cross-attention with traditional attention mechanisms\n",
        "\n",
        "## Key Concepts\n",
        "- **Self-Attention**: Attention within the same sequence (Q, K, V from same source)\n",
        "- **Cross-Attention**: Attention between different sequences (Q from decoder, K,V from encoder)\n",
        "- **Multi-Head Attention**: Multiple attention heads working in parallel\n",
        "- **Attention Heads**: Different learned attention patterns\n",
        "\n",
        "## Cross-Attention vs Self-Attention\n",
        "- **Self-Attention**: \"How do words in this sentence relate to each other?\"\n",
        "- **Cross-Attention**: \"How should this output word attend to input words?\"\n",
        "\n",
        "This notebook explores the cross-attention mechanism that enables Transformer encoder-decoder communication.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Custom colormap for attention visualization\n",
        "colors = ['white', 'lightblue', 'blue', 'darkblue', 'navy']\n",
        "attention_cmap = LinearSegmentedColormap.from_list('attention', colors)\n",
        "\n",
        "print(\"Ready to explore cross-attention mechanisms!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Understanding Self-Attention vs Cross-Attention\n",
        "\n",
        "Let's first understand the conceptual difference between these two types of attention.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_attention_types():\n",
        "    \"\"\"Compare self-attention and cross-attention conceptually\"\"\"\n",
        "    \n",
        "    # Example sentences\n",
        "    english = \"The cat sits on the mat\"\n",
        "    french = \"Le chat est assis sur le tapis\"\n",
        "    \n",
        "    en_words = english.split()\n",
        "    fr_words = french.split()\n",
        "    \n",
        "    print(\"SELF-ATTENTION vs CROSS-ATTENTION COMPARISON\")\n",
        "    print(\"=\" * 55)\n",
        "    print(f\"English: {english}\")\n",
        "    print(f\"French:  {french}\")\n",
        "    print()\n",
        "    \n",
        "    # Self-attention explanation\n",
        "    print(\"ðŸ”„ SELF-ATTENTION (within same sequence)\")\n",
        "    print(\"-\" * 40)\n",
        "    print(\"English Self-Attention:\")\n",
        "    print(\"  - How does 'cat' relate to 'sits', 'mat', etc.?\")\n",
        "    print(\"  - Captures syntactic and semantic relationships\")\n",
        "    print(\"  - Q, K, V all come from English sentence\")\n",
        "    print()\n",
        "    \n",
        "    print(\"French Self-Attention:\")\n",
        "    print(\"  - How does 'chat' relate to 'assis', 'tapis', etc.?\")\n",
        "    print(\"  - Captures French grammar and word relationships\")\n",
        "    print(\"  - Q, K, V all come from French sentence\")\n",
        "    print()\n",
        "    \n",
        "    # Cross-attention explanation\n",
        "    print(\"ðŸ”€ CROSS-ATTENTION (between different sequences)\")\n",
        "    print(\"-\" * 45)\n",
        "    print(\"English-to-French Cross-Attention:\")\n",
        "    print(\"  - When generating 'chat', which English words to focus on?\")\n",
        "    print(\"  - Q comes from French (what we're generating)\")\n",
        "    print(\"  - K, V come from English (what we're translating from)\")\n",
        "    print(\"  - Enables translation alignment\")\n",
        "    print()\n",
        "    \n",
        "    # Create conceptual attention matrices\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # English self-attention (dummy data for visualization)\n",
        "    en_self_attention = np.random.rand(len(en_words), len(en_words))\n",
        "    en_self_attention = en_self_attention / en_self_attention.sum(axis=1, keepdims=True)\n",
        "    \n",
        "    im1 = ax1.imshow(en_self_attention, cmap=attention_cmap, aspect='auto')\n",
        "    ax1.set_title('English Self-Attention\\n(English words attending to English words)')\n",
        "    ax1.set_xticks(range(len(en_words)))\n",
        "    ax1.set_yticks(range(len(en_words)))\n",
        "    ax1.set_xticklabels(en_words, rotation=45)\n",
        "    ax1.set_yticklabels(en_words)\n",
        "    ax1.set_xlabel('Attended Words (Keys)')\n",
        "    ax1.set_ylabel('Attending Words (Queries)')\n",
        "    \n",
        "    # French self-attention\n",
        "    fr_self_attention = np.random.rand(len(fr_words), len(fr_words))\n",
        "    fr_self_attention = fr_self_attention / fr_self_attention.sum(axis=1, keepdims=True)\n",
        "    \n",
        "    im2 = ax2.imshow(fr_self_attention, cmap=attention_cmap, aspect='auto')\n",
        "    ax2.set_title('French Self-Attention\\n(French words attending to French words)')\n",
        "    ax2.set_xticks(range(len(fr_words)))\n",
        "    ax2.set_yticks(range(len(fr_words)))\n",
        "    ax2.set_xticklabels(fr_words, rotation=45)\n",
        "    ax2.set_yticklabels(fr_words)\n",
        "    ax2.set_xlabel('Attended Words (Keys)')\n",
        "    ax2.set_ylabel('Attending Words (Queries)')\n",
        "    \n",
        "    # Cross-attention (French attending to English)\n",
        "    cross_attention = np.random.rand(len(fr_words), len(en_words))\n",
        "    # Make it more realistic - similar words have higher attention\n",
        "    cross_attention[1, 1] = 0.8  # chat -> cat\n",
        "    cross_attention[0, 0] = 0.7  # Le -> The\n",
        "    cross_attention[2, 2] = 0.6  # est -> sits (approximate)\n",
        "    cross_attention = cross_attention / cross_attention.sum(axis=1, keepdims=True)\n",
        "    \n",
        "    im3 = ax3.imshow(cross_attention, cmap=attention_cmap, aspect='auto')\n",
        "    ax3.set_title('Cross-Attention\\n(French words attending to English words)')\n",
        "    ax3.set_xticks(range(len(en_words)))\n",
        "    ax3.set_yticks(range(len(fr_words)))\n",
        "    ax3.set_xticklabels(en_words, rotation=45)\n",
        "    ax3.set_yticklabels(fr_words)\n",
        "    ax3.set_xlabel('English Words (Keys/Values)')\n",
        "    ax3.set_ylabel('French Words (Queries)')\n",
        "    \n",
        "    # Information flow diagram\n",
        "    ax4.text(0.5, 0.8, 'INFORMATION FLOW', ha='center', va='center', \n",
        "             fontsize=16, fontweight='bold', transform=ax4.transAxes)\n",
        "    \n",
        "    ax4.text(0.1, 0.6, 'Self-Attention:\\nQ, K, V from\\nsame sequence', \n",
        "             ha='left', va='center', fontsize=12, transform=ax4.transAxes,\n",
        "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
        "    \n",
        "    ax4.text(0.1, 0.3, 'Cross-Attention:\\nQ from target\\nK, V from source', \n",
        "             ha='left', va='center', fontsize=12, transform=ax4.transAxes,\n",
        "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\"))\n",
        "    \n",
        "    ax4.set_xlim(0, 1)\n",
        "    ax4.set_ylim(0, 1)\n",
        "    ax4.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return en_self_attention, fr_self_attention, cross_attention\n",
        "\n",
        "# Run the comparison\n",
        "en_self, fr_self, cross = compare_attention_types()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Cross-Attention Mechanisms in Transformers\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the difference between self-attention and cross-attention\n",
        "- Learn how cross-attention enables encoder-decoder communication\n",
        "- Implement cross-attention layers from scratch\n",
        "- Understand query-key-value relationships across sequences\n",
        "- Explore applications in translation and other seq2seq tasks\n",
        "\n",
        "## Cross-Attention vs Self-Attention\n",
        "- **Self-Attention**: Attention within the same sequence (Q, K, V from same source)\n",
        "- **Cross-Attention**: Attention between different sequences (Q from decoder, K, V from encoder)\n",
        "- **Use Cases**: Machine translation, text summarization, question answering\n",
        "\n",
        "## Key Concepts\n",
        "- **Encoder-Decoder Attention**: How decoder attends to encoder representations\n",
        "- **Multi-Head Cross-Attention**: Parallel attention heads for different representation subspaces\n",
        "- **Masked Attention**: Preventing future information leakage in autoregressive generation\n",
        "\n",
        "This notebook explores the cross-attention mechanism that enables effective encoder-decoder communication in Transformers.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
