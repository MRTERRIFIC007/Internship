# Week 5 - Thursday: Cross-Attention and Transformer Components

## Overview

Today we explore cross-attention mechanisms and implement core Transformer attention components.

## Topics Covered

- Cross-attention vs self-attention
- Implementing cross-attention models
- Transformer attention mechanisms
- Shallow vs deep attention fusion

## Notebooks

1. cross_attention_mechanisms.ipynb
2. cross_attention_implementation.ipynb
3. transformer_attention_components.ipynb
4. attention_fusion_strategies.ipynb

## Key Concepts

- Cross-attention mechanism
- Multi-head attention
- Positional encoding
- Attention fusion methods
- Shallow vs deep attention
