{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Understanding Attention Mechanisms: Intuition and Motivation\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand why basic encoder-decoder models fail on long sequences\n",
        "- Learn the human-inspired intuition behind attention\n",
        "- Visualize the information bottleneck problem\n",
        "- Understand attention as a solution to information compression\n",
        "\n",
        "## The Core Problem\n",
        "Yesterday we learned about encoder-decoder models and discovered their fundamental limitation: the **information bottleneck**. Today we explore the elegant solution that revolutionized sequence-to-sequence learning.\n",
        "\n",
        "## Key Questions We'll Answer\n",
        "1. Why do encoder-decoder models struggle with long sequences?\n",
        "2. How do human translators handle long sentences?\n",
        "3. What is attention and how does it work?\n",
        "4. How can we visualize attention mechanisms?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import random\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Enhanced plotting setup\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Custom colormap for attention visualization\n",
        "colors = ['white', 'lightblue', 'blue', 'darkblue']\n",
        "attention_cmap = LinearSegmentedColormap.from_list('attention', colors)\n",
        "\n",
        "print(\"Ready to explore attention mechanisms!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## The Information Bottleneck Problem Revisited\n",
        "\n",
        "Let's start by clearly understanding the fundamental problem with basic encoder-decoder models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the information bottleneck problem\n",
        "def visualize_information_bottleneck():\n",
        "    \"\"\"Demonstrate how information gets compressed and lost\"\"\"\n",
        "    \n",
        "    # Simulate different sentence lengths and information content\n",
        "    sentences = [\n",
        "        \"Hi\",\n",
        "        \"Hello there\",\n",
        "        \"How are you doing today?\",\n",
        "        \"I am currently working on a machine learning project that involves natural language processing\",\n",
        "        \"The field of artificial intelligence has been rapidly advancing in recent years with breakthrough developments in deep learning\"\n",
        "    ]\n",
        "    \n",
        "    # Fixed context vector size (typical: 256-512 dimensions)\n",
        "    context_vector_size = 256\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Information content vs context vector capacity\n",
        "    sentence_lengths = [len(sent.split()) for sent in sentences]\n",
        "    # Assume each word contains ~50 units of information\n",
        "    information_content = [length * 50 for length in sentence_lengths]\n",
        "    \n",
        "    ax1.bar(range(len(sentences)), information_content, alpha=0.7, color='skyblue', label='Input Information')\n",
        "    ax1.axhline(y=context_vector_size, color='red', linestyle='--', linewidth=2, label='Context Vector Capacity')\n",
        "    ax1.fill_between(range(len(sentences)), 0, context_vector_size, alpha=0.3, color='red')\n",
        "    \n",
        "    ax1.set_xlabel('Sentence Index')\n",
        "    ax1.set_ylabel('Information Units')\n",
        "    ax1.set_title('Information Bottleneck Problem')\n",
        "    ax1.legend()\n",
        "    ax1.set_xticks(range(len(sentences)))\n",
        "    ax1.set_xticklabels([f'S{i+1}' for i in range(len(sentences))])\n",
        "    \n",
        "    # Information retention percentage\n",
        "    retention_percentages = [min(100, 100 * context_vector_size / info) for info in information_content]\n",
        "    \n",
        "    colors = ['green' if r > 90 else 'orange' if r > 50 else 'red' for r in retention_percentages]\n",
        "    bars = ax2.bar(range(len(sentences)), retention_percentages, alpha=0.7, color=colors)\n",
        "    \n",
        "    ax2.set_xlabel('Sentence Index')\n",
        "    ax2.set_ylabel('Information Retained (%)')\n",
        "    ax2.set_title('Information Loss by Sentence Length')\n",
        "    ax2.set_ylim(0, 100)\n",
        "    ax2.set_xticks(range(len(sentences)))\n",
        "    ax2.set_xticklabels([f'S{i+1}' for i in range(len(sentences))])\n",
        "    \n",
        "    # Add percentage labels on bars\n",
        "    for i, (bar, pct) in enumerate(zip(bars, retention_percentages)):\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                f'{pct:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print detailed analysis\n",
        "    print(\"INFORMATION BOTTLENECK ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    for i, (sent, length, info, retention) in enumerate(zip(sentences, sentence_lengths, information_content, retention_percentages)):\n",
        "        print(f\"\\nSentence {i+1}: \\\"{sent[:50]}{'...' if len(sent) > 50 else ''}\\\"\")\n",
        "        print(f\"  Length: {length} words\")\n",
        "        print(f\"  Estimated information: {info} units\")\n",
        "        print(f\"  Information retained: {retention:.1f}%\")\n",
        "        if retention < 50:\n",
        "            print(\"  ⚠️  SEVERE INFORMATION LOSS!\")\n",
        "        elif retention < 90:\n",
        "            print(\"  ⚠️  Moderate information loss\")\n",
        "        else:\n",
        "            print(\"  ✅ Good information preservation\")\n",
        "\n",
        "visualize_information_bottleneck()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## How Humans Translate: The Inspiration for Attention\n",
        "\n",
        "Before diving into the technical details, let's understand how human translators work, which inspired the attention mechanism.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate human translation process vs encoder-decoder\n",
        "def compare_translation_processes():\n",
        "    \"\"\"Compare how humans translate vs basic encoder-decoder models\"\"\"\n",
        "    \n",
        "    # Example sentence to translate\n",
        "    source_sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "    target_sentence = \"Le renard brun rapide saute par-dessus le chien paresseux\"\n",
        "    \n",
        "    source_words = source_sentence.split()\n",
        "    target_words = target_sentence.split()\n",
        "    \n",
        "    print(\"TRANSLATION PROCESS COMPARISON\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Source: {source_sentence}\")\n",
        "    print(f\"Target: {target_sentence}\")\n",
        "    print()\n",
        "    \n",
        "    # Human translation process\n",
        "    print(\"🧠 HUMAN TRANSLATOR PROCESS:\")\n",
        "    print(\"-\" * 30)\n",
        "    human_process = [\n",
        "        (\"Le\", \"Looking at 'The' - translating article\"),\n",
        "        (\"renard\", \"Looking at 'fox' - main subject\"),\n",
        "        (\"brun\", \"Looking back at 'brown' - adjective for fox\"),\n",
        "        (\"rapide\", \"Looking at 'quick' - another adjective\"),\n",
        "        (\"saute\", \"Looking at 'jumps' - main verb\"),\n",
        "        (\"par-dessus\", \"Looking at 'over' - preposition\"),\n",
        "        (\"le\", \"Looking at 'the' - second article\"),\n",
        "        (\"chien\", \"Looking at 'dog' - second noun\"),\n",
        "        (\"paresseux\", \"Looking at 'lazy' - final adjective\")\n",
        "    ]\n",
        "    \n",
        "    for i, (target_word, process) in enumerate(human_process):\n",
        "        print(f\"Step {i+1}: Generate '{target_word}' - {process}\")\n",
        "    \n",
        "    print(f\"\\n✅ Human translators look back at different parts of the source\")\n",
        "    print(\"✅ They don't memorize everything at once\")\n",
        "    print(\"✅ They focus on relevant words for each output word\")\n",
        "    print()\n",
        "    \n",
        "    # Basic encoder-decoder process\n",
        "    print(\"🤖 BASIC ENCODER-DECODER PROCESS:\")\n",
        "    print(\"-\" * 35)\n",
        "    print(\"Step 1: Encoder reads entire source sentence\")\n",
        "    print(\"Step 2: Encoder creates single context vector\")\n",
        "    print(\"Step 3: Decoder generates target using ONLY context vector\")\n",
        "    print()\n",
        "    print(\"Decoder steps:\")\n",
        "    for i, word in enumerate(target_words):\n",
        "        print(f\"  Step {i+1}: Generate '{word}' - using same context vector\")\n",
        "    \n",
        "    print(f\"\\n❌ Same context vector for all output words\")\n",
        "    print(\"❌ No looking back at specific source words\")\n",
        "    print(\"❌ Information bottleneck problem\")\n",
        "    \n",
        "    return source_words, target_words\n",
        "\n",
        "# Visualize the attention concept\n",
        "def visualize_attention_concept():\n",
        "    \"\"\"Create a visual representation of attention mechanism\"\"\"\n",
        "    \n",
        "    source_words = [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
        "    target_words = [\"Le\", \"renard\", \"brun\", \"rapide\", \"saute\", \"par-dessus\", \"le\", \"chien\", \"paresseux\"]\n",
        "    \n",
        "    # Simulated attention weights (what words to focus on for each target word)\n",
        "    attention_patterns = [\n",
        "        [0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0],  # \"Le\" attends to \"The\"\n",
        "        [0.0, 0.0, 0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.1],  # \"renard\" attends to \"fox\"\n",
        "        [0.0, 0.0, 0.9, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0],  # \"brun\" attends to \"brown\"\n",
        "        [0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1],  # \"rapide\" attends to \"quick\"\n",
        "        [0.0, 0.0, 0.0, 0.0, 0.9, 0.0, 0.0, 0.0, 0.1],  # \"saute\" attends to \"jumps\"\n",
        "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0, 0.0, 0.1],  # \"par-dessus\" attends to \"over\"\n",
        "        [0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0, 0.0],  # \"le\" attends to \"the\"\n",
        "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9],  # \"chien\" attends to \"dog\"\n",
        "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.1],  # \"paresseux\" attends to \"lazy\"\n",
        "    ]\n",
        "    \n",
        "    # Create attention heatmap\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    \n",
        "    attention_matrix = np.array(attention_patterns)\n",
        "    im = ax.imshow(attention_matrix, cmap=attention_cmap, aspect='auto')\n",
        "    \n",
        "    # Set ticks and labels\n",
        "    ax.set_xticks(range(len(source_words)))\n",
        "    ax.set_yticks(range(len(target_words)))\n",
        "    ax.set_xticklabels(source_words, rotation=45, ha='right')\n",
        "    ax.set_yticklabels(target_words)\n",
        "    \n",
        "    # Add text annotations\n",
        "    for i in range(len(target_words)):\n",
        "        for j in range(len(source_words)):\n",
        "            if attention_matrix[i, j] > 0.1:\n",
        "                text = ax.text(j, i, f'{attention_matrix[i, j]:.1f}',\n",
        "                             ha=\"center\", va=\"center\", color=\"white\", fontweight='bold')\n",
        "    \n",
        "    ax.set_xlabel('Source Words (English)', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Target Words (French)', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Attention Mechanism: Where Each Target Word \"Looks\"', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
        "    cbar.set_label('Attention Weight', rotation=270, labelpad=20)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return attention_matrix\n",
        "\n",
        "# Run the comparisons\n",
        "source, target = compare_translation_processes()\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "attention_matrix = visualize_attention_concept()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## The Attention Solution: Technical Intuition\n",
        "\n",
        "Now let's understand how attention mechanisms work technically to solve the information bottleneck.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explain attention mechanism components step by step\n",
        "def explain_attention_components():\n",
        "    \"\"\"Break down the attention mechanism into understandable components\"\"\"\n",
        "    \n",
        "    print(\"ATTENTION MECHANISM COMPONENTS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Step 1: Query, Key, Value concept\n",
        "    print(\"1. QUERY-KEY-VALUE FRAMEWORK\")\n",
        "    print(\"-\" * 30)\n",
        "    print(\"🔍 QUERY (Q): 'What am I trying to generate right now?'\")\n",
        "    print(\"   - Current decoder state\")\n",
        "    print(\"   - Represents what we want to focus on\")\n",
        "    print(\"   - Example: When generating 'renard', Q represents the current decoding step\")\n",
        "    print()\n",
        "    \n",
        "    print(\"🔑 KEY (K): 'What information is available to attend to?'\")\n",
        "    print(\"   - All encoder hidden states\")\n",
        "    print(\"   - Represents available information\")\n",
        "    print(\"   - Example: Hidden states for ['The', 'quick', 'brown', 'fox', ...]\")\n",
        "    print()\n",
        "    \n",
        "    print(\"💎 VALUE (V): 'What information do I extract?'\")\n",
        "    print(\"   - Same as keys (encoder hidden states)\")\n",
        "    print(\"   - The actual information we combine\")\n",
        "    print(\"   - Example: Actual vector representations of input words\")\n",
        "    print()\n",
        "    \n",
        "    # Step 2: Attention computation\n",
        "    print(\"2. ATTENTION COMPUTATION PROCESS\")\n",
        "    print(\"-\" * 35)\n",
        "    print(\"Step 1: Compute similarity scores between Query and all Keys\")\n",
        "    print(\"        scores = Q · K (dot product)\")\n",
        "    print()\n",
        "    print(\"Step 2: Convert scores to probabilities (attention weights)\")\n",
        "    print(\"        attention_weights = softmax(scores)\")\n",
        "    print()\n",
        "    print(\"Step 3: Compute weighted average of Values\")\n",
        "    print(\"        context = Σ(attention_weights × Values)\")\n",
        "    print()\n",
        "    \n",
        "    # Step 3: Mathematical example\n",
        "    print(\"3. SIMPLIFIED MATHEMATICAL EXAMPLE\")\n",
        "    print(\"-\" * 38)\n",
        "    \n",
        "    # Simulate small vectors for explanation\n",
        "    print(\"Assume we have 3 encoder states and 1 decoder state:\")\n",
        "    \n",
        "    # Dummy vectors (simplified to 2D for visualization)\n",
        "    query = np.array([1.0, 0.5])\n",
        "    keys = np.array([[0.8, 0.2],   # Key 1\n",
        "                     [0.1, 0.9],   # Key 2  \n",
        "                     [0.9, 0.1]])  # Key 3\n",
        "    values = keys.copy()  # Values same as keys in basic attention\n",
        "    \n",
        "    print(f\"Query (current decoder state): {query}\")\n",
        "    print(f\"Key 1 (encoder state 1): {keys[0]}\")\n",
        "    print(f\"Key 2 (encoder state 2): {keys[1]}\")\n",
        "    print(f\"Key 3 (encoder state 3): {keys[2]}\")\n",
        "    print()\n",
        "    \n",
        "    # Compute attention scores\n",
        "    scores = np.dot(keys, query)\n",
        "    print(f\"Attention scores (Q·K): {scores}\")\n",
        "    \n",
        "    # Compute attention weights\n",
        "    attention_weights = np.exp(scores) / np.sum(np.exp(scores))\n",
        "    print(f\"Attention weights (softmax): {attention_weights}\")\n",
        "    print(f\"Sum of weights: {np.sum(attention_weights):.3f} (should be 1.0)\")\n",
        "    print()\n",
        "    \n",
        "    # Compute context vector\n",
        "    context = np.sum(attention_weights.reshape(-1, 1) * values, axis=0)\n",
        "    print(f\"Context vector: {context}\")\n",
        "    print()\n",
        "    \n",
        "    # Visualize the process\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n",
        "    \n",
        "    # Plot 1: Attention scores\n",
        "    ax1.bar(['Key 1', 'Key 2', 'Key 3'], scores, color='lightblue', alpha=0.7)\n",
        "    ax1.set_title('Step 1: Attention Scores (Q·K)')\n",
        "    ax1.set_ylabel('Score')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Attention weights\n",
        "    colors = ['red', 'green', 'blue']\n",
        "    bars = ax2.bar(['Key 1', 'Key 2', 'Key 3'], attention_weights, color=colors, alpha=0.7)\n",
        "    ax2.set_title('Step 2: Attention Weights (Softmax)')\n",
        "    ax2.set_ylabel('Weight')\n",
        "    ax2.set_ylim(0, 1)\n",
        "    for bar, weight in zip(bars, attention_weights):\n",
        "        height = bar.get_height()\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{weight:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 3: Values weighted by attention\n",
        "    weighted_values = attention_weights.reshape(-1, 1) * values\n",
        "    ax3.bar(range(len(values)), [v[0] for v in weighted_values], alpha=0.7, \n",
        "            color=colors, label='Dimension 1')\n",
        "    ax3.bar(range(len(values)), [v[1] for v in weighted_values], bottom=[v[0] for v in weighted_values],\n",
        "            alpha=0.7, color=colors, label='Dimension 2')\n",
        "    ax3.set_title('Step 3: Weighted Values')\n",
        "    ax3.set_xlabel('Encoder State')\n",
        "    ax3.set_ylabel('Weighted Value')\n",
        "    ax3.set_xticks(range(3))\n",
        "    ax3.set_xticklabels(['State 1', 'State 2', 'State 3'])\n",
        "    ax3.legend()\n",
        "    \n",
        "    # Plot 4: Final context vector\n",
        "    ax4.bar(['Dim 1', 'Dim 2'], context, color='purple', alpha=0.7)\n",
        "    ax4.set_title('Step 4: Context Vector (Weighted Sum)')\n",
        "    ax4.set_ylabel('Value')\n",
        "    for i, val in enumerate(context):\n",
        "        ax4.text(i, val + 0.01, f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return attention_weights, context\n",
        "\n",
        "# Run the explanation\n",
        "weights, context = explain_attention_components()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary: The Attention Revolution\n",
        "\n",
        "### What We've Learned Today\n",
        "1. **The Problem**: Information bottleneck in basic encoder-decoder models\n",
        "2. **The Inspiration**: How human translators actually work\n",
        "3. **The Solution**: Attention mechanisms that allow \"looking back\"\n",
        "4. **The Implementation**: Query-Key-Value framework for computing attention\n",
        "\n",
        "### Key Insights\n",
        "- **Fixed Context Vector**: Major limitation of basic seq2seq models\n",
        "- **Dynamic Context**: Attention creates different context for each output step\n",
        "- **Interpretability**: We can visualize what the model focuses on\n",
        "- **Performance**: Dramatic improvement on long sequences\n",
        "\n",
        "### Mathematical Foundation\n",
        "- **Similarity Computation**: Dot product between query and keys\n",
        "- **Normalization**: Softmax to convert scores to probabilities  \n",
        "- **Weighted Combination**: Attention weights applied to values\n",
        "- **Scaling**: Important for numerical stability\n",
        "\n",
        "### Tomorrow's Preview\n",
        "We'll implement complete translation systems using attention and explore bidirectional encoders to further improve translation quality.\n",
        "\n",
        "### Personal Reflections\n",
        "- The attention mechanism feels like a natural solution once explained\n",
        "- The human translation analogy really helps understand the concept\n",
        "- Seeing attention weights visualized is incredibly insightful\n",
        "- This is clearly the foundation for modern NLP breakthroughs\n",
        "- The progression from information bottleneck to attention is elegant\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## The Information Bottleneck Problem Visualized\n",
        "\n",
        "Let's create a concrete example showing how the fixed context vector fails with longer sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate information bottleneck with different sequence lengths\n",
        "def demonstrate_bottleneck_problem():\n",
        "    \"\"\"Show how information gets lost in longer sequences\"\"\"\n",
        "    \n",
        "    # Example sentences of increasing length\n",
        "    sentences = [\n",
        "        (\"Hi\", \"Salut\"),\n",
        "        (\"Hello there\", \"Bonjour là\"),\n",
        "        (\"How are you doing today\", \"Comment allez-vous aujourd'hui\"),\n",
        "        (\"I would like to reserve a table for two people at eight o'clock tonight\", \n",
        "         \"Je voudrais réserver une table pour deux personnes à huit heures ce soir\"),\n",
        "        (\"The weather forecast says it will be sunny tomorrow with a high temperature of twenty-five degrees\", \n",
        "         \"Les prévisions météorologiques disent qu'il fera ensoleillé demain avec une température maximale de vingt-cinq degrés\")\n",
        "    ]\n",
        "    \n",
        "    # Simulate context vector capacity (fixed size)\n",
        "    context_capacity = 256  # typical hidden dimension\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
        "    \n",
        "    # Data for plotting\n",
        "    input_lengths = [len(s[0].split()) for s in sentences]\n",
        "    output_lengths = [len(s[1].split()) for s in sentences]\n",
        "    \n",
        "    # Estimate information content (simplified)\n",
        "    input_info = [length * 40 for length in input_lengths]  # 40 info units per word\n",
        "    \n",
        "    # Information retention calculation\n",
        "    retention_rates = [min(100, 100 * context_capacity / info) for info in input_info]\n",
        "    \n",
        "    # Plot 1: Information vs Context Capacity\n",
        "    ax1.bar(range(len(sentences)), input_info, alpha=0.7, color='skyblue', label='Input Information')\n",
        "    ax1.axhline(y=context_capacity, color='red', linestyle='--', linewidth=2, label='Context Vector Capacity')\n",
        "    ax1.fill_between(range(len(sentences)), 0, context_capacity, alpha=0.3, color='red', label='Representable Information')\n",
        "    \n",
        "    ax1.set_xlabel('Sentence Complexity')\n",
        "    ax1.set_ylabel('Information Units')\n",
        "    ax1.set_title('Information Bottleneck Problem')\n",
        "    ax1.set_xticks(range(len(sentences)))\n",
        "    ax1.set_xticklabels([f'Sent {i+1}\\n({l} words)' for i, l in enumerate(input_lengths)])\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Information Retention Rate\n",
        "    colors = ['green' if r > 80 else 'orange' if r > 50 else 'red' for r in retention_rates]\n",
        "    bars = ax2.bar(range(len(sentences)), retention_rates, color=colors, alpha=0.7)\n",
        "    \n",
        "    ax2.set_xlabel('Sentence Complexity')\n",
        "    ax2.set_ylabel('Information Retained (%)')\n",
        "    ax2.set_title('Information Retention Rate')\n",
        "    ax2.set_xticks(range(len(sentences)))\n",
        "    ax2.set_xticklabels([f'Sent {i+1}\\n({l} words)' for i, l in enumerate(input_lengths)])\n",
        "    ax2.set_ylim(0, 100)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add percentage labels on bars\n",
        "    for i, (bar, rate) in enumerate(zip(bars, retention_rates)):\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
        "                f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print detailed analysis\n",
        "    print(\"BOTTLENECK ANALYSIS\")\n",
        "    print(\"=\" * 50)\n",
        "    for i, ((eng, fr), info, retention) in enumerate(zip(sentences, input_info, retention_rates)):\n",
        "        print(f\"\\nSentence {i+1}:\")\n",
        "        print(f\"  English: '{eng}' ({len(eng.split())} words)\")\n",
        "        print(f\"  French:  '{fr}' ({len(fr.split())} words)\")\n",
        "        print(f\"  Information: {info} units\")\n",
        "        print(f\"  Retention: {retention:.1f}%\")\n",
        "        if retention < 50:\n",
        "            print(\"  ⚠️  SEVERE INFORMATION LOSS - Translation quality will suffer!\")\n",
        "        elif retention < 80:\n",
        "            print(\"  ⚠️  MODERATE INFORMATION LOSS - Some details may be lost\")\n",
        "        else:\n",
        "            print(\"  ✅ GOOD RETENTION - Should translate well\")\n",
        "\n",
        "demonstrate_bottleneck_problem()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
