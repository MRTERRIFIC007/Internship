{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Scaled Dot-Product Attention: The Foundation of Transformers\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand scaled dot-product attention mechanism\n",
        "- Learn why scaling is important for stability\n",
        "- Implement the attention function from \"Attention Is All You Need\"\n",
        "- Explore the mathematical properties\n",
        "- Understand the foundation for transformer architecture\n",
        "\n",
        "## The Formula\n",
        "The scaled dot-product attention is defined as:\n",
        "\n",
        "**Attention(Q, K, V) = softmax(QK^T / √d_k)V**\n",
        "\n",
        "Where:\n",
        "- Q: Queries matrix\n",
        "- K: Keys matrix  \n",
        "- V: Values matrix\n",
        "- d_k: Dimension of the key vectors\n",
        "- √d_k: Scaling factor to prevent vanishing gradients\n",
        "\n",
        "This is the core attention mechanism used in Transformer models like BERT, GPT, and T5.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Ready to implement scaled dot-product attention!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Implementation of Scaled Dot-Product Attention\n",
        "\n",
        "Let's implement the exact attention mechanism used in the Transformer paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    \"\"\"\n",
        "    Scaled Dot-Product Attention as defined in \"Attention Is All You Need\"\n",
        "    \n",
        "    Args:\n",
        "        query: [batch_size, seq_len_q, d_k]\n",
        "        key: [batch_size, seq_len_k, d_k]  \n",
        "        value: [batch_size, seq_len_k, d_v]\n",
        "        mask: [batch_size, seq_len_q, seq_len_k] optional\n",
        "    \n",
        "    Returns:\n",
        "        output: [batch_size, seq_len_q, d_v]\n",
        "        attention_weights: [batch_size, seq_len_q, seq_len_k]\n",
        "    \"\"\"\n",
        "    \n",
        "    # Get dimensions\n",
        "    d_k = query.size(-1)\n",
        "    \n",
        "    # Compute attention scores: QK^T\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "    \n",
        "    # Scale by sqrt(d_k)\n",
        "    scores = scores / math.sqrt(d_k)\n",
        "    \n",
        "    # Apply mask if provided (set masked positions to large negative value)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    \n",
        "    # Apply softmax to get attention weights\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    \n",
        "    # Apply attention weights to values\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "    \n",
        "    return output, attention_weights\n",
        "\n",
        "# Test the implementation\n",
        "def test_scaled_dot_product_attention():\n",
        "    \"\"\"Test scaled dot-product attention with simple examples\"\"\"\n",
        "    \n",
        "    batch_size = 2\n",
        "    seq_len = 4\n",
        "    d_k = 8\n",
        "    d_v = 8\n",
        "    \n",
        "    # Create test tensors\n",
        "    query = torch.randn(batch_size, seq_len, d_k)\n",
        "    key = torch.randn(batch_size, seq_len, d_k) \n",
        "    value = torch.randn(batch_size, seq_len, d_v)\n",
        "    \n",
        "    # Forward pass\n",
        "    output, attention_weights = scaled_dot_product_attention(query, key, value)\n",
        "    \n",
        "    print(\"SCALED DOT-PRODUCT ATTENTION TEST\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"Input shapes:\")\n",
        "    print(f\"  Query: {query.shape}\")\n",
        "    print(f\"  Key: {key.shape}\")\n",
        "    print(f\"  Value: {value.shape}\")\n",
        "    print()\n",
        "    print(f\"Output shapes:\")\n",
        "    print(f\"  Output: {output.shape}\")\n",
        "    print(f\"  Attention weights: {attention_weights.shape}\")\n",
        "    print()\n",
        "    \n",
        "    # Verify attention weights sum to 1\n",
        "    attention_sums = attention_weights.sum(dim=-1)\n",
        "    print(f\"Attention weights sum (should be 1.0):\")\n",
        "    print(f\"  Min: {attention_sums.min().item():.6f}\")\n",
        "    print(f\"  Max: {attention_sums.max().item():.6f}\")\n",
        "    print(f\"  Mean: {attention_sums.mean().item():.6f}\")\n",
        "    \n",
        "    return output, attention_weights\n",
        "\n",
        "# Run the test\n",
        "output, weights = test_scaled_dot_product_attention()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Scaled Dot-Product Attention: The Heart of Transformers\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the scaled dot-product attention formula\n",
        "- Learn why scaling is necessary for stable gradients\n",
        "- Implement from scratch and using PyTorch\n",
        "- Understand query, key, value projections\n",
        "- Explore computational efficiency benefits\n",
        "\n",
        "## Mathematical Foundation\n",
        "The scaled dot-product attention is defined as:\n",
        "\n",
        "**Attention(Q, K, V) = softmax(QK^T / √d_k)V**\n",
        "\n",
        "Where:\n",
        "- Q: Query matrix\n",
        "- K: Key matrix  \n",
        "- V: Value matrix\n",
        "- d_k: Dimension of key vectors\n",
        "\n",
        "This simple yet powerful mechanism became the foundation of Transformer architectures.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
