{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Basic Attention Mechanism Implementation\n",
        "\n",
        "## Learning Objectives\n",
        "- Implement attention mechanism from scratch\n",
        "- Understand query, key, value framework\n",
        "- Learn attention weight computation\n",
        "- Visualize attention patterns\n",
        "- Compare with and without attention\n",
        "\n",
        "## Implementation Steps\n",
        "1. **Basic Attention Layer**: Core attention computation\n",
        "2. **Encoder with Attention**: Modified encoder to output all hidden states\n",
        "3. **Decoder with Attention**: Decoder that attends to encoder outputs\n",
        "4. **Complete Model**: Full attention-based seq2seq model\n",
        "5. **Training and Evaluation**: Compare performance with basic model\n",
        "\n",
        "This notebook provides hands-on implementation of the attention mechanism we learned about conceptually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Plotting setup\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Ready to implement attention mechanisms!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 1: Basic Attention Layer Implementation\n",
        "\n",
        "Let's start by implementing a basic attention mechanism from scratch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BasicAttention(nn.Module):\n",
        "    \"\"\"Basic attention mechanism implementation\"\"\"\n",
        "    \n",
        "    def __init__(self, hidden_dim):\n",
        "        super(BasicAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # Linear layers for computing attention scores\n",
        "        self.attention = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "        \n",
        "    def forward(self, query, keys, values):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            query: [batch_size, hidden_dim] - current decoder state\n",
        "            keys: [batch_size, seq_len, hidden_dim] - encoder outputs\n",
        "            values: [batch_size, seq_len, hidden_dim] - encoder outputs (same as keys)\n",
        "        \n",
        "        Returns:\n",
        "            context: [batch_size, hidden_dim] - attended context vector\n",
        "            attention_weights: [batch_size, seq_len] - attention weights\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, hidden_dim = keys.size()\n",
        "        \n",
        "        # Expand query to match keys dimensions\n",
        "        # query: [batch_size, hidden_dim] -> [batch_size, seq_len, hidden_dim]\n",
        "        query_expanded = query.unsqueeze(1).expand(batch_size, seq_len, hidden_dim)\n",
        "        \n",
        "        # Concatenate query and keys\n",
        "        # combined: [batch_size, seq_len, hidden_dim * 2]\n",
        "        combined = torch.cat([query_expanded, keys], dim=2)\n",
        "        \n",
        "        # Compute attention scores\n",
        "        # energy: [batch_size, seq_len, hidden_dim]\n",
        "        energy = torch.tanh(self.attention(combined))\n",
        "        \n",
        "        # energy: [batch_size, seq_len, 1] -> [batch_size, seq_len]\n",
        "        attention_scores = self.v(energy).squeeze(2)\n",
        "        \n",
        "        # Convert to attention weights using softmax\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)\n",
        "        \n",
        "        # Compute context vector as weighted sum of values\n",
        "        # attention_weights: [batch_size, seq_len] -> [batch_size, seq_len, 1]\n",
        "        attention_weights_expanded = attention_weights.unsqueeze(2)\n",
        "        \n",
        "        # context: [batch_size, hidden_dim]\n",
        "        context = torch.sum(attention_weights_expanded * values, dim=1)\n",
        "        \n",
        "        return context, attention_weights\n",
        "\n",
        "# Test the basic attention mechanism\n",
        "def test_basic_attention():\n",
        "    \"\"\"Test the basic attention implementation\"\"\"\n",
        "    \n",
        "    batch_size = 2\n",
        "    seq_len = 5\n",
        "    hidden_dim = 8\n",
        "    \n",
        "    # Create test data\n",
        "    query = torch.randn(batch_size, hidden_dim)\n",
        "    keys = torch.randn(batch_size, seq_len, hidden_dim)\n",
        "    values = keys.clone()  # In basic attention, values = keys\n",
        "    \n",
        "    # Initialize attention layer\n",
        "    attention_layer = BasicAttention(hidden_dim)\n",
        "    \n",
        "    # Forward pass\n",
        "    context, attention_weights = attention_layer(query, keys, values)\n",
        "    \n",
        "    print(\"BASIC ATTENTION TEST\")\n",
        "    print(\"=\" * 30)\n",
        "    print(f\"Query shape: {query.shape}\")\n",
        "    print(f\"Keys shape: {keys.shape}\")\n",
        "    print(f\"Values shape: {values.shape}\")\n",
        "    print()\n",
        "    print(f\"Context shape: {context.shape}\")\n",
        "    print(f\"Attention weights shape: {attention_weights.shape}\")\n",
        "    print()\n",
        "    print(\"Attention weights (should sum to 1):\")\n",
        "    for i in range(batch_size):\n",
        "        weights = attention_weights[i].detach().numpy()\n",
        "        print(f\"  Batch {i}: {weights}\")\n",
        "        print(f\"  Sum: {weights.sum():.6f}\")\n",
        "    \n",
        "    return context, attention_weights, attention_layer\n",
        "\n",
        "# Run the test\n",
        "context, weights, attention_layer = test_basic_attention()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 1: Basic Attention Layer Implementation\n",
        "\n",
        "The core of attention is computing attention weights and using them to create a context vector. Let's implement this step by step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BasicAttention(nn.Module):\n",
        "    \"\"\"Basic attention mechanism implementation\"\"\"\n",
        "    \n",
        "    def __init__(self, hidden_dim):\n",
        "        super(BasicAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # Linear layers for computing attention scores\n",
        "        self.attention = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "        \n",
        "    def forward(self, decoder_hidden, encoder_outputs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            decoder_hidden: (batch_size, hidden_dim) - current decoder state\n",
        "            encoder_outputs: (batch_size, seq_len, hidden_dim) - all encoder states\n",
        "        \n",
        "        Returns:\n",
        "            context_vector: (batch_size, hidden_dim) - weighted sum of encoder outputs\n",
        "            attention_weights: (batch_size, seq_len) - attention weights\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, hidden_dim = encoder_outputs.size()\n",
        "        \n",
        "        # Repeat decoder hidden state for each encoder position\n",
        "        # (batch_size, seq_len, hidden_dim)\n",
        "        decoder_hidden_repeated = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
        "        \n",
        "        # Concatenate decoder and encoder states\n",
        "        # (batch_size, seq_len, hidden_dim * 2)\n",
        "        combined = torch.cat([decoder_hidden_repeated, encoder_outputs], dim=2)\n",
        "        \n",
        "        # Compute attention scores\n",
        "        # (batch_size, seq_len, hidden_dim)\n",
        "        energy = torch.tanh(self.attention(combined))\n",
        "        \n",
        "        # Convert to scalar attention scores\n",
        "        # (batch_size, seq_len, 1) -> (batch_size, seq_len)\n",
        "        attention_scores = self.v(energy).squeeze(2)\n",
        "        \n",
        "        # Convert scores to probabilities (attention weights)\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)\n",
        "        \n",
        "        # Compute context vector as weighted sum of encoder outputs\n",
        "        # (batch_size, 1, seq_len) x (batch_size, seq_len, hidden_dim) -> (batch_size, 1, hidden_dim)\n",
        "        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
        "        context_vector = context_vector.squeeze(1)  # (batch_size, hidden_dim)\n",
        "        \n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# Test the attention mechanism\n",
        "hidden_dim = 128\n",
        "seq_len = 6\n",
        "batch_size = 2\n",
        "\n",
        "# Create test data\n",
        "attention_layer = BasicAttention(hidden_dim)\n",
        "decoder_hidden = torch.randn(batch_size, hidden_dim)\n",
        "encoder_outputs = torch.randn(batch_size, seq_len, hidden_dim)\n",
        "\n",
        "# Forward pass\n",
        "context_vector, attention_weights = attention_layer(decoder_hidden, encoder_outputs)\n",
        "\n",
        "print(\"Attention Mechanism Test:\")\n",
        "print(f\"Decoder hidden shape: {decoder_hidden.shape}\")\n",
        "print(f\"Encoder outputs shape: {encoder_outputs.shape}\")\n",
        "print(f\"Context vector shape: {context_vector.shape}\")\n",
        "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
        "print(f\"Attention weights sum: {attention_weights.sum(dim=1)}\")  # Should be 1.0\n",
        "print(f\"Sample attention weights: {attention_weights[0].detach().numpy()}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
