{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Testing and Debugging Sequence-to-Sequence Models\n",
        "\n",
        "## Learning Objectives\n",
        "- Test trained seq2seq models on new data\n",
        "- Debug common issues in sequence generation\n",
        "- Understand evaluation metrics for sequence tasks\n",
        "- Learn about inference without teacher forcing\n",
        "- Identify and solve typical seq2seq problems\n",
        "\n",
        "## Common Issues We'll Address\n",
        "- **Repetitive Outputs**: Model gets stuck generating same tokens\n",
        "- **Short Sequences**: Model terminates too early\n",
        "- **Nonsensical Results**: Poor context understanding\n",
        "- **Exposure Bias**: Train/test mismatch from teacher forcing\n",
        "\n",
        "## Evaluation Strategies\n",
        "- **BLEU Score**: Translation quality metric\n",
        "- **Perplexity**: Language modeling evaluation\n",
        "- **Manual Inspection**: Human evaluation of outputs\n",
        "- **Error Analysis**: Understanding failure modes\n",
        "\n",
        "This notebook focuses on practical debugging and evaluation of our seq2seq models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Common Seq2Seq Problems and Solutions\n",
        "\n",
        "Let's explore the typical issues that arise when training and deploying sequence-to-sequence models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Common problems in sequence-to-sequence models\n",
        "def analyze_seq2seq_problems():\n",
        "    \"\"\"Analyze typical problems and their causes\"\"\"\n",
        "    \n",
        "    problems = {\n",
        "        \"Repetitive Outputs\": {\n",
        "            \"description\": \"Model generates the same token repeatedly\",\n",
        "            \"example\": \"Input: 'Hello' -> Output: 'the the the the the'\",\n",
        "            \"causes\": [\n",
        "                \"Poor context vector representation\",\n",
        "                \"Insufficient training data\",\n",
        "                \"Local minima in training\",\n",
        "                \"Vanishing gradients\"\n",
        "            ],\n",
        "            \"solutions\": [\n",
        "                \"Add attention mechanisms\",\n",
        "                \"Increase model capacity\",\n",
        "                \"Better initialization\",\n",
        "                \"Gradient clipping\"\n",
        "            ]\n",
        "        },\n",
        "        \"Short Sequences\": {\n",
        "            \"description\": \"Model terminates sequences too early\", \n",
        "            \"example\": \"Input: 'How are you today?' -> Output: 'Fine.'\",\n",
        "            \"causes\": [\n",
        "                \"Biased training data (short targets)\",\n",
        "                \"EOS token learned too aggressively\",\n",
        "                \"Information loss in context vector\"\n",
        "            ],\n",
        "            \"solutions\": [\n",
        "                \"Balanced sequence lengths in training\",\n",
        "                \"Sequence length penalties\",\n",
        "                \"Better context representation\"\n",
        "            ]\n",
        "        },\n",
        "        \"Nonsensical Results\": {\n",
        "            \"description\": \"Grammatically incorrect or meaningless output\",\n",
        "            \"example\": \"Input: 'Good morning' -> Output: 'Car blue elephant'\",\n",
        "            \"causes\": [\n",
        "                \"Insufficient training\",\n",
        "                \"Poor vocabulary handling\",\n",
        "                \"Context vector bottleneck\",\n",
        "                \"Out-of-vocabulary words\"\n",
        "            ],\n",
        "            \"solutions\": [\n",
        "                \"More training data\",\n",
        "                \"Better preprocessing\",\n",
        "                \"Attention mechanisms\",\n",
        "                \"Subword tokenization\"\n",
        "            ]\n",
        "        },\n",
        "        \"Exposure Bias\": {\n",
        "            \"description\": \"Performance gap between training and inference\",\n",
        "            \"example\": \"Good training loss but poor inference quality\",\n",
        "            \"causes\": [\n",
        "                \"Teacher forcing during training\",\n",
        "                \"Model never sees its own errors\",\n",
        "                \"Different input distributions\"\n",
        "            ],\n",
        "            \"solutions\": [\n",
        "                \"Scheduled sampling\",\n",
        "                \"Curriculum learning\",\n",
        "                \"Inference-time training\"\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Display problems and solutions\n",
        "    for i, (problem, details) in enumerate(problems.items()):\n",
        "        print(f\"{i+1}. {problem}\")\n",
        "        print(\"=\" * (len(problem) + 3))\n",
        "        print(f\"Description: {details['description']}\")\n",
        "        print(f\"Example: {details['example']}\")\n",
        "        print(\"Causes:\")\n",
        "        for cause in details['causes']:\n",
        "            print(f\"  • {cause}\")\n",
        "        print(\"Solutions:\")\n",
        "        for solution in details['solutions']:\n",
        "            print(f\"  ✓ {solution}\")\n",
        "        print()\n",
        "\n",
        "analyze_seq2seq_problems()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
