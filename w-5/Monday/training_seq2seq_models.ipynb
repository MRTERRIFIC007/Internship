{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Training Sequence-to-Sequence Models\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the training procedure for seq2seq models\n",
        "- Learn about teacher forcing and its benefits/drawbacks\n",
        "- Implement data preparation and batching for variable-length sequences\n",
        "- Handle sequence padding and masking\n",
        "- Understand training loop specifics for encoder-decoder models\n",
        "\n",
        "## Key Training Concepts\n",
        "- **Teacher Forcing**: Using ground truth as decoder input during training\n",
        "- **Inference Mode**: Using model predictions during testing\n",
        "- **Sequence Padding**: Making variable-length sequences the same length\n",
        "- **Loss Masking**: Ignoring padded positions in loss calculation\n",
        "\n",
        "Today we'll implement a complete training pipeline for sequence-to-sequence models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import random\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Understanding Teacher Forcing\n",
        "\n",
        "Teacher forcing is a key training technique for sequence-to-sequence models. Let's understand what it is and why it's important.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate teacher forcing vs free running\n",
        "def compare_training_modes():\n",
        "    \"\"\"Compare teacher forcing and free running during training\"\"\"\n",
        "    \n",
        "    print(\"TEACHER FORCING vs FREE RUNNING COMPARISON\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Example: translating \"Hello\" to \"Bonjour\"\n",
        "    input_sentence = \"Hello\"\n",
        "    target_sentence = \"Bonjour\"\n",
        "    \n",
        "    print(f\"Input: {input_sentence}\")\n",
        "    print(f\"Target: {target_sentence}\")\n",
        "    print()\n",
        "    \n",
        "    # Teacher forcing mode (training)\n",
        "    print(\"TEACHER FORCING (Training Mode):\")\n",
        "    print(\"Decoder Input:  [SOS] B  o  n  j  o  u\")\n",
        "    print(\"Decoder Target: B  o  n  j  o  u  r [EOS]\")\n",
        "    print(\"- Uses ground truth as input at each step\")\n",
        "    print(\"- Fast and stable training\")\n",
        "    print(\"- No error accumulation during training\")\n",
        "    print()\n",
        "    \n",
        "    # Free running mode (inference)\n",
        "    print(\"FREE RUNNING (Inference Mode):\")\n",
        "    print(\"Step 1: Input=[SOS]     -> Output=B\")\n",
        "    print(\"Step 2: Input=B         -> Output=o\") \n",
        "    print(\"Step 3: Input=o         -> Output=n\")\n",
        "    print(\"Step 4: Input=n         -> Output=j\")\n",
        "    print(\"- Uses own predictions as input\")\n",
        "    print(\"- Errors can accumulate\")\n",
        "    print(\"- Real-world usage scenario\")\n",
        "\n",
        "compare_training_modes()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Implementing Sequence Padding\n",
        "\n",
        "One of the key challenges in seq2seq training is handling variable-length sequences efficiently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate sequence padding for batch processing\n",
        "def demonstrate_sequence_padding():\n",
        "    \"\"\"Show how to handle variable-length sequences in batches\"\"\"\n",
        "    \n",
        "    # Example sentences of different lengths\n",
        "    sentences = [\n",
        "        [\"I\", \"love\", \"AI\"],\n",
        "        [\"Machine\", \"learning\", \"is\", \"fascinating\"],\n",
        "        [\"Deep\", \"neural\", \"networks\", \"are\", \"powerful\", \"tools\"],\n",
        "        [\"NLP\"]\n",
        "    ]\n",
        "    \n",
        "    print(\"SEQUENCE PADDING DEMONSTRATION\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Show original sequences\n",
        "    print(\"Original sequences:\")\n",
        "    for i, sent in enumerate(sentences):\n",
        "        print(f\"  {i+1}: {sent} (length: {len(sent)})\")\n",
        "    \n",
        "    # Find maximum length\n",
        "    max_len = max(len(sent) for sent in sentences)\n",
        "    print(f\"\\nMaximum length: {max_len}\")\n",
        "    \n",
        "    # Pad sequences\n",
        "    padded_sequences = []\n",
        "    for sent in sentences:\n",
        "        padded = sent + [\"<PAD>\"] * (max_len - len(sent))\n",
        "        padded_sequences.append(padded)\n",
        "    \n",
        "    print(\"\\nPadded sequences:\")\n",
        "    for i, sent in enumerate(padded_sequences):\n",
        "        print(f\"  {i+1}: {sent}\")\n",
        "    \n",
        "    return padded_sequences\n",
        "\n",
        "# Run the demonstration\n",
        "padded_seqs = demonstrate_sequence_padding()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
