{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# T5 Evaluation Metrics and Analysis\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand evaluation challenges for multi-task models\n",
        "- Learn task-specific metrics for T5 outputs\n",
        "- Implement comprehensive evaluation frameworks\n",
        "- Analyze model performance across different tasks\n",
        "- Compare single-task vs multi-task performance\n",
        "\n",
        "## Why Evaluation is Complex for T5\n",
        "T5's text-to-text approach creates unique evaluation challenges:\n",
        "- **Multiple Tasks**: Need metrics for each task type\n",
        "- **Text Generation**: Outputs are sequences, not single values\n",
        "- **Open-Ended**: Multiple correct answers possible\n",
        "- **Transfer Effects**: Tasks influence each other's performance\n",
        "\n",
        "This notebook explores comprehensive evaluation strategies for T5 models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict, Counter\n",
        "import re\n",
        "import random\n",
        "from typing import List, Dict, Tuple, Any\n",
        "import math\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device available: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Task-Specific Evaluation Metrics\n",
        "\n",
        "Different tasks require different evaluation approaches. Let's implement comprehensive metrics for each task type.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive evaluation framework for T5 models\n",
        "print(\"T5 EVALUATION FRAMEWORK\")\n",
        "print(\"=\" * 40)\n",
        "print(\"Key evaluation challenges:\")\n",
        "print(\"1. Multiple tasks require different metrics\")\n",
        "print(\"2. Text generation makes evaluation complex\")\n",
        "print(\"3. Need to balance task-specific vs overall performance\")\n",
        "print(\"4. Transfer effects between tasks must be measured\")\n",
        "print()\n",
        "print(\"Week 5 T5 learning complete! ðŸŽ‰\")\n",
        "print(\"You've learned:\")\n",
        "print(\"- Text-to-text paradigm fundamentals\")\n",
        "print(\"- Multi-task learning strategies\")\n",
        "print(\"- Task balancing and sampling techniques\")\n",
        "print(\"- Evaluation approaches for complex models\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class T5EvaluationMetrics:\n",
        "    \"\"\"Comprehensive evaluation metrics for T5 multi-task models\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    # Classification/Sentiment Analysis Metrics\n",
        "    def exact_match_accuracy(self, predictions: List[str], targets: List[str]) -> float:\n",
        "        \"\"\"Calculate exact match accuracy for classification tasks\"\"\"\n",
        "        if len(predictions) != len(targets):\n",
        "            raise ValueError(\"Predictions and targets must have same length\")\n",
        "        \n",
        "        correct = sum(1 for pred, target in zip(predictions, targets) \n",
        "                     if pred.strip().lower() == target.strip().lower())\n",
        "        return correct / len(predictions)\n",
        "    \n",
        "    def fuzzy_match_accuracy(self, predictions: List[str], targets: List[str], threshold: float = 0.8) -> float:\n",
        "        \"\"\"Calculate fuzzy match accuracy using string similarity\"\"\"\n",
        "        if len(predictions) != len(targets):\n",
        "            raise ValueError(\"Predictions and targets must have same length\")\n",
        "        \n",
        "        correct = 0\n",
        "        for pred, target in zip(predictions, targets):\n",
        "            similarity = self._string_similarity(pred.strip().lower(), target.strip().lower())\n",
        "            if similarity >= threshold:\n",
        "                correct += 1\n",
        "        \n",
        "        return correct / len(predictions)\n",
        "    \n",
        "    # Translation Metrics\n",
        "    def bleu_score_simple(self, predictions: List[str], targets: List[str]) -> float:\n",
        "        \"\"\"Simplified BLEU score calculation\"\"\"\n",
        "        total_score = 0\n",
        "        \n",
        "        for pred, target in zip(predictions, targets):\n",
        "            pred_tokens = pred.lower().split()\n",
        "            target_tokens = target.lower().split()\n",
        "            \n",
        "            if len(pred_tokens) == 0:\n",
        "                continue\n",
        "            \n",
        "            # Calculate precision for unigrams\n",
        "            matches = 0\n",
        "            for token in pred_tokens:\n",
        "                if token in target_tokens:\n",
        "                    matches += 1\n",
        "            \n",
        "            precision = matches / len(pred_tokens)\n",
        "            \n",
        "            # Length penalty\n",
        "            bp = min(1.0, len(pred_tokens) / len(target_tokens)) if len(target_tokens) > 0 else 0\n",
        "            \n",
        "            score = bp * precision\n",
        "            total_score += score\n",
        "        \n",
        "        return total_score / len(predictions) if len(predictions) > 0 else 0\n",
        "    \n",
        "    # Question Answering Metrics\n",
        "    def f1_score_qa(self, predictions: List[str], targets: List[str]) -> float:\n",
        "        \"\"\"Calculate F1 score for QA tasks\"\"\"\n",
        "        total_f1 = 0\n",
        "        \n",
        "        for pred, target in zip(predictions, targets):\n",
        "            pred_tokens = set(pred.lower().split())\n",
        "            target_tokens = set(target.lower().split())\n",
        "            \n",
        "            if len(pred_tokens) == 0 and len(target_tokens) == 0:\n",
        "                f1 = 1.0\n",
        "            elif len(pred_tokens) == 0 or len(target_tokens) == 0:\n",
        "                f1 = 0.0\n",
        "            else:\n",
        "                common = pred_tokens.intersection(target_tokens)\n",
        "                precision = len(common) / len(pred_tokens)\n",
        "                recall = len(common) / len(target_tokens)\n",
        "                \n",
        "                if precision + recall == 0:\n",
        "                    f1 = 0.0\n",
        "                else:\n",
        "                    f1 = 2 * precision * recall / (precision + recall)\n",
        "            \n",
        "            total_f1 += f1\n",
        "        \n",
        "        return total_f1 / len(predictions) if len(predictions) > 0 else 0\n",
        "    \n",
        "    # Summarization Metrics\n",
        "    def rouge_l_simple(self, predictions: List[str], targets: List[str]) -> float:\n",
        "        \"\"\"Simplified ROUGE-L score\"\"\"\n",
        "        total_rouge = 0\n",
        "        \n",
        "        for pred, target in zip(predictions, targets):\n",
        "            pred_tokens = pred.lower().split()\n",
        "            target_tokens = target.lower().split()\n",
        "            \n",
        "            # Find LCS (Longest Common Subsequence)\n",
        "            lcs_length = self._lcs_length(pred_tokens, target_tokens)\n",
        "            \n",
        "            if len(target_tokens) == 0:\n",
        "                rouge_l = 0\n",
        "            else:\n",
        "                rouge_l = lcs_length / len(target_tokens)\n",
        "            \n",
        "            total_rouge += rouge_l\n",
        "        \n",
        "        return total_rouge / len(predictions) if len(predictions) > 0 else 0\n",
        "    \n",
        "    def _string_similarity(self, s1: str, s2: str) -> float:\n",
        "        \"\"\"Calculate string similarity using simple character overlap\"\"\"\n",
        "        if len(s1) == 0 and len(s2) == 0:\n",
        "            return 1.0\n",
        "        if len(s1) == 0 or len(s2) == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        common_chars = len(set(s1).intersection(set(s2)))\n",
        "        total_chars = len(set(s1).union(set(s2)))\n",
        "        \n",
        "        return common_chars / total_chars if total_chars > 0 else 0\n",
        "    \n",
        "    def _lcs_length(self, seq1: List[str], seq2: List[str]) -> int:\n",
        "        \"\"\"Calculate length of longest common subsequence\"\"\"\n",
        "        m, n = len(seq1), len(seq2)\n",
        "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "        \n",
        "        for i in range(1, m + 1):\n",
        "            for j in range(1, n + 1):\n",
        "                if seq1[i-1] == seq2[j-1]:\n",
        "                    dp[i][j] = dp[i-1][j-1] + 1\n",
        "                else:\n",
        "                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
        "        \n",
        "        return dp[m][n]\n",
        "\n",
        "# Create evaluator instance\n",
        "evaluator = T5EvaluationMetrics()\n",
        "\n",
        "# Test with sample data\n",
        "print(\"T5 EVALUATION METRICS TESTING\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Test classification metrics\n",
        "sentiment_preds = [\"positive\", \"negative\", \"positive\", \"neutral\", \"positive\"]\n",
        "sentiment_targets = [\"positive\", \"negative\", \"neutral\", \"neutral\", \"positive\"]\n",
        "\n",
        "exact_acc = evaluator.exact_match_accuracy(sentiment_preds, sentiment_targets)\n",
        "fuzzy_acc = evaluator.fuzzy_match_accuracy(sentiment_preds, sentiment_targets)\n",
        "\n",
        "print(f\"Sentiment Analysis:\")\n",
        "print(f\"  Predictions: {sentiment_preds}\")\n",
        "print(f\"  Targets:     {sentiment_targets}\")\n",
        "print(f\"  Exact Accuracy: {exact_acc:.3f}\")\n",
        "print(f\"  Fuzzy Accuracy: {fuzzy_acc:.3f}\")\n",
        "print()\n",
        "\n",
        "# Test translation metrics\n",
        "translation_preds = [\"bonjour le monde\", \"au revoir\", \"comment allez vous\"]\n",
        "translation_targets = [\"bonjour monde\", \"au revoir\", \"comment allez-vous\"]\n",
        "\n",
        "bleu = evaluator.bleu_score_simple(translation_preds, translation_targets)\n",
        "print(f\"Translation:\")\n",
        "print(f\"  BLEU Score: {bleu:.3f}\")\n",
        "print()\n",
        "\n",
        "# Test QA metrics\n",
        "qa_preds = [\"Paris\", \"Alexander Bell\", \"Jupiter is largest\"]\n",
        "qa_targets = [\"Paris\", \"Alexander Graham Bell\", \"Jupiter\"]\n",
        "\n",
        "f1 = evaluator.f1_score_qa(qa_preds, qa_targets)\n",
        "print(f\"Question Answering:\")\n",
        "print(f\"  F1 Score: {f1:.3f}\")\n",
        "print()\n",
        "\n",
        "# Test summarization metrics\n",
        "summ_preds = [\"AI learns from data\", \"Deep learning uses neural networks\", \"NLP processes language\"]\n",
        "summ_targets = [\"AI learns from data automatically\", \"Deep learning uses multiple neural network layers\", \"NLP helps computers understand language\"]\n",
        "\n",
        "rouge = evaluator.rouge_l_simple(summ_preds, summ_targets)\n",
        "print(f\"Summarization:\")\n",
        "print(f\"  ROUGE-L Score: {rouge:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
