{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Text Processing Fundamentals\n",
        "\n",
        "## Learning Objectives\n",
        "- Master text tokenization and preprocessing techniques\n",
        "- Implement text cleaning pipelines\n",
        "- Understand stop word removal and its impact\n",
        "- Apply basic text clustering methods\n",
        "- Explore text vectorization techniques\n",
        "\n",
        "## Why Text Processing Matters\n",
        "Text processing is essential for:\n",
        "- Natural Language Processing (NLP) applications\n",
        "- Information retrieval systems\n",
        "- Sentiment analysis\n",
        "- Document classification\n",
        "- Chatbots and language models\n",
        "\n",
        "Text data is unstructured and requires preprocessing to convert it into numerical representations that machine learning algorithms can work with.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "from collections import Counter, defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Text processing libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Scikit-learn for text processing and clustering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "print(\"Libraries imported and NLTK data downloaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Sample Text Data\n",
        "\n",
        "Let's create some sample text data to work with, simulating different types of documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample documents for text processing\n",
        "documents = [\n",
        "    \"Machine learning is a subset of artificial intelligence that focuses on algorithms and statistical models.\",\n",
        "    \"Deep learning uses neural networks with multiple layers to learn patterns in data automatically.\",\n",
        "    \"Natural language processing enables computers to understand, interpret, and generate human language.\",\n",
        "    \"Computer vision allows machines to interpret and understand visual information from the world around them.\",\n",
        "    \"Data science combines statistics, programming, and domain expertise to extract insights from data.\",\n",
        "    \"Python is a popular programming language for data science, machine learning, and web development.\",\n",
        "    \"Statistical analysis helps us understand patterns, relationships, and trends in datasets.\",\n",
        "    \"Big data technologies handle large volumes of structured and unstructured data efficiently.\",\n",
        "    \"Cloud computing provides scalable and flexible computing resources over the internet.\",\n",
        "    \"Artificial intelligence aims to create systems that can perform tasks typically requiring human intelligence.\",\n",
        "    \"The weather today is sunny and warm, perfect for outdoor activities and sports.\",\n",
        "    \"Cooking delicious meals requires fresh ingredients, proper techniques, and creative recipes.\",\n",
        "    \"Travel planning involves researching destinations, booking accommodations, and creating itineraries.\",\n",
        "    \"Sports activities promote physical fitness, teamwork, and mental well-being for participants.\",\n",
        "    \"Music and art inspire creativity, emotional expression, and cultural understanding across societies.\"\n",
        "]\n",
        "\n",
        "# Create labels for different topics\n",
        "labels = [\n",
        "    'Technology', 'Technology', 'Technology', 'Technology', 'Technology',\n",
        "    'Technology', 'Technology', 'Technology', 'Technology', 'Technology',\n",
        "    'Lifestyle', 'Lifestyle', 'Lifestyle', 'Lifestyle', 'Lifestyle'\n",
        "]\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'document': documents,\n",
        "    'category': labels,\n",
        "    'doc_id': range(len(documents))\n",
        "})\n",
        "\n",
        "print(f\"Created dataset with {len(documents)} documents\")\n",
        "print(f\"Categories: {set(labels)}\")\n",
        "print(\"\\nFirst few documents:\")\n",
        "for i, doc in enumerate(documents[:3]):\n",
        "    print(f\"{i+1}. {doc}\")\n",
        "    \n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Tokenization\n",
        "\n",
        "Tokenization is the process of breaking text into individual words or tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example text for tokenization\n",
        "sample_text = \"Hello, World! This is a sample sentence. It contains punctuation, numbers like 123, and symbols @#$.\"\n",
        "\n",
        "print(\"Original text:\")\n",
        "print(sample_text)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# 1. Word tokenization using NLTK\n",
        "nltk_tokens = word_tokenize(sample_text)\n",
        "print(f\"\\n1. NLTK Word Tokenization ({len(nltk_tokens)} tokens):\")\n",
        "print(nltk_tokens)\n",
        "\n",
        "# 2. Simple split tokenization\n",
        "simple_tokens = sample_text.split()\n",
        "print(f\"\\n2. Simple Split Tokenization ({len(simple_tokens)} tokens):\")\n",
        "print(simple_tokens)\n",
        "\n",
        "# 3. Regular expression tokenization\n",
        "regex_tokens = re.findall(r'\\b\\w+\\b', sample_text)\n",
        "print(f\"\\n3. Regex Tokenization ({len(regex_tokens)} tokens):\")\n",
        "print(regex_tokens)\n",
        "\n",
        "# 4. Sentence tokenization\n",
        "sentences = sent_tokenize(sample_text)\n",
        "print(f\"\\n4. Sentence Tokenization ({len(sentences)} sentences):\")\n",
        "for i, sent in enumerate(sentences):\n",
        "    print(f\"   {i+1}. {sent}\")\n",
        "\n",
        "# Compare tokenization methods\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"COMPARISON:\")\n",
        "print(f\"NLTK preserves punctuation: {any(token in string.punctuation for token in nltk_tokens)}\")\n",
        "print(f\"Simple split keeps punctuation attached: {'World!' in simple_tokens}\")\n",
        "print(f\"Regex extracts only word characters: {regex_tokens}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Text Cleaning Pipeline\n",
        "\n",
        "Text cleaning involves removing or standardizing elements that might interfere with analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def comprehensive_text_cleaner(text):\n",
        "    \"\"\"Comprehensive text cleaning function\"\"\"\n",
        "    # Store original for comparison\n",
        "    original = text\n",
        "    \n",
        "    # Step 1: Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Step 2: Remove URLs\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "    \n",
        "    # Step 3: Remove email addresses\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "    \n",
        "    # Step 4: Remove numbers (optional - depends on use case)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    \n",
        "    # Step 5: Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    \n",
        "    # Step 6: Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "    # Step 7: Strip leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Demonstrate cleaning pipeline\n",
        "messy_text = \"\"\"\n",
        "   Hello!!! This is a MESSY text with NUMBERS 123, URLs like https://example.com, \n",
        "   emails like user@domain.com, and    too much    whitespace... \n",
        "   It also has @special #symbols and unnecessary punctuation!!! \n",
        "\"\"\".strip()\n",
        "\n",
        "print(\"Original messy text:\")\n",
        "print(repr(messy_text))\n",
        "print(f\"Length: {len(messy_text)}\")\n",
        "\n",
        "cleaned_text = comprehensive_text_cleaner(messy_text)\n",
        "print(\"\\nCleaned text:\")\n",
        "print(repr(cleaned_text))\n",
        "print(f\"Length: {len(cleaned_text)}\")\n",
        "\n",
        "# Apply cleaning to our document collection\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLEANING DOCUMENT COLLECTION:\")\n",
        "\n",
        "df['cleaned_document'] = df['document'].apply(comprehensive_text_cleaner)\n",
        "\n",
        "# Show before and after for first few documents\n",
        "for i in range(3):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(f\"Original:  {df.iloc[i]['document']}\")\n",
        "    print(f\"Cleaned:   {df.iloc[i]['cleaned_document']}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
