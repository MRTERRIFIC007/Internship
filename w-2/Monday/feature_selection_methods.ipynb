{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Feature Selection Methods - Filter vs Wrapper vs Embedded\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand different types of feature selection methods\n",
        "- Implement filter methods using statistical tests\n",
        "- Apply wrapper methods like Recursive Feature Elimination\n",
        "- Use embedded methods with regularization\n",
        "- Compare PCA for dimensionality reduction\n",
        "\n",
        "## Theory Overview\n",
        "\n",
        "Feature selection is crucial for:\n",
        "- Reducing overfitting\n",
        "- Improving model performance\n",
        "- Reducing computational cost\n",
        "- Better interpretability\n",
        "\n",
        "### Types of Feature Selection:\n",
        "1. **Filter Methods**: Statistical measures independent of ML algorithm\n",
        "2. **Wrapper Methods**: Use ML algorithm to evaluate feature subsets\n",
        "3. **Embedded Methods**: Feature selection during model training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer, make_classification\n",
        "from sklearn.feature_selection import (\n",
        "    SelectKBest, f_classif, chi2, mutual_info_classif,\n",
        "    RFE, RFECV, SelectFromModel\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression, Lasso\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "Let's use the breast cancer dataset and create some additional noisy features to demonstrate feature selection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Create feature names\n",
        "feature_names = data.feature_names\n",
        "\n",
        "print(f\"Original dataset shape: {X.shape}\")\n",
        "print(f\"Number of classes: {len(np.unique(y))}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")\n",
        "\n",
        "# Add some noisy features to make feature selection more meaningful\n",
        "np.random.seed(42)\n",
        "noise_features = np.random.randn(X.shape[0], 20)\n",
        "X_with_noise = np.hstack([X, noise_features])\n",
        "\n",
        "# Update feature names\n",
        "noise_names = [f'noise_feature_{i}' for i in range(20)]\n",
        "all_feature_names = list(feature_names) + noise_names\n",
        "\n",
        "print(f\"Dataset with noise shape: {X_with_noise.shape}\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_with_noise, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Filter Methods\n",
        "\n",
        "Filter methods evaluate features independently using statistical measures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardize features for statistical tests\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 1.1 ANOVA F-test\n",
        "print(\"=== ANOVA F-test (Filter Method) ===\")\n",
        "f_selector = SelectKBest(score_func=f_classif, k=15)\n",
        "X_train_f = f_selector.fit_transform(X_train_scaled, y_train)\n",
        "X_test_f = f_selector.transform(X_test_scaled)\n",
        "\n",
        "# Get selected features\n",
        "f_selected_features = f_selector.get_support(indices=True)\n",
        "f_scores = f_selector.scores_\n",
        "\n",
        "print(f\"Selected {len(f_selected_features)} features using F-test\")\n",
        "print(\"Top 10 features by F-score:\")\n",
        "feature_scores_f = list(zip(all_feature_names, f_scores))\n",
        "feature_scores_f.sort(key=lambda x: x[1], reverse=True)\n",
        "for name, score in feature_scores_f[:10]:\n",
        "    print(f\"  {name}: {score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.2 Mutual Information\n",
        "print(\"\\n=== Mutual Information (Filter Method) ===\")\n",
        "mi_selector = SelectKBest(score_func=mutual_info_classif, k=15)\n",
        "X_train_mi = mi_selector.fit_transform(X_train_scaled, y_train)\n",
        "X_test_mi = mi_selector.transform(X_test_scaled)\n",
        "\n",
        "mi_selected_features = mi_selector.get_support(indices=True)\n",
        "mi_scores = mi_selector.scores_\n",
        "\n",
        "print(f\"Selected {len(mi_selected_features)} features using Mutual Information\")\n",
        "print(\"Top 10 features by MI score:\")\n",
        "feature_scores_mi = list(zip(all_feature_names, mi_scores))\n",
        "feature_scores_mi.sort(key=lambda x: x[1], reverse=True)\n",
        "for name, score in feature_scores_mi[:10]:\n",
        "    print(f\"  {name}: {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize filter method results\n",
        "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
        "\n",
        "# F-test scores\n",
        "axes[0].bar(range(len(f_scores)), f_scores, alpha=0.7)\n",
        "axes[0].set_title('ANOVA F-test Scores for All Features')\n",
        "axes[0].set_xlabel('Feature Index')\n",
        "axes[0].set_ylabel('F-score')\n",
        "axes[0].axhline(y=np.mean(f_scores), color='r', linestyle='--', label='Mean F-score')\n",
        "axes[0].legend()\n",
        "\n",
        "# Mutual Information scores\n",
        "axes[1].bar(range(len(mi_scores)), mi_scores, alpha=0.7, color='orange')\n",
        "axes[1].set_title('Mutual Information Scores for All Features')\n",
        "axes[1].set_xlabel('Feature Index')\n",
        "axes[1].set_ylabel('MI Score')\n",
        "axes[1].axhline(y=np.mean(mi_scores), color='r', linestyle='--', label='Mean MI score')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Note: Features 30-49 are the noise features we added\n",
        "print(\"\\nNote: Features 30-49 are artificially added noise features\")\n",
        "print(\"Good feature selection should rank them lower!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Wrapper Methods\n",
        "\n",
        "Wrapper methods use a machine learning algorithm to evaluate feature subsets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.1 Recursive Feature Elimination (RFE)\n",
        "print(\"=== Recursive Feature Elimination (Wrapper Method) ===\")\n",
        "\n",
        "# Use logistic regression as the estimator\n",
        "estimator = LogisticRegression(random_state=42, max_iter=1000)\n",
        "rfe_selector = RFE(estimator=estimator, n_features_to_select=15)\n",
        "X_train_rfe = rfe_selector.fit_transform(X_train_scaled, y_train)\n",
        "X_test_rfe = rfe_selector.transform(X_test_scaled)\n",
        "\n",
        "rfe_selected_features = rfe_selector.get_support(indices=True)\n",
        "rfe_ranking = rfe_selector.ranking_\n",
        "\n",
        "print(f\"Selected {len(rfe_selected_features)} features using RFE\")\n",
        "print(\"Selected features:\")\n",
        "for idx in rfe_selected_features:\n",
        "    print(f\"  {all_feature_names[idx]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.2 Recursive Feature Elimination with Cross-Validation (RFECV)\n",
        "print(\"\\n=== RFE with Cross-Validation (Wrapper Method) ===\")\n",
        "\n",
        "rfecv_selector = RFECV(\n",
        "    estimator=LogisticRegression(random_state=42, max_iter=1000),\n",
        "    step=1,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "X_train_rfecv = rfecv_selector.fit_transform(X_train_scaled, y_train)\n",
        "X_test_rfecv = rfecv_selector.transform(X_test_scaled)\n",
        "\n",
        "rfecv_selected_features = rfecv_selector.get_support(indices=True)\n",
        "\n",
        "print(f\"Optimal number of features: {rfecv_selector.n_features_}\")\n",
        "print(f\"Selected {len(rfecv_selected_features)} features using RFECV\")\n",
        "\n",
        "# Plot RFECV results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(rfecv_selector.cv_results_['mean_test_score']) + 1), \n",
        "         rfecv_selector.cv_results_['mean_test_score'])\n",
        "plt.xlabel('Number of Features Selected')\n",
        "plt.ylabel('Cross-Validation Accuracy')\n",
        "plt.title('RFECV: Optimal Number of Features')\n",
        "plt.axvline(x=rfecv_selector.n_features_, color='r', linestyle='--', \n",
        "           label=f'Optimal: {rfecv_selector.n_features_} features')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Embedded Methods\n",
        "\n",
        "Embedded methods perform feature selection during model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.1 LASSO Regularization (L1)\n",
        "print(\"=== LASSO Regularization (Embedded Method) ===\")\n",
        "\n",
        "# Try different alpha values\n",
        "alphas = [0.001, 0.01, 0.1, 1.0]\n",
        "lasso_results = {}\n",
        "\n",
        "for alpha in alphas:\n",
        "    lasso = Lasso(alpha=alpha, random_state=42, max_iter=1000)\n",
        "    lasso.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Count non-zero coefficients\n",
        "    n_selected = np.sum(lasso.coef_ != 0)\n",
        "    lasso_results[alpha] = {\n",
        "        'n_features': n_selected,\n",
        "        'coefficients': lasso.coef_\n",
        "    }\n",
        "    \n",
        "    print(f\"Alpha={alpha}: {n_selected} features selected\")\n",
        "\n",
        "# Use SelectFromModel with LASSO\n",
        "lasso_selector = SelectFromModel(Lasso(alpha=0.01, random_state=42, max_iter=1000))\n",
        "X_train_lasso = lasso_selector.fit_transform(X_train_scaled, y_train)\n",
        "X_test_lasso = lasso_selector.transform(X_test_scaled)\n",
        "\n",
        "lasso_selected_features = lasso_selector.get_support(indices=True)\n",
        "print(f\"\\nSelected {len(lasso_selected_features)} features using LASSO (alpha=0.01)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.2 Random Forest Feature Importance\n",
        "print(\"\\n=== Random Forest Feature Importance (Embedded Method) ===\")\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf.feature_importances_\n",
        "\n",
        "# Use SelectFromModel\n",
        "rf_selector = SelectFromModel(rf, prefit=True)\n",
        "X_train_rf = rf_selector.transform(X_train_scaled)\n",
        "X_test_rf = rf_selector.transform(X_test_scaled)\n",
        "\n",
        "rf_selected_features = rf_selector.get_support(indices=True)\n",
        "print(f\"Selected {len(rf_selected_features)} features using Random Forest importance\")\n",
        "\n",
        "# Show top features by importance\n",
        "print(\"Top 10 features by Random Forest importance:\")\n",
        "feature_importance_pairs = list(zip(all_feature_names, feature_importances))\n",
        "feature_importance_pairs.sort(key=lambda x: x[1], reverse=True)\n",
        "for name, importance in feature_importance_pairs[:10]:\n",
        "    print(f\"  {name}: {importance:.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Principal Component Analysis (PCA)\n",
        "\n",
        "PCA is a dimensionality reduction technique that creates new features (principal components) that are linear combinations of original features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply PCA\n",
        "print(\"=== Principal Component Analysis (PCA) ===\")\n",
        "\n",
        "pca = PCA(n_components=15)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Total explained variance: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
        "\n",
        "# Plot explained variance\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), \n",
        "        pca.explained_variance_ratio_)\n",
        "plt.title('Explained Variance by Principal Component')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
        "         np.cumsum(pca.explained_variance_ratio_), 'bo-')\n",
        "plt.title('Cumulative Explained Variance')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find number of components for 95% variance\n",
        "cumsum_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "n_components_95 = np.argmax(cumsum_variance >= 0.95) + 1\n",
        "print(f\"\\nNumber of components needed for 95% variance: {n_components_95}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Key Insights and Reflections\n",
        "\n",
        "### What I Learned Today:\n",
        "\n",
        "1. **Filter Methods** (F-test, Mutual Information):\n",
        "   - Fast and model-agnostic\n",
        "   - Good for initial feature screening\n",
        "   - May miss feature interactions\n",
        "\n",
        "2. **Wrapper Methods** (RFE, RFECV):\n",
        "   - Consider feature interactions\n",
        "   - More computationally expensive\n",
        "   - Better performance but risk of overfitting\n",
        "\n",
        "3. **Embedded Methods** (LASSO, Random Forest):\n",
        "   - Feature selection during training\n",
        "   - Good balance of performance and efficiency\n",
        "   - Model-specific selection\n",
        "\n",
        "4. **PCA**:\n",
        "   - Creates new features rather than selecting existing ones\n",
        "   - Good for reducing dimensionality\n",
        "   - May lose interpretability\n",
        "\n",
        "### Practical Considerations:\n",
        "- Start with filter methods for quick screening\n",
        "- Use wrapper methods when computational cost is acceptable\n",
        "- Consider embedded methods for specific algorithms\n",
        "- PCA when interpretability is less important than dimensionality reduction\n",
        "\n",
        "### Tomorrow's Preview:\n",
        "We'll explore neural networks, starting with perceptron architecture and moving to more complex topics like self-supervised learning!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
