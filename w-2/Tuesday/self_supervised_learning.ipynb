{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Self-Supervised Learning: Contrastive Methods\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand self-supervised learning paradigms\n",
        "- Implement contrastive learning principles\n",
        "- Compare SimCLR and MoCo frameworks\n",
        "- Build simple contrastive learning models\n",
        "- Explore data augmentation strategies\n",
        "- Understand momentum-based learning\n",
        "\n",
        "## Theory Overview\n",
        "\n",
        "### Self-Supervised Learning\n",
        "- Learn representations without explicit labels\n",
        "- Use pretext tasks to create supervisory signals\n",
        "- Popular in computer vision and NLP\n",
        "\n",
        "### Contrastive Learning\n",
        "- **Positive pairs**: Similar samples (augmented versions)\n",
        "- **Negative pairs**: Different samples\n",
        "- **Goal**: Pull positives together, push negatives apart\n",
        "\n",
        "### Key Frameworks:\n",
        "- **SimCLR**: Simple contrastive learning with large batch sizes\n",
        "- **MoCo**: Momentum contrast with memory bank\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Data Augmentation for Contrastive Learning\n",
        "\n",
        "First, let's implement data augmentation strategies that create positive pairs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleDataAugmentation:\n",
        "    \"\"\"Simple data augmentation for tabular/image-like data\"\"\"\n",
        "    \n",
        "    def __init__(self, noise_factor=0.1, scale_factor=0.1):\n",
        "        self.noise_factor = noise_factor\n",
        "        self.scale_factor = scale_factor\n",
        "    \n",
        "    def add_noise(self, x):\n",
        "        \"\"\"Add Gaussian noise\"\"\"\n",
        "        noise = np.random.normal(0, self.noise_factor, x.shape)\n",
        "        return x + noise\n",
        "    \n",
        "    def scale(self, x):\n",
        "        \"\"\"Random scaling\"\"\"\n",
        "        scale = np.random.normal(1, self.scale_factor)\n",
        "        return x * scale\n",
        "    \n",
        "    def rotate_2d(self, x):\n",
        "        \"\"\"Simple 2D rotation for 2D data\"\"\"\n",
        "        if x.shape[-1] != 2:\n",
        "            return x\n",
        "        \n",
        "        angle = np.random.uniform(-np.pi/6, np.pi/6)  # Â±30 degrees\n",
        "        cos_angle, sin_angle = np.cos(angle), np.sin(angle)\n",
        "        rotation_matrix = np.array([[cos_angle, -sin_angle],\n",
        "                                   [sin_angle, cos_angle]])\n",
        "        \n",
        "        return np.dot(x, rotation_matrix.T)\n",
        "    \n",
        "    def augment(self, x):\n",
        "        \"\"\"Apply random augmentation\"\"\"\n",
        "        x = x.copy()\n",
        "        \n",
        "        # Apply random combination of augmentations\n",
        "        if np.random.random() > 0.5:\n",
        "            x = self.add_noise(x)\n",
        "        if np.random.random() > 0.5:\n",
        "            x = self.scale(x)\n",
        "        if x.shape[-1] == 2 and np.random.random() > 0.5:\n",
        "            x = self.rotate_2d(x)\n",
        "            \n",
        "        return x\n",
        "\n",
        "# Create sample dataset\n",
        "print(\"Creating sample dataset...\")\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, \n",
        "    n_features=2, \n",
        "    n_redundant=0, \n",
        "    n_informative=2,\n",
        "    n_clusters_per_class=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Number of classes: {len(np.unique(y))}\")\n",
        "\n",
        "# Initialize augmentation\n",
        "augmenter = SimpleDataAugmentation(noise_factor=0.1, scale_factor=0.05)\n",
        "\n",
        "# Demonstrate augmentations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# Original data\n",
        "axes[0, 0].scatter(X[:, 0], X[:, 1], c=y, alpha=0.6, cmap='viridis')\n",
        "axes[0, 0].set_title('Original Data')\n",
        "axes[0, 0].set_xlabel('Feature 1')\n",
        "axes[0, 0].set_ylabel('Feature 2')\n",
        "\n",
        "# Different augmentations\n",
        "augmentation_names = ['Noise', 'Scale', 'Rotation', 'Combined', 'Multiple Augmented']\n",
        "\n",
        "for i, name in enumerate(augmentation_names):\n",
        "    row = i // 3\n",
        "    col = (i + 1) % 3\n",
        "    \n",
        "    if name == 'Noise':\n",
        "        X_aug = augmenter.add_noise(X)\n",
        "    elif name == 'Scale':\n",
        "        X_aug = augmenter.scale(X)\n",
        "    elif name == 'Rotation':\n",
        "        X_aug = augmenter.rotate_2d(X)\n",
        "    elif name == 'Combined':\n",
        "        X_aug = augmenter.augment(X)\n",
        "    else:  # Multiple augmented\n",
        "        X_aug = np.vstack([augmenter.augment(X) for _ in range(3)])\n",
        "        y_aug = np.tile(y, 3)\n",
        "        axes[row, col].scatter(X_aug[:, 0], X_aug[:, 1], c=y_aug, alpha=0.6, cmap='viridis')\n",
        "        axes[row, col].set_title(f'{name} (3x data)')\n",
        "        axes[row, col].set_xlabel('Feature 1')\n",
        "        axes[row, col].set_ylabel('Feature 2')\n",
        "        continue\n",
        "    \n",
        "    axes[row, col].scatter(X_aug[:, 0], X_aug[:, 1], c=y, alpha=0.6, cmap='viridis')\n",
        "    axes[row, col].set_title(f'{name} Augmentation')\n",
        "    axes[row, col].set_xlabel('Feature 1')\n",
        "    axes[row, col].set_ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Data augmentation strategies demonstrated!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Simple Contrastive Learning Implementation\n",
        "\n",
        "Let's implement a basic contrastive learning framework inspired by SimCLR.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleContrastiveLearner:\n",
        "    \"\"\"Simple contrastive learning implementation\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, hidden_dim=64, output_dim=32, temperature=0.1, learning_rate=0.01):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.temperature = temperature\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # Initialize encoder network (simple 2-layer MLP)\n",
        "        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.1\n",
        "        self.b1 = np.zeros((1, hidden_dim))\n",
        "        self.W2 = np.random.randn(hidden_dim, output_dim) * 0.1\n",
        "        self.b2 = np.zeros((1, output_dim))\n",
        "        \n",
        "        self.training_losses = []\n",
        "        \n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "    \n",
        "    def l2_normalize(self, x, axis=-1):\n",
        "        \"\"\"L2 normalize along specified axis\"\"\"\n",
        "        norm = np.linalg.norm(x, axis=axis, keepdims=True)\n",
        "        return x / (norm + 1e-8)\n",
        "    \n",
        "    def encode(self, x):\n",
        "        \"\"\"Encode input to representation space\"\"\"\n",
        "        # Forward pass through encoder\n",
        "        h1 = self.relu(np.dot(x, self.W1) + self.b1)\n",
        "        h2 = np.dot(h1, self.W2) + self.b2\n",
        "        \n",
        "        # L2 normalize the output\n",
        "        return self.l2_normalize(h2)\n",
        "    \n",
        "    def cosine_similarity(self, a, b):\n",
        "        \"\"\"Compute cosine similarity between normalized vectors\"\"\"\n",
        "        return np.dot(a, b.T)\n",
        "    \n",
        "    def contrastive_loss(self, z_i, z_j, batch_size):\n",
        "        \"\"\"NT-Xent loss (Normalized Temperature-scaled Cross Entropy)\"\"\"\n",
        "        # Concatenate positive pairs\n",
        "        z = np.concatenate([z_i, z_j], axis=0)  # Shape: (2*batch_size, output_dim)\n",
        "        \n",
        "        # Compute similarity matrix\n",
        "        sim_matrix = self.cosine_similarity(z, z) / self.temperature\n",
        "        \n",
        "        # Create masks for positive pairs\n",
        "        batch_size = z_i.shape[0]\n",
        "        mask = np.eye(2 * batch_size, dtype=bool)\n",
        "        \n",
        "        # Positive pairs: (i, i+batch_size) and (i+batch_size, i)\n",
        "        pos_mask = np.zeros((2 * batch_size, 2 * batch_size), dtype=bool)\n",
        "        for i in range(batch_size):\n",
        "            pos_mask[i, i + batch_size] = True\n",
        "            pos_mask[i + batch_size, i] = True\n",
        "        \n",
        "        # Remove self-similarities\n",
        "        sim_matrix = sim_matrix[~mask].reshape(2 * batch_size, -1)\n",
        "        pos_sim = sim_matrix[pos_mask[~mask].reshape(2 * batch_size, -1)]\n",
        "        \n",
        "        # Compute loss (simplified version)\n",
        "        numerator = np.exp(pos_sim)\n",
        "        denominator = np.sum(np.exp(sim_matrix), axis=1)\n",
        "        loss = -np.mean(np.log(numerator / denominator))\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def simplified_contrastive_loss(self, z_i, z_j):\n",
        "        \"\"\"Simplified contrastive loss for easier implementation\"\"\"\n",
        "        batch_size = z_i.shape[0]\n",
        "        \n",
        "        # Positive similarity (between augmented pairs)\n",
        "        pos_sim = np.sum(z_i * z_j, axis=1) / self.temperature\n",
        "        \n",
        "        # Negative similarities (all other pairs)\n",
        "        neg_sim_i = np.dot(z_i, z_j.T) / self.temperature\n",
        "        neg_sim_j = np.dot(z_j, z_i.T) / self.temperature\n",
        "        \n",
        "        # Remove diagonal (positive pairs)\n",
        "        mask = np.eye(batch_size, dtype=bool)\n",
        "        neg_sim_i = neg_sim_i[~mask].reshape(batch_size, -1)\n",
        "        neg_sim_j = neg_sim_j[~mask].reshape(batch_size, -1)\n",
        "        \n",
        "        # Compute loss\n",
        "        pos_loss = -np.mean(pos_sim)\n",
        "        neg_loss = np.mean(np.log(np.sum(np.exp(neg_sim_i), axis=1) + \n",
        "                                 np.sum(np.exp(neg_sim_j), axis=1)))\n",
        "        \n",
        "        return pos_loss + neg_loss\n",
        "    \n",
        "    def train_step(self, x1, x2):\n",
        "        \"\"\"Single training step\"\"\"\n",
        "        batch_size = x1.shape[0]\n",
        "        \n",
        "        # Forward pass\n",
        "        z1 = self.encode(x1)\n",
        "        z2 = self.encode(x2)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = self.simplified_contrastive_loss(z1, z2)\n",
        "        \n",
        "        # Simple gradient computation (placeholder - in practice use autograd)\n",
        "        # For demonstration, we'll just store the loss\n",
        "        self.training_losses.append(loss)\n",
        "        \n",
        "        return loss, z1, z2\n",
        "    \n",
        "    def get_representations(self, x):\n",
        "        \"\"\"Get learned representations\"\"\"\n",
        "        return self.encode(x)\n",
        "\n",
        "# Demonstrate contrastive learning\n",
        "print(\"=== Contrastive Learning Training ===\")\n",
        "\n",
        "# Create positive pairs through augmentation\n",
        "batch_size = 64\n",
        "n_batches = 50\n",
        "\n",
        "learner = SimpleContrastiveLearner(\n",
        "    input_dim=X.shape[1],\n",
        "    hidden_dim=32,\n",
        "    output_dim=16,\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "for batch in range(n_batches):\n",
        "    # Sample random batch\n",
        "    indices = np.random.choice(X.shape[0], batch_size, replace=True)\n",
        "    x_batch = X[indices]\n",
        "    \n",
        "    # Create positive pairs through augmentation\n",
        "    x1 = augmenter.augment(x_batch)\n",
        "    x2 = augmenter.augment(x_batch)\n",
        "    \n",
        "    # Training step\n",
        "    loss, z1, z2 = learner.train_step(x1, x2)\n",
        "    \n",
        "    if batch % 10 == 0:\n",
        "        print(f\"Batch {batch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Plot training progress\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(learner.training_losses)\n",
        "plt.title('Contrastive Learning Training Loss')\n",
        "plt.xlabel('Batch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"Contrastive learning training completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Key Insights and Reflections\n",
        "\n",
        "### What I Learned About Self-Supervised Learning:\n",
        "\n",
        "1. **Contrastive Learning**: Learn by comparing similar vs different samples\n",
        "2. **Data Augmentation**: Critical for creating positive pairs\n",
        "3. **Temperature Scaling**: Controls the sharpness of similarity distributions\n",
        "4. **Representation Quality**: Good representations cluster similar samples\n",
        "\n",
        "### SimCLR vs MoCo Comparison:\n",
        "- **SimCLR**: Requires large batch sizes, simpler architecture\n",
        "- **MoCo**: Uses momentum updates and memory bank, more memory efficient\n",
        "- **Both**: Focus on learning invariant representations\n",
        "\n",
        "### Tomorrow's Preview:\n",
        "We'll explore manifold learning with t-SNE and advanced training strategies!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
