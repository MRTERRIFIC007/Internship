{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Neural Network Architecture - From Perceptron to MLP\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the fundamental building blocks of neural networks\n",
        "- Implement a perceptron from scratch\n",
        "- Build multi-layer perceptrons (MLPs) \n",
        "- Explore weights, biases, and their roles\n",
        "- Compare different network architectures\n",
        "- Visualize network decision boundaries\n",
        "\n",
        "## Theory Overview\n",
        "\n",
        "### The Perceptron (1957)\n",
        "- Simplest form of artificial neural network\n",
        "- Linear classifier with step activation function\n",
        "- Foundation for modern deep learning\n",
        "\n",
        "### Multi-Layer Perceptron (MLP)\n",
        "- Multiple layers of perceptrons\n",
        "- Can learn non-linear relationships\n",
        "- Universal function approximator (with enough neurons)\n",
        "\n",
        "### Key Components:\n",
        "- **Weights**: Control signal strength between neurons\n",
        "- **Biases**: Allow shifting of activation function\n",
        "- **Activation Functions**: Introduce non-linearity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, make_moons, make_circles\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Building a Perceptron from Scratch\n",
        "\n",
        "Let's implement the simplest neural network - a single perceptron.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Perceptron:\n",
        "    \"\"\"Simple perceptron implementation from scratch\"\"\"\n",
        "    \n",
        "    def __init__(self, learning_rate=0.01, max_epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_epochs = max_epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.training_errors = []\n",
        "        \n",
        "    def activation_function(self, x):\n",
        "        \"\"\"Step function activation\"\"\"\n",
        "        return np.where(x >= 0, 1, 0)\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the perceptron\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # Initialize weights and bias\n",
        "        self.weights = np.random.normal(0, 0.1, n_features)\n",
        "        self.bias = 0\n",
        "        \n",
        "        # Training loop\n",
        "        for epoch in range(self.max_epochs):\n",
        "            errors = 0\n",
        "            \n",
        "            for i in range(n_samples):\n",
        "                # Forward pass\n",
        "                linear_output = np.dot(X[i], self.weights) + self.bias\n",
        "                prediction = self.activation_function(linear_output)\n",
        "                \n",
        "                # Calculate error\n",
        "                error = y[i] - prediction\n",
        "                \n",
        "                # Update weights and bias if there's an error\n",
        "                if error != 0:\n",
        "                    self.weights += self.learning_rate * error * X[i]\n",
        "                    self.bias += self.learning_rate * error\n",
        "                    errors += 1\n",
        "            \n",
        "            self.training_errors.append(errors)\n",
        "            \n",
        "            # Stop if no errors (convergence)\n",
        "            if errors == 0:\n",
        "                print(f\"Converged after {epoch + 1} epochs\")\n",
        "                break\n",
        "                \n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return self.activation_function(linear_output)\n",
        "    \n",
        "    def decision_function(self, X):\n",
        "        \"\"\"Return decision function values\"\"\"\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "# Test perceptron on linearly separable data\n",
        "print(\"=== Testing Perceptron on Linearly Separable Data ===\")\n",
        "\n",
        "# Create simple linearly separable dataset\n",
        "np.random.seed(42)\n",
        "X_simple = np.array([\n",
        "    [0, 0], [0, 1], [1, 0], [1, 1],\n",
        "    [0.1, 0.1], [0.1, 0.9], [0.9, 0.1], [0.9, 0.9]\n",
        "])\n",
        "y_simple = np.array([0, 0, 0, 1, 0, 0, 0, 1])  # AND-like function\n",
        "\n",
        "# Train perceptron\n",
        "perceptron = Perceptron(learning_rate=0.1, max_epochs=100)\n",
        "perceptron.fit(X_simple, y_simple)\n",
        "\n",
        "# Make predictions\n",
        "predictions = perceptron.predict(X_simple)\n",
        "accuracy = accuracy_score(y_simple, predictions)\n",
        "\n",
        "print(f\"Final weights: {perceptron.weights}\")\n",
        "print(f\"Final bias: {perceptron.bias}\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Predictions: {predictions}\")\n",
        "print(f\"True labels: {y_simple}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize perceptron decision boundary and training process\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Plot 1: Training error over epochs\n",
        "axes[0].plot(perceptron.training_errors, linewidth=2)\n",
        "axes[0].set_title('Training Errors Over Epochs')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Number of Errors')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Data points and decision boundary\n",
        "x_min, x_max = X_simple[:, 0].min() - 0.1, X_simple[:, 0].max() + 0.1\n",
        "y_min, y_max = X_simple[:, 1].min() - 0.1, X_simple[:, 1].max() + 0.1\n",
        "\n",
        "# Create mesh for decision boundary\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
        "                     np.linspace(y_min, y_max, 100))\n",
        "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "Z = perceptron.decision_function(mesh_points)\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot decision boundary\n",
        "axes[1].contour(xx, yy, Z, levels=[0], colors='red', linestyles='--', linewidths=2)\n",
        "axes[1].contourf(xx, yy, Z, levels=50, alpha=0.3, cmap='RdYlBu')\n",
        "\n",
        "# Plot data points\n",
        "scatter = axes[1].scatter(X_simple[:, 0], X_simple[:, 1], c=y_simple, \n",
        "                         cmap='RdYlBu', s=100, edgecolors='black', linewidth=2)\n",
        "axes[1].set_title('Perceptron Decision Boundary')\n",
        "axes[1].set_xlabel('Feature 1')\n",
        "axes[1].set_ylabel('Feature 2')\n",
        "axes[1].colorbar(scatter)\n",
        "\n",
        "# Plot 3: Weight and bias evolution (simplified visualization)\n",
        "axes[2].bar(['Weight 1', 'Weight 2', 'Bias'], \n",
        "           [perceptron.weights[0], perceptron.weights[1], perceptron.bias])\n",
        "axes[2].set_title('Final Learned Parameters')\n",
        "axes[2].set_ylabel('Parameter Value')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Test on XOR problem (non-linearly separable)\n",
        "print(\"\\n=== Testing Perceptron on XOR Problem (Non-linearly Separable) ===\")\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([0, 1, 1, 0])  # XOR function\n",
        "\n",
        "perceptron_xor = Perceptron(learning_rate=0.1, max_epochs=100)\n",
        "perceptron_xor.fit(X_xor, y_xor)\n",
        "\n",
        "predictions_xor = perceptron_xor.predict(X_xor)\n",
        "accuracy_xor = accuracy_score(y_xor, predictions_xor)\n",
        "\n",
        "print(f\"XOR Accuracy: {accuracy_xor:.2f}\")\n",
        "print(f\"XOR Predictions: {predictions_xor}\")\n",
        "print(f\"XOR True labels: {y_xor}\")\n",
        "print(\"Note: Perceptron cannot solve XOR problem (non-linearly separable)!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Activation Functions\n",
        "\n",
        "Before building MLPs, let's understand different activation functions and their properties.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define common activation functions\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation function\"\"\"\n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip to prevent overflow\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    \"\"\"Derivative of sigmoid\"\"\"\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def tanh(x):\n",
        "    \"\"\"Hyperbolic tangent activation function\"\"\"\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    \"\"\"Derivative of tanh\"\"\"\n",
        "    return 1 - np.tanh(x)**2\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"ReLU activation function\"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    \"\"\"Derivative of ReLU\"\"\"\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    \"\"\"Leaky ReLU activation function\"\"\"\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "def leaky_relu_derivative(x, alpha=0.01):\n",
        "    \"\"\"Derivative of Leaky ReLU\"\"\"\n",
        "    return np.where(x > 0, 1, alpha)\n",
        "\n",
        "# Visualize activation functions and their derivatives\n",
        "x = np.linspace(-5, 5, 1000)\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "\n",
        "# Row 1: Activation functions\n",
        "activation_functions = [\n",
        "    (sigmoid, \"Sigmoid\"),\n",
        "    (tanh, \"Tanh\"),\n",
        "    (relu, \"ReLU\"),\n",
        "    (lambda x: leaky_relu(x, 0.1), \"Leaky ReLU\")\n",
        "]\n",
        "\n",
        "# Row 2: Derivatives\n",
        "derivative_functions = [\n",
        "    (sigmoid_derivative, \"Sigmoid Derivative\"),\n",
        "    (tanh_derivative, \"Tanh Derivative\"),\n",
        "    (relu_derivative, \"ReLU Derivative\"),\n",
        "    (lambda x: leaky_relu_derivative(x, 0.1), \"Leaky ReLU Derivative\")\n",
        "]\n",
        "\n",
        "for i, (func, name) in enumerate(activation_functions):\n",
        "    y = func(x)\n",
        "    axes[0, i].plot(x, y, linewidth=2, label=name)\n",
        "    axes[0, i].set_title(f'{name} Function')\n",
        "    axes[0, i].set_xlabel('Input')\n",
        "    axes[0, i].set_ylabel('Output')\n",
        "    axes[0, i].grid(True, alpha=0.3)\n",
        "    axes[0, i].legend()\n",
        "\n",
        "for i, (func, name) in enumerate(derivative_functions):\n",
        "    y = func(x)\n",
        "    axes[1, i].plot(x, y, linewidth=2, label=name, color='orange')\n",
        "    axes[1, i].set_title(f'{name}')\n",
        "    axes[1, i].set_xlabel('Input')\n",
        "    axes[1, i].set_ylabel('Derivative')\n",
        "    axes[1, i].grid(True, alpha=0.3)\n",
        "    axes[1, i].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare activation function properties\n",
        "print(\"Activation Function Properties:\")\n",
        "print(\"=\"*60)\n",
        "print(\"Function    | Range        | Derivative Range | Zero-Centered | Non-linear\")\n",
        "print(\"-\" * 60)\n",
        "print(\"Sigmoid     | (0, 1)       | (0, 0.25]       | No            | Yes\")\n",
        "print(\"Tanh        | (-1, 1)      | (0, 1]          | Yes           | Yes\")\n",
        "print(\"ReLU        | [0, ∞)       | {0, 1}          | No            | Yes\")\n",
        "print(\"Leaky ReLU  | (-∞, ∞)      | {α, 1}          | No            | Yes\")\n",
        "print(\"\\nKey Insights:\")\n",
        "print(\"- Sigmoid: Saturates (vanishing gradients), output not zero-centered\")\n",
        "print(\"- Tanh: Zero-centered, still saturates\")\n",
        "print(\"- ReLU: Solves vanishing gradient, but can 'die' (always output 0)\")\n",
        "print(\"- Leaky ReLU: Prevents dying ReLU problem\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Multi-Layer Perceptron (MLP) from Scratch\n",
        "\n",
        "Now let's build a full MLP that can solve non-linear problems like XOR.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    \"\"\"Multi-Layer Perceptron implementation from scratch\"\"\"\n",
        "    \n",
        "    def __init__(self, layer_sizes, activation='sigmoid', learning_rate=0.01, max_epochs=1000):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.activation = activation\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_epochs = max_epochs\n",
        "        \n",
        "        # Initialize weights and biases\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        \n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            # Xavier/Glorot initialization\n",
        "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / layer_sizes[i])\n",
        "            b = np.zeros((1, layer_sizes[i+1]))\n",
        "            self.weights.append(w)\n",
        "            self.biases.append(b)\n",
        "        \n",
        "        self.training_losses = []\n",
        "        \n",
        "    def _activation_function(self, x):\n",
        "        \"\"\"Apply activation function\"\"\"\n",
        "        if self.activation == 'sigmoid':\n",
        "            return sigmoid(x)\n",
        "        elif self.activation == 'tanh':\n",
        "            return tanh(x)\n",
        "        elif self.activation == 'relu':\n",
        "            return relu(x)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation: {self.activation}\")\n",
        "    \n",
        "    def _activation_derivative(self, x):\n",
        "        \"\"\"Apply activation derivative\"\"\"\n",
        "        if self.activation == 'sigmoid':\n",
        "            return sigmoid_derivative(x)\n",
        "        elif self.activation == 'tanh':\n",
        "            return tanh_derivative(x)\n",
        "        elif self.activation == 'relu':\n",
        "            return relu_derivative(x)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation: {self.activation}\")\n",
        "    \n",
        "    def forward_pass(self, X):\n",
        "        \"\"\"Forward propagation through the network\"\"\"\n",
        "        self.activations = [X]\n",
        "        self.z_values = []\n",
        "        \n",
        "        current_input = X\n",
        "        \n",
        "        for i, (W, b) in enumerate(zip(self.weights, self.biases)):\n",
        "            # Linear transformation\n",
        "            z = np.dot(current_input, W) + b\n",
        "            self.z_values.append(z)\n",
        "            \n",
        "            # Apply activation (except for output layer in regression)\n",
        "            if i == len(self.weights) - 1:  # Output layer\n",
        "                if self.layer_sizes[-1] == 1:  # Binary classification\n",
        "                    a = sigmoid(z)  # Always use sigmoid for binary output\n",
        "                else:\n",
        "                    a = self._activation_function(z)\n",
        "            else:  # Hidden layers\n",
        "                a = self._activation_function(z)\n",
        "            \n",
        "            self.activations.append(a)\n",
        "            current_input = a\n",
        "        \n",
        "        return self.activations[-1]\n",
        "    \n",
        "    def backward_pass(self, X, y, output):\n",
        "        \"\"\"Backward propagation to compute gradients\"\"\"\n",
        "        m = X.shape[0]\n",
        "        \n",
        "        # Calculate output layer error\n",
        "        if self.layer_sizes[-1] == 1:  # Binary classification\n",
        "            dz = output - y.reshape(-1, 1)\n",
        "        else:\n",
        "            dz = output - y\n",
        "        \n",
        "        # Store gradients\n",
        "        dW = []\n",
        "        db = []\n",
        "        \n",
        "        # Propagate error backwards\n",
        "        for i in reversed(range(len(self.weights))):\n",
        "            # Gradient w.r.t weights and biases\n",
        "            dW_i = (1/m) * np.dot(self.activations[i].T, dz)\n",
        "            db_i = (1/m) * np.sum(dz, axis=0, keepdims=True)\n",
        "            \n",
        "            dW.insert(0, dW_i)\n",
        "            db.insert(0, db_i)\n",
        "            \n",
        "            # Propagate error to previous layer (if not input layer)\n",
        "            if i > 0:\n",
        "                dz = np.dot(dz, self.weights[i].T) * self._activation_derivative(self.z_values[i-1])\n",
        "        \n",
        "        return dW, db\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the MLP\"\"\"\n",
        "        for epoch in range(self.max_epochs):\n",
        "            # Forward pass\n",
        "            output = self.forward_pass(X)\n",
        "            \n",
        "            # Calculate loss (binary cross-entropy for classification)\n",
        "            if self.layer_sizes[-1] == 1:\n",
        "                loss = -np.mean(y * np.log(output + 1e-15) + (1-y) * np.log(1-output + 1e-15))\n",
        "            else:\n",
        "                loss = np.mean((output - y)**2)  # MSE for regression\n",
        "            \n",
        "            self.training_losses.append(loss)\n",
        "            \n",
        "            # Backward pass\n",
        "            dW, db = self.backward_pass(X, y, output)\n",
        "            \n",
        "            # Update weights and biases\n",
        "            for i in range(len(self.weights)):\n",
        "                self.weights[i] -= self.learning_rate * dW[i]\n",
        "                self.biases[i] -= self.learning_rate * db[i]\n",
        "            \n",
        "            # Print progress\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        output = self.forward_pass(X)\n",
        "        if self.layer_sizes[-1] == 1:  # Binary classification\n",
        "            return (output > 0.5).astype(int).flatten()\n",
        "        else:\n",
        "            return output\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict probabilities\"\"\"\n",
        "        return self.forward_pass(X)\n",
        "\n",
        "# Test MLP on XOR problem\n",
        "print(\"=== Testing MLP on XOR Problem ===\")\n",
        "\n",
        "# XOR dataset\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([0, 1, 1, 0])\n",
        "\n",
        "# Create MLP: 2 inputs -> 4 hidden -> 1 output\n",
        "mlp = MLP(layer_sizes=[2, 4, 1], activation='sigmoid', learning_rate=1.0, max_epochs=1000)\n",
        "\n",
        "# Train the network\n",
        "mlp.fit(X_xor, y_xor)\n",
        "\n",
        "# Make predictions\n",
        "predictions = mlp.predict(X_xor)\n",
        "probabilities = mlp.predict_proba(X_xor).flatten()\n",
        "\n",
        "print(f\"\\nXOR Results:\")\n",
        "print(f\"True labels:    {y_xor}\")\n",
        "print(f\"Predictions:    {predictions}\")\n",
        "print(f\"Probabilities:  {probabilities}\")\n",
        "print(f\"Accuracy:       {accuracy_score(y_xor, predictions):.2f}\")\n",
        "\n",
        "# Test on a more complex dataset\n",
        "print(\"\\n=== Testing MLP on Moons Dataset ===\")\n",
        "X_moons, y_moons = make_moons(n_samples=1000, noise=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_moons, y_moons, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train MLP\n",
        "mlp_moons = MLP(layer_sizes=[2, 10, 10, 1], activation='relu', learning_rate=0.01, max_epochs=1000)\n",
        "mlp_moons.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate\n",
        "train_pred = mlp_moons.predict(X_train_scaled)\n",
        "test_pred = mlp_moons.predict(X_test_scaled)\n",
        "\n",
        "print(f\"Training Accuracy: {accuracy_score(y_train, train_pred):.3f}\")\n",
        "print(f\"Test Accuracy:     {accuracy_score(y_test, test_pred):.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
