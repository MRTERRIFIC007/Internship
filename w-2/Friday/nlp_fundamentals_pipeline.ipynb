{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# NLP Fundamentals: Complete Text Processing Pipeline\n",
        "\n",
        "## Learning Objectives\n",
        "- Master essential NLP preprocessing techniques\n",
        "- Implement tokenization, stemming, and lemmatization from scratch\n",
        "- Build comprehensive text cleaning and normalization pipelines\n",
        "- Apply TF-IDF and Bag of Words vectorization methods\n",
        "- Explore text similarity and distance metrics\n",
        "- Create evaluation frameworks for text processing quality\n",
        "\n",
        "## NLP Pipeline Overview\n",
        "\n",
        "### Core Text Processing Steps\n",
        "1. **Text Cleaning**: Remove noise, special characters, normalize case\n",
        "2. **Tokenization**: Split text into individual words/tokens\n",
        "3. **Stop Word Removal**: Filter out common, non-informative words\n",
        "4. **Stemming/Lemmatization**: Reduce words to their root forms\n",
        "5. **Vectorization**: Convert text to numerical representations\n",
        "6. **Analysis**: Apply similarity metrics and clustering\n",
        "\n",
        "### Key Challenges in NLP\n",
        "- **Ambiguity**: Same words can have different meanings in context\n",
        "- **Variability**: Many ways to express the same concept\n",
        "- **Context Dependency**: Meaning changes based on surrounding text\n",
        "- **Language Evolution**: New words, slang, and changing usage patterns\n",
        "- **Multilingual Support**: Different languages have different structures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try to import NLTK\n",
        "try:\n",
        "    import nltk\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "    from nltk.tag import pos_tag\n",
        "    \n",
        "    # Download required NLTK data\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "    NLTK_AVAILABLE = True\n",
        "    print(\"NLTK available with all required datasets!\")\n",
        "except ImportError:\n",
        "    NLTK_AVAILABLE = False\n",
        "    print(\"NLTK not available - will use simplified implementations\")\n",
        "\n",
        "# Set style and random seed\n",
        "plt.style.use('seaborn-v0_8')\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample text corpus for demonstration\n",
        "def create_sample_corpus():\n",
        "    \"\"\"Create a diverse text corpus for NLP demonstration\"\"\"\n",
        "    \n",
        "    documents = [\n",
        "        # Technology\n",
        "        \"Machine learning algorithms can automatically improve through experience and data analysis. \"\n",
        "        \"Deep learning networks use artificial neural networks with multiple layers to model complex patterns.\",\n",
        "        \n",
        "        # Science\n",
        "        \"The theory of relativity revolutionized our understanding of space, time, and gravity. \"\n",
        "        \"Quantum mechanics describes the behavior of matter and energy at the molecular, atomic, nuclear levels.\",\n",
        "        \n",
        "        # Medicine\n",
        "        \"Medical researchers are developing new treatments for cancer using immunotherapy approaches. \"\n",
        "        \"Precision medicine uses genetic information to tailor treatments to individual patients.\",\n",
        "        \n",
        "        # Business\n",
        "        \"Digital transformation is changing how companies operate and deliver value to customers. \"\n",
        "        \"Data-driven decision making helps organizations optimize their business processes and strategies.\",\n",
        "        \n",
        "        # Education\n",
        "        \"Online learning platforms are making education more accessible to students worldwide. \"\n",
        "        \"Artificial intelligence is being used to personalize learning experiences for different students.\",\n",
        "        \n",
        "        # Environment\n",
        "        \"Climate change is affecting global weather patterns and ecosystem sustainability. \"\n",
        "        \"Renewable energy sources like solar and wind power are becoming more cost-effective solutions.\",\n",
        "        \n",
        "        # Sports\n",
        "        \"Athletic performance analysis uses advanced statistics and biomechanical measurements. \"\n",
        "        \"Sports teams employ data scientists to analyze player performance and game strategies.\",\n",
        "        \n",
        "        # Arts\n",
        "        \"Digital art and computer graphics are creating new forms of creative expression. \"\n",
        "        \"Artists are using machine learning to generate novel artistic styles and compositions.\"\n",
        "    ]\n",
        "    \n",
        "    labels = ['Technology', 'Science', 'Medicine', 'Business', \n",
        "              'Education', 'Environment', 'Sports', 'Arts']\n",
        "    \n",
        "    return documents, labels\n",
        "\n",
        "# Create sample corpus\n",
        "corpus, document_labels = create_sample_corpus()\n",
        "\n",
        "print(\"Sample Corpus Created:\")\n",
        "print(f\"Number of documents: {len(corpus)}\")\n",
        "print(f\"Categories: {document_labels}\")\n",
        "print(\"\\\\nFirst document preview:\")\n",
        "print(f'\"{corpus[0][:100]}...\"')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive NLP preprocessing pipeline\n",
        "class NLPPipeline:\n",
        "    \"\"\"Complete NLP preprocessing pipeline with multiple options\"\"\"\n",
        "    \n",
        "    def __init__(self, use_nltk=True):\n",
        "        self.use_nltk = use_nltk and NLTK_AVAILABLE\n",
        "        self.processing_stats = {}\n",
        "        \n",
        "        if self.use_nltk:\n",
        "            self.stemmer = PorterStemmer()\n",
        "            self.lemmatizer = WordNetLemmatizer()\n",
        "            self.stop_words = set(stopwords.words('english'))\n",
        "        else:\n",
        "            # Simple implementations when NLTK is not available\n",
        "            self.stop_words = {\n",
        "                'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', \n",
        "                'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
        "                'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', \n",
        "                'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
        "                'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
        "                'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', \n",
        "                'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \n",
        "                'at', 'by', 'for', 'with', 'through', 'during', 'before', 'after', 'above', \n",
        "                'below', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
        "                'further', 'then', 'once'\n",
        "            }\n",
        "    \n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Basic text cleaning\"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        \n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\\\s+', ' ', text)\n",
        "        \n",
        "        # Remove special characters (keep basic punctuation)\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\\\s.,!?;:-]', '', text)\n",
        "        \n",
        "        # Remove multiple punctuation\n",
        "        text = re.sub(r'[.,!?;:-]+', lambda m: m.group(0)[0], text)\n",
        "        \n",
        "        return text.strip()\n",
        "    \n",
        "    def simple_tokenize(self, text):\n",
        "        \"\"\"Simple tokenization when NLTK is not available\"\"\"\n",
        "        # Remove punctuation and split on whitespace\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        return text.split()\n",
        "    \n",
        "    def simple_stem(self, word):\n",
        "        \"\"\"Very simple stemming - remove common suffixes\"\"\"\n",
        "        suffixes = ['ing', 'ed', 'er', 'est', 'ly', 'tion', 'ness', 'ment']\n",
        "        \n",
        "        for suffix in suffixes:\n",
        "            if word.endswith(suffix) and len(word) > len(suffix) + 2:\n",
        "                return word[:-len(suffix)]\n",
        "        return word\n",
        "    \n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenize text into words\"\"\"\n",
        "        if self.use_nltk:\n",
        "            return word_tokenize(text)\n",
        "        else:\n",
        "            return self.simple_tokenize(text)\n",
        "    \n",
        "    def remove_stopwords(self, tokens):\n",
        "        \"\"\"Remove stop words from token list\"\"\"\n",
        "        return [token for token in tokens if token.lower() not in self.stop_words]\n",
        "    \n",
        "    def stem_tokens(self, tokens):\n",
        "        \"\"\"Apply stemming to tokens\"\"\"\n",
        "        if self.use_nltk:\n",
        "            return [self.stemmer.stem(token) for token in tokens]\n",
        "        else:\n",
        "            return [self.simple_stem(token) for token in tokens]\n",
        "    \n",
        "    def lemmatize_tokens(self, tokens):\n",
        "        \"\"\"Apply lemmatization to tokens\"\"\"\n",
        "        if self.use_nltk:\n",
        "            return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "        else:\n",
        "            # Use stemming as approximation\n",
        "            return self.stem_tokens(tokens)\n",
        "    \n",
        "    def process_document(self, text, steps=['clean', 'tokenize', 'stopwords', 'lemmatize']):\n",
        "        \"\"\"Process a single document through the pipeline\"\"\"\n",
        "        \n",
        "        original_length = len(text.split())\n",
        "        \n",
        "        # Track processing steps\n",
        "        result = {'original': text}\n",
        "        \n",
        "        if 'clean' in steps:\n",
        "            text = self.clean_text(text)\n",
        "            result['cleaned'] = text\n",
        "        \n",
        "        if 'tokenize' in steps:\n",
        "            tokens = self.tokenize(text)\n",
        "            result['tokens'] = tokens\n",
        "        else:\n",
        "            tokens = text.split()\n",
        "        \n",
        "        if 'stopwords' in steps:\n",
        "            tokens = self.remove_stopwords(tokens)\n",
        "            result['no_stopwords'] = tokens\n",
        "        \n",
        "        if 'stem' in steps:\n",
        "            tokens = self.stem_tokens(tokens)\n",
        "            result['stemmed'] = tokens\n",
        "        \n",
        "        if 'lemmatize' in steps:\n",
        "            tokens = self.lemmatize_tokens(tokens)\n",
        "            result['lemmatized'] = tokens\n",
        "        \n",
        "        # Final processed text\n",
        "        result['final'] = ' '.join(tokens)\n",
        "        result['final_tokens'] = tokens\n",
        "        \n",
        "        # Statistics\n",
        "        result['stats'] = {\n",
        "            'original_words': original_length,\n",
        "            'final_words': len(tokens),\n",
        "            'reduction_ratio': 1 - (len(tokens) / original_length) if original_length > 0 else 0\n",
        "        }\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    def process_corpus(self, documents, steps=['clean', 'tokenize', 'stopwords', 'lemmatize']):\n",
        "        \"\"\"Process entire corpus\"\"\"\n",
        "        processed_docs = []\n",
        "        total_stats = {'original_words': 0, 'final_words': 0}\n",
        "        \n",
        "        print(f\"Processing {len(documents)} documents...\")\n",
        "        \n",
        "        for i, doc in enumerate(documents):\n",
        "            processed = self.process_document(doc, steps)\n",
        "            processed_docs.append(processed)\n",
        "            \n",
        "            # Accumulate statistics\n",
        "            total_stats['original_words'] += processed['stats']['original_words']\n",
        "            total_stats['final_words'] += processed['stats']['final_words']\n",
        "            \n",
        "            if (i + 1) % 2 == 0:\n",
        "                print(f\"  Processed {i + 1}/{len(documents)} documents\")\n",
        "        \n",
        "        # Calculate overall statistics\n",
        "        total_stats['reduction_ratio'] = 1 - (total_stats['final_words'] / total_stats['original_words'])\n",
        "        \n",
        "        self.processing_stats = total_stats\n",
        "        print(f\"\\\\nProcessing complete!\")\n",
        "        print(f\"Total word reduction: {total_stats['reduction_ratio']:.2%}\")\n",
        "        \n",
        "        return processed_docs\n",
        "\n",
        "# Initialize and demonstrate NLP pipeline\n",
        "print(\"=== NLP Pipeline Demonstration ===\")\n",
        "\n",
        "nlp_pipeline = NLPPipeline(use_nltk=NLTK_AVAILABLE)\n",
        "\n",
        "# Process a single document to show step-by-step\n",
        "print(\"\\\\n1. Single Document Processing:\")\n",
        "sample_doc = corpus[0]\n",
        "print(f\"Original: {sample_doc}\")\n",
        "\n",
        "steps_demo = ['clean', 'tokenize', 'stopwords', 'lemmatize']\n",
        "processed_sample = nlp_pipeline.process_document(sample_doc, steps_demo)\n",
        "\n",
        "print(f\"\\\\nCleaned: {processed_sample['cleaned']}\")\n",
        "print(f\"Tokens (first 10): {processed_sample['tokens'][:10]}\")\n",
        "print(f\"No stopwords (first 10): {processed_sample['no_stopwords'][:10]}\")\n",
        "print(f\"Lemmatized (first 10): {processed_sample['lemmatized'][:10]}\")\n",
        "print(f\"Final: {processed_sample['final']}\")\n",
        "print(f\"Word reduction: {processed_sample['stats']['reduction_ratio']:.2%}\")\n",
        "\n",
        "# Process entire corpus\n",
        "print(\"\\\\n2. Full Corpus Processing:\")\n",
        "processed_corpus = nlp_pipeline.process_corpus(corpus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive NLP preprocessing pipeline\n",
        "class NLPPipeline:\n",
        "    \"\"\"Complete NLP preprocessing pipeline with multiple options\"\"\"\n",
        "    \n",
        "    def __init__(self, use_nltk=True):\n",
        "        self.use_nltk = use_nltk and NLTK_AVAILABLE\n",
        "        self.processing_stats = {}\n",
        "        \n",
        "        if self.use_nltk:\n",
        "            self.stemmer = PorterStemmer()\n",
        "            self.lemmatizer = WordNetLemmatizer()\n",
        "            self.stop_words = set(stopwords.words('english'))\n",
        "        else:\n",
        "            # Simple implementations when NLTK is not available\n",
        "            self.stop_words = {\n",
        "                'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', \n",
        "                'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
        "                'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', \n",
        "                'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
        "                'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
        "                'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', \n",
        "                'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \n",
        "                'at', 'by', 'for', 'with', 'through', 'during', 'before', 'after', 'above', \n",
        "                'below', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
        "                'further', 'then', 'once'\n",
        "            }\n",
        "    \n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Basic text cleaning\"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        \n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\\\s+', ' ', text)\n",
        "        \n",
        "        # Remove special characters (keep basic punctuation)\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\\\s.,!?;:-]', '', text)\n",
        "        \n",
        "        # Remove multiple punctuation\n",
        "        text = re.sub(r'[.,!?;:-]+', lambda m: m.group(0)[0], text)\n",
        "        \n",
        "        return text.strip()\n",
        "    \n",
        "    def simple_tokenize(self, text):\n",
        "        \"\"\"Simple tokenization when NLTK is not available\"\"\"\n",
        "        # Remove punctuation and split on whitespace\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        return text.split()\n",
        "    \n",
        "    def simple_stem(self, word):\n",
        "        \"\"\"Very simple stemming - remove common suffixes\"\"\"\n",
        "        suffixes = ['ing', 'ed', 'er', 'est', 'ly', 'tion', 'ness', 'ment']\n",
        "        \n",
        "        for suffix in suffixes:\n",
        "            if word.endswith(suffix) and len(word) > len(suffix) + 2:\n",
        "                return word[:-len(suffix)]\n",
        "        return word\n",
        "    \n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenize text into words\"\"\"\n",
        "        if self.use_nltk:\n",
        "            return word_tokenize(text)\n",
        "        else:\n",
        "            return self.simple_tokenize(text)\n",
        "    \n",
        "    def remove_stopwords(self, tokens):\n",
        "        \"\"\"Remove stop words from token list\"\"\"\n",
        "        return [token for token in tokens if token.lower() not in self.stop_words]\n",
        "    \n",
        "    def stem_tokens(self, tokens):\n",
        "        \"\"\"Apply stemming to tokens\"\"\"\n",
        "        if self.use_nltk:\n",
        "            return [self.stemmer.stem(token) for token in tokens]\n",
        "        else:\n",
        "            return [self.simple_stem(token) for token in tokens]\n",
        "    \n",
        "    def lemmatize_tokens(self, tokens):\n",
        "        \"\"\"Apply lemmatization to tokens\"\"\"\n",
        "        if self.use_nltk:\n",
        "            return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "        else:\n",
        "            # Use stemming as approximation\n",
        "            return self.stem_tokens(tokens)\n",
        "    \n",
        "    def process_document(self, text, steps=['clean', 'tokenize', 'stopwords', 'lemmatize']):\n",
        "        \"\"\"Process a single document through the pipeline\"\"\"\n",
        "        \n",
        "        original_length = len(text.split())\n",
        "        \n",
        "        # Track processing steps\n",
        "        result = {'original': text}\n",
        "        \n",
        "        if 'clean' in steps:\n",
        "            text = self.clean_text(text)\n",
        "            result['cleaned'] = text\n",
        "        \n",
        "        if 'tokenize' in steps:\n",
        "            tokens = self.tokenize(text)\n",
        "            result['tokens'] = tokens\n",
        "        else:\n",
        "            tokens = text.split()\n",
        "        \n",
        "        if 'stopwords' in steps:\n",
        "            tokens = self.remove_stopwords(tokens)\n",
        "            result['no_stopwords'] = tokens\n",
        "        \n",
        "        if 'stem' in steps:\n",
        "            tokens = self.stem_tokens(tokens)\n",
        "            result['stemmed'] = tokens\n",
        "        \n",
        "        if 'lemmatize' in steps:\n",
        "            tokens = self.lemmatize_tokens(tokens)\n",
        "            result['lemmatized'] = tokens\n",
        "        \n",
        "        # Final processed text\n",
        "        result['final'] = ' '.join(tokens)\n",
        "        result['final_tokens'] = tokens\n",
        "        \n",
        "        # Statistics\n",
        "        result['stats'] = {\n",
        "            'original_words': original_length,\n",
        "            'final_words': len(tokens),\n",
        "            'reduction_ratio': 1 - (len(tokens) / original_length) if original_length > 0 else 0\n",
        "        }\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    def process_corpus(self, documents, steps=['clean', 'tokenize', 'stopwords', 'lemmatize']):\n",
        "        \"\"\"Process entire corpus\"\"\"\n",
        "        processed_docs = []\n",
        "        total_stats = {'original_words': 0, 'final_words': 0}\n",
        "        \n",
        "        print(f\"Processing {len(documents)} documents...\")\n",
        "        \n",
        "        for i, doc in enumerate(documents):\n",
        "            processed = self.process_document(doc, steps)\n",
        "            processed_docs.append(processed)\n",
        "            \n",
        "            # Accumulate statistics\n",
        "            total_stats['original_words'] += processed['stats']['original_words']\n",
        "            total_stats['final_words'] += processed['stats']['final_words']\n",
        "            \n",
        "            if (i + 1) % 2 == 0:\n",
        "                print(f\"  Processed {i + 1}/{len(documents)} documents\")\n",
        "        \n",
        "        # Calculate overall statistics\n",
        "        total_stats['reduction_ratio'] = 1 - (total_stats['final_words'] / total_stats['original_words'])\n",
        "        \n",
        "        self.processing_stats = total_stats\n",
        "        print(f\"\\\\nProcessing complete!\")\n",
        "        print(f\"Total word reduction: {total_stats['reduction_ratio']:.2%}\")\n",
        "        \n",
        "        return processed_docs\n",
        "\n",
        "# Initialize and demonstrate NLP pipeline\n",
        "print(\"=== NLP Pipeline Demonstration ===\")\n",
        "\n",
        "nlp_pipeline = NLPPipeline(use_nltk=NLTK_AVAILABLE)\n",
        "\n",
        "# Process a single document to show step-by-step\n",
        "print(\"\\\\n1. Single Document Processing:\")\n",
        "sample_doc = corpus[0]\n",
        "print(f\"Original: {sample_doc}\")\n",
        "\n",
        "steps_demo = ['clean', 'tokenize', 'stopwords', 'lemmatize']\n",
        "processed_sample = nlp_pipeline.process_document(sample_doc, steps_demo)\n",
        "\n",
        "print(f\"\\\\nCleaned: {processed_sample['cleaned']}\")\n",
        "print(f\"Tokens (first 10): {processed_sample['tokens'][:10]}\")\n",
        "print(f\"No stopwords (first 10): {processed_sample['no_stopwords'][:10]}\")\n",
        "print(f\"Lemmatized (first 10): {processed_sample['lemmatized'][:10]}\")\n",
        "print(f\"Final: {processed_sample['final']}\")\n",
        "print(f\"Word reduction: {processed_sample['stats']['reduction_ratio']:.2%}\")\n",
        "\n",
        "# Process entire corpus\n",
        "print(\"\\\\n2. Full Corpus Processing:\")\n",
        "processed_corpus = nlp_pipeline.process_corpus(corpus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text vectorization and analysis\n",
        "class TextVectorizer:\n",
        "    \"\"\"Text vectorization using multiple methods\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.vectorizers = {}\n",
        "        self.feature_names = {}\n",
        "        self.vocabulary = {}\n",
        "    \n",
        "    def bag_of_words(self, documents, max_features=1000):\n",
        "        \"\"\"Create Bag of Words representation\"\"\"\n",
        "        vectorizer = CountVectorizer(max_features=max_features, ngram_range=(1, 2))\n",
        "        bow_matrix = vectorizer.fit_transform(documents)\n",
        "        \n",
        "        self.vectorizers['bow'] = vectorizer\n",
        "        self.feature_names['bow'] = vectorizer.get_feature_names_out()\n",
        "        \n",
        "        return bow_matrix.toarray()\n",
        "    \n",
        "    def tfidf_vectorization(self, documents, max_features=1000):\n",
        "        \"\"\"Create TF-IDF representation\"\"\"\n",
        "        vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=(1, 2))\n",
        "        tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "        \n",
        "        self.vectorizers['tfidf'] = vectorizer\n",
        "        self.feature_names['tfidf'] = vectorizer.get_feature_names_out()\n",
        "        \n",
        "        return tfidf_matrix.toarray()\n",
        "    \n",
        "    def analyze_vocabulary(self, documents):\n",
        "        \"\"\"Analyze vocabulary statistics\"\"\"\n",
        "        all_words = []\n",
        "        for doc in documents:\n",
        "            all_words.extend(doc.split())\n",
        "        \n",
        "        word_freq = Counter(all_words)\n",
        "        \n",
        "        stats = {\n",
        "            'total_words': len(all_words),\n",
        "            'unique_words': len(word_freq),\n",
        "            'vocabulary_richness': len(word_freq) / len(all_words),\n",
        "            'most_common': word_freq.most_common(10),\n",
        "            'singleton_ratio': sum(1 for count in word_freq.values() if count == 1) / len(word_freq)\n",
        "        }\n",
        "        \n",
        "        return stats, word_freq\n",
        "    \n",
        "    def document_similarity(self, doc_vectors, method='cosine'):\n",
        "        \"\"\"Calculate document similarity matrix\"\"\"\n",
        "        if method == 'cosine':\n",
        "            similarity_matrix = cosine_similarity(doc_vectors)\n",
        "        else:\n",
        "            # Euclidean distance converted to similarity\n",
        "            from sklearn.metrics.pairwise import euclidean_distances\n",
        "            distances = euclidean_distances(doc_vectors)\n",
        "            similarity_matrix = 1 / (1 + distances)\n",
        "        \n",
        "        return similarity_matrix\n",
        "\n",
        "# Apply text vectorization\n",
        "print(\"\\\\n=== Text Vectorization Analysis ===\")\n",
        "\n",
        "vectorizer = TextVectorizer()\n",
        "\n",
        "# Get processed documents\n",
        "processed_texts = [doc['final'] for doc in processed_corpus]\n",
        "\n",
        "# Vocabulary analysis\n",
        "print(\"\\\\n1. Vocabulary Analysis:\")\n",
        "vocab_stats, word_frequencies = vectorizer.analyze_vocabulary(processed_texts)\n",
        "\n",
        "print(f\"Total words: {vocab_stats['total_words']}\")\n",
        "print(f\"Unique words: {vocab_stats['unique_words']}\")\n",
        "print(f\"Vocabulary richness: {vocab_stats['vocabulary_richness']:.3f}\")\n",
        "print(f\"Singleton ratio: {vocab_stats['singleton_ratio']:.3f}\")\n",
        "print(\"\\\\nMost common words:\")\n",
        "for word, count in vocab_stats['most_common']:\n",
        "    print(f\"  {word}: {count}\")\n",
        "\n",
        "# Bag of Words\n",
        "print(\"\\\\n2. Bag of Words Vectorization:\")\n",
        "bow_vectors = vectorizer.bag_of_words(processed_texts, max_features=50)\n",
        "print(f\"BoW matrix shape: {bow_vectors.shape}\")\n",
        "\n",
        "# TF-IDF\n",
        "print(\"\\\\n3. TF-IDF Vectorization:\")\n",
        "tfidf_vectors = vectorizer.tfidf_vectorization(processed_texts, max_features=50)\n",
        "print(f\"TF-IDF matrix shape: {tfidf_vectors.shape}\")\n",
        "\n",
        "# Document similarity analysis\n",
        "print(\"\\\\n4. Document Similarity Analysis:\")\n",
        "bow_similarity = vectorizer.document_similarity(bow_vectors)\n",
        "tfidf_similarity = vectorizer.document_similarity(tfidf_vectors)\n",
        "\n",
        "print(f\"BoW similarity matrix shape: {bow_similarity.shape}\")\n",
        "print(f\"TF-IDF similarity matrix shape: {tfidf_similarity.shape}\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# Word frequency distribution\n",
        "word_counts = list(word_frequencies.values())\n",
        "axes[0, 0].hist(word_counts, bins=20, alpha=0.7, edgecolor='black')\n",
        "axes[0, 0].set_title('Word Frequency Distribution')\n",
        "axes[0, 0].set_xlabel('Frequency')\n",
        "axes[0, 0].set_ylabel('Number of Words')\n",
        "axes[0, 0].set_yscale('log')\n",
        "\n",
        "# Most common words\n",
        "common_words = [word for word, _ in vocab_stats['most_common']]\n",
        "common_counts = [count for _, count in vocab_stats['most_common']]\n",
        "axes[0, 1].bar(range(len(common_words)), common_counts)\n",
        "axes[0, 1].set_title('Most Common Words')\n",
        "axes[0, 1].set_xlabel('Words')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_xticks(range(len(common_words)))\n",
        "axes[0, 1].set_xticklabels(common_words, rotation=45)\n",
        "\n",
        "# Document length distribution\n",
        "doc_lengths = [len(doc.split()) for doc in processed_texts]\n",
        "axes[0, 2].hist(doc_lengths, bins=10, alpha=0.7, edgecolor='black')\n",
        "axes[0, 2].set_title('Document Length Distribution')\n",
        "axes[0, 2].set_xlabel('Number of Words')\n",
        "axes[0, 2].set_ylabel('Number of Documents')\n",
        "\n",
        "# BoW similarity heatmap\n",
        "im1 = axes[1, 0].imshow(bow_similarity, cmap='viridis', aspect='auto')\n",
        "axes[1, 0].set_title('BoW Similarity Matrix')\n",
        "axes[1, 0].set_xlabel('Documents')\n",
        "axes[1, 0].set_ylabel('Documents')\n",
        "plt.colorbar(im1, ax=axes[1, 0])\n",
        "\n",
        "# TF-IDF similarity heatmap\n",
        "im2 = axes[1, 1].imshow(tfidf_similarity, cmap='viridis', aspect='auto')\n",
        "axes[1, 1].set_title('TF-IDF Similarity Matrix')\n",
        "axes[1, 1].set_xlabel('Documents')\n",
        "axes[1, 1].set_ylabel('Documents')\n",
        "plt.colorbar(im2, ax=axes[1, 1])\n",
        "\n",
        "# Similarity comparison\n",
        "bow_avg_sim = np.mean(bow_similarity[np.triu_indices_from(bow_similarity, k=1)])\n",
        "tfidf_avg_sim = np.mean(tfidf_similarity[np.triu_indices_from(tfidf_similarity, k=1)])\n",
        "\n",
        "methods = ['BoW', 'TF-IDF']\n",
        "avg_similarities = [bow_avg_sim, tfidf_avg_sim]\n",
        "\n",
        "bars = axes[1, 2].bar(methods, avg_similarities, alpha=0.7)\n",
        "axes[1, 2].set_title('Average Document Similarity')\n",
        "axes[1, 2].set_ylabel('Average Similarity')\n",
        "\n",
        "# Add value labels\n",
        "for bar, sim in zip(bars, avg_similarities):\n",
        "    axes[1, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                   f'{sim:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\\\nAverage BoW similarity: {bow_avg_sim:.3f}\")\n",
        "print(f\"Average TF-IDF similarity: {tfidf_avg_sim:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text clustering and dimensionality reduction\n",
        "def text_clustering_analysis(vectors, labels, method_name):\n",
        "    \"\"\"Perform clustering analysis on text vectors\"\"\"\n",
        "    \n",
        "    print(f\"\\\\n=== {method_name} Clustering Analysis ===\")\n",
        "    \n",
        "    # K-means clustering\n",
        "    n_clusters = len(set(labels))  # Number of categories\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    cluster_assignments = kmeans.fit_predict(vectors)\n",
        "    \n",
        "    # Dimensionality reduction for visualization\n",
        "    if vectors.shape[1] > 2:\n",
        "        # Use PCA for initial reduction\n",
        "        if vectors.shape[1] > 50:\n",
        "            pca = PCA(n_components=50)\n",
        "            vectors_reduced = pca.fit_transform(vectors)\n",
        "        else:\n",
        "            vectors_reduced = vectors\n",
        "        \n",
        "        # t-SNE for 2D visualization\n",
        "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(vectors)-1))\n",
        "        vectors_2d = tsne.fit_transform(vectors_reduced)\n",
        "    else:\n",
        "        vectors_2d = vectors\n",
        "    \n",
        "    # Calculate clustering metrics\n",
        "    from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
        "    \n",
        "    # Convert text labels to numeric for comparison\n",
        "    label_to_num = {label: i for i, label in enumerate(set(labels))}\n",
        "    true_labels = [label_to_num[label] for label in labels]\n",
        "    \n",
        "    ari_score = adjusted_rand_score(true_labels, cluster_assignments)\n",
        "    silhouette_avg = silhouette_score(vectors, cluster_assignments)\n",
        "    \n",
        "    print(f\"Adjusted Rand Index: {ari_score:.3f}\")\n",
        "    print(f\"Silhouette Score: {silhouette_avg:.3f}\")\n",
        "    \n",
        "    # Visualization\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Plot by true categories\n",
        "    unique_labels = list(set(labels))\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))\n",
        "    \n",
        "    for i, label in enumerate(unique_labels):\n",
        "        mask = [l == label for l in labels]\n",
        "        axes[0].scatter(vectors_2d[mask, 0], vectors_2d[mask, 1], \n",
        "                       c=[colors[i]], label=label, alpha=0.7, s=50)\n",
        "    \n",
        "    axes[0].set_title(f'{method_name} - True Categories')\n",
        "    axes[0].set_xlabel('t-SNE Component 1')\n",
        "    axes[0].set_ylabel('t-SNE Component 2')\n",
        "    axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    \n",
        "    # Plot by clusters\n",
        "    cluster_colors = plt.cm.Set1(np.linspace(0, 1, n_clusters))\n",
        "    \n",
        "    for i in range(n_clusters):\n",
        "        mask = cluster_assignments == i\n",
        "        axes[1].scatter(vectors_2d[mask, 0], vectors_2d[mask, 1], \n",
        "                       c=[cluster_colors[i]], label=f'Cluster {i}', alpha=0.7, s=50)\n",
        "    \n",
        "    axes[1].set_title(f'{method_name} - K-means Clusters')\n",
        "    axes[1].set_xlabel('t-SNE Component 1')\n",
        "    axes[1].set_ylabel('t-SNE Component 2')\n",
        "    axes[1].legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Analyze cluster composition\n",
        "    print(\"\\\\nCluster Composition:\")\n",
        "    for i in range(n_clusters):\n",
        "        cluster_mask = cluster_assignments == i\n",
        "        cluster_labels = [labels[j] for j, mask in enumerate(cluster_mask) if mask]\n",
        "        label_counts = Counter(cluster_labels)\n",
        "        print(f\"  Cluster {i}: {dict(label_counts)}\")\n",
        "    \n",
        "    return {\n",
        "        'cluster_assignments': cluster_assignments,\n",
        "        'vectors_2d': vectors_2d,\n",
        "        'ari_score': ari_score,\n",
        "        'silhouette_score': silhouette_avg\n",
        "    }\n",
        "\n",
        "# Perform clustering analysis\n",
        "print(\"=== Text Clustering and Visualization ===\")\n",
        "\n",
        "# Clustering with BoW\n",
        "bow_results = text_clustering_analysis(bow_vectors, document_labels, \"Bag of Words\")\n",
        "\n",
        "# Clustering with TF-IDF\n",
        "tfidf_results = text_clustering_analysis(tfidf_vectors, document_labels, \"TF-IDF\")\n",
        "\n",
        "# Comparison summary\n",
        "print(\"\\\\n=== Clustering Comparison Summary ===\")\n",
        "print(f\"BoW - ARI: {bow_results['ari_score']:.3f}, Silhouette: {bow_results['silhouette_score']:.3f}\")\n",
        "print(f\"TF-IDF - ARI: {tfidf_results['ari_score']:.3f}, Silhouette: {tfidf_results['silhouette_score']:.3f}\")\n",
        "\n",
        "# Feature importance analysis\n",
        "print(\"\\\\n=== Feature Importance Analysis ===\")\n",
        "\n",
        "# Get top TF-IDF features\n",
        "tfidf_feature_names = vectorizer.feature_names['tfidf']\n",
        "tfidf_means = np.mean(tfidf_vectors, axis=0)\n",
        "top_indices = np.argsort(tfidf_means)[-10:][::-1]\n",
        "\n",
        "print(\"Top 10 TF-IDF features (by average importance):\")\n",
        "for i, idx in enumerate(top_indices):\n",
        "    print(f\"  {i+1}. {tfidf_feature_names[idx]}: {tfidf_means[idx]:.3f}\")\n",
        "\n",
        "# Document-specific analysis\n",
        "print(\"\\\\nDocument-specific high TF-IDF terms:\")\n",
        "for doc_idx, (doc_label, tfidf_vec) in enumerate(zip(document_labels, tfidf_vectors)):\n",
        "    top_features_idx = np.argsort(tfidf_vec)[-3:][::-1]\n",
        "    top_features = [(tfidf_feature_names[i], tfidf_vec[i]) for i in top_features_idx]\n",
        "    print(f\"  {doc_label}: {', '.join([f'{term}({score:.2f})' for term, score in top_features])}\")\n",
        "\n",
        "# Visualization of top features\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_features = [tfidf_feature_names[idx] for idx in top_indices]\n",
        "top_scores = [tfidf_means[idx] for idx in top_indices]\n",
        "\n",
        "bars = plt.bar(range(len(top_features)), top_scores, alpha=0.7)\n",
        "plt.title('Top 10 TF-IDF Features by Average Importance')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Average TF-IDF Score')\n",
        "plt.xticks(range(len(top_features)), top_features, rotation=45, ha='right')\n",
        "\n",
        "# Add value labels\n",
        "for bar, score in zip(bars, top_scores):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
        "             f'{score:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\\\nNLP fundamentals pipeline analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Key Insights and Learning Reflections\n",
        "\n",
        "### NLP Pipeline Understanding:\n",
        "\n",
        "1. **Text Preprocessing Impact**:\n",
        "   - **Cleaning**: Standardizes text format, reduces noise\n",
        "   - **Tokenization**: Breaks text into analyzable units\n",
        "   - **Stop Word Removal**: Focuses on content-bearing words\n",
        "   - **Stemming/Lemmatization**: Reduces vocabulary size, groups related forms\n",
        "\n",
        "2. **Vectorization Trade-offs**:\n",
        "   - **Bag of Words**: Simple, interpretable, but loses word order\n",
        "   - **TF-IDF**: Weights important terms, reduces common word impact\n",
        "   - **N-grams**: Captures some context but increases dimensionality\n",
        "   - **Sparsity**: High-dimensional, sparse representations are common\n",
        "\n",
        "3. **Similarity Metrics**:\n",
        "   - **Cosine Similarity**: Good for high-dimensional sparse vectors\n",
        "   - **Euclidean Distance**: Sensitive to magnitude differences\n",
        "   - **Document Length**: Affects similarity calculations significantly\n",
        "\n",
        "### Practical Applications:\n",
        "\n",
        "1. **Information Retrieval**: TF-IDF excellent for document ranking\n",
        "2. **Text Classification**: Proper preprocessing crucial for accuracy\n",
        "3. **Clustering**: Dimensionality reduction often necessary for visualization\n",
        "4. **Feature Selection**: Top TF-IDF terms often good starting features\n",
        "\n",
        "### Key Takeaways:\n",
        "- Text preprocessing choices significantly impact downstream tasks\n",
        "- Different vectorization methods capture different aspects of text\n",
        "- Evaluation requires both quantitative metrics and qualitative analysis\n",
        "- Domain-specific preprocessing may be necessary for optimal results\n",
        "- Balance between vocabulary size and computational efficiency is crucial\n",
        "\n",
        "### Next Steps:\n",
        "Moving forward to advanced embeddings that capture semantic relationships beyond simple word co-occurrence!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
