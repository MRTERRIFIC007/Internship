{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Word Embeddings From Scratch: Word2Vec and GloVe\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand theoretical foundations of word embeddings\n",
        "- Implement simplified Word2Vec (Skip-gram) from scratch\n",
        "- Build GloVe (Global Vectors) algorithm components\n",
        "- Compare different embedding approaches and evaluation methods\n",
        "- Explore semantic relationships and analogies in vector space\n",
        "- Apply embeddings to text similarity and clustering tasks\n",
        "\n",
        "## Word Embeddings Theory\n",
        "\n",
        "### Core Concept: Distributional Hypothesis\n",
        "**\"Words that occur in similar contexts tend to have similar meanings\"**\n",
        "\n",
        "### Key Approaches:\n",
        "\n",
        "1. **Count-based Methods (GloVe)**:\n",
        "   - Global co-occurrence statistics\n",
        "   - Matrix factorization approach\n",
        "   - Captures global corpus statistics\n",
        "\n",
        "2. **Prediction-based Methods (Word2Vec)**:\n",
        "   - Local context windows\n",
        "   - Neural network training\n",
        "   - Two architectures: CBOW and Skip-gram\n",
        "\n",
        "### Mathematical Foundations:\n",
        "\n",
        "**Word2Vec Skip-gram Objective**:\n",
        "$$\\mathcal{L} = -\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log p(w_{t+j} | w_t)$$\n",
        "\n",
        "**GloVe Objective**:\n",
        "$$J = \\sum_{i,j=1}^{V} f(X_{ij})(w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij})^2$$\n",
        "\n",
        "Where $X_{ij}$ represents co-occurrence counts between words $i$ and $j$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import re\n",
        "import itertools\n",
        "from scipy.sparse import coo_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style and random seed\n",
        "plt.style.use('seaborn-v0_8')\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive text corpus for word embeddings\n",
        "def create_text_corpus():\n",
        "    \"\"\"Create a diverse text corpus for training word embeddings\"\"\"\n",
        "    \n",
        "    corpus = [\n",
        "        # Technology and AI\n",
        "        \"artificial intelligence machine learning algorithms process data efficiently\",\n",
        "        \"deep learning neural networks train on large datasets using gpu acceleration\",\n",
        "        \"computer vision recognizes patterns in images and videos automatically\",\n",
        "        \"natural language processing understands human communication and text analysis\",\n",
        "        \"machine learning algorithms improve performance through experience and feedback\",\n",
        "        \"artificial neural networks mimic biological brain processing and learning\",\n",
        "        \"deep learning models require extensive computational resources and data\",\n",
        "        \"computer algorithms solve complex problems using mathematical optimization\",\n",
        "        \n",
        "        # Science and Research\n",
        "        \"scientific research discovers new knowledge through experimental methods\",\n",
        "        \"laboratory experiments test hypotheses using controlled conditions\",\n",
        "        \"research scientists analyze data to understand natural phenomena\",\n",
        "        \"scientific method involves observation hypothesis testing and conclusion\",\n",
        "        \"experimental design controls variables to ensure reliable results\",\n",
        "        \"peer review process validates scientific findings and methodology\",\n",
        "        \"research publications share discoveries with scientific community\",\n",
        "        \"data analysis reveals patterns and trends in experimental results\",\n",
        "        \n",
        "        # Mathematics and Logic\n",
        "        \"mathematical equations describe relationships between variables and constants\",\n",
        "        \"statistical analysis interprets data patterns and significance levels\",\n",
        "        \"probability theory models uncertainty and random events\",\n",
        "        \"linear algebra operations manipulate vectors and matrices efficiently\",\n",
        "        \"calculus studies rates of change and accumulation of quantities\",\n",
        "        \"mathematical proofs establish logical foundations for theorems\",\n",
        "        \"geometric shapes have properties defined by mathematical relationships\",\n",
        "        \"number theory explores properties of integers and prime numbers\",\n",
        "        \n",
        "        # Business and Economics\n",
        "        \"business organizations create value through efficient resource allocation\",\n",
        "        \"economic markets determine prices through supply and demand dynamics\",\n",
        "        \"financial analysis evaluates investment opportunities and risks\",\n",
        "        \"business strategy guides decision making and competitive positioning\",\n",
        "        \"market research identifies customer needs and preferences\",\n",
        "        \"economic growth depends on productivity and technological innovation\",\n",
        "        \"business management coordinates resources to achieve organizational goals\",\n",
        "        \"financial planning helps individuals and organizations manage money\",\n",
        "        \n",
        "        # Education and Learning\n",
        "        \"education systems develop knowledge and skills in students\",\n",
        "        \"learning process involves acquiring understanding through study and practice\",\n",
        "        \"teaching methods adapt to different learning styles and abilities\",\n",
        "        \"educational research improves instructional techniques and outcomes\",\n",
        "        \"student assessment measures learning progress and achievement\",\n",
        "        \"curriculum design organizes knowledge into structured learning experiences\",\n",
        "        \"educational technology enhances learning through digital tools\",\n",
        "        \"lifelong learning continues throughout personal and professional development\",\n",
        "        \n",
        "        # Communication and Language\n",
        "        \"human communication uses language to share ideas and information\",\n",
        "        \"written language preserves knowledge across time and distance\",\n",
        "        \"oral communication enables immediate interaction and feedback\",\n",
        "        \"language evolution reflects cultural and social changes\",\n",
        "        \"translation bridges communication between different languages\",\n",
        "        \"linguistic analysis studies structure and meaning in language\",\n",
        "        \"communication skills improve personal and professional relationships\",\n",
        "        \"digital communication connects people across geographical boundaries\"\n",
        "    ]\n",
        "    \n",
        "    return corpus\n",
        "\n",
        "# Preprocessing functions\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Simple text preprocessing for word embeddings\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\\\s]', '', text)\n",
        "    # Split into words\n",
        "    words = text.split()\n",
        "    # Remove very short words\n",
        "    words = [word for word in words if len(word) > 2]\n",
        "    return words\n",
        "\n",
        "def build_vocabulary(corpus, min_count=2):\n",
        "    \"\"\"Build vocabulary from corpus with minimum frequency threshold\"\"\"\n",
        "    word_counts = Counter()\n",
        "    \n",
        "    for text in corpus:\n",
        "        words = preprocess_text(text)\n",
        "        word_counts.update(words)\n",
        "    \n",
        "    # Filter words by minimum count\n",
        "    vocab = {word: idx for idx, (word, count) in enumerate(word_counts.items()) \n",
        "             if count >= min_count}\n",
        "    \n",
        "    # Create reverse mapping\n",
        "    idx_to_word = {idx: word for word, idx in vocab.items()}\n",
        "    \n",
        "    return vocab, idx_to_word, word_counts\n",
        "\n",
        "# Initialize corpus and vocabulary\n",
        "print(\"=== Building Text Corpus and Vocabulary ===\")\n",
        "corpus = create_text_corpus()\n",
        "vocab, idx_to_word, word_counts = build_vocabulary(corpus, min_count=2)\n",
        "\n",
        "print(f\"Corpus size: {len(corpus)} documents\")\n",
        "print(f\"Total unique words: {len(word_counts)}\")\n",
        "print(f\"Vocabulary size (min_count=2): {len(vocab)}\")\n",
        "print(f\"Most common words: {list(word_counts.most_common(10))}\")\n",
        "\n",
        "# Sample processed text\n",
        "sample_text = corpus[0]\n",
        "processed_sample = preprocess_text(sample_text)\n",
        "print(f\"\\\\nSample text: {sample_text}\")\n",
        "print(f\"Processed: {processed_sample}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simplified Word2Vec Skip-gram implementation\n",
        "class SimpleWord2Vec:\n",
        "    \"\"\"Simplified Word2Vec Skip-gram implementation for educational purposes\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim=100, window_size=2, learning_rate=0.01):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.window_size = window_size\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # Initialize word embeddings (input and output matrices)\n",
        "        self.W_in = np.random.uniform(-0.5/embedding_dim, 0.5/embedding_dim, \n",
        "                                     (vocab_size, embedding_dim))\n",
        "        self.W_out = np.random.uniform(-0.5/embedding_dim, 0.5/embedding_dim, \n",
        "                                      (embedding_dim, vocab_size))\n",
        "        \n",
        "        self.loss_history = []\n",
        "    \n",
        "    def softmax(self, x):\n",
        "        \"\"\"Compute softmax with numerical stability\"\"\"\n",
        "        exp_x = np.exp(x - np.max(x))\n",
        "        return exp_x / np.sum(exp_x)\n",
        "    \n",
        "    def forward_pass(self, center_word_idx, context_word_idx):\n",
        "        \"\"\"Forward pass for skip-gram model\"\"\"\n",
        "        # Get center word embedding\n",
        "        h = self.W_in[center_word_idx]  # (embedding_dim,)\n",
        "        \n",
        "        # Compute output scores\n",
        "        u = np.dot(h, self.W_out)  # (vocab_size,)\n",
        "        \n",
        "        # Apply softmax\n",
        "        y_pred = self.softmax(u)\n",
        "        \n",
        "        # Compute loss (negative log likelihood)\n",
        "        loss = -np.log(y_pred[context_word_idx] + 1e-10)\n",
        "        \n",
        "        return h, u, y_pred, loss\n",
        "    \n",
        "    def backward_pass(self, center_word_idx, context_word_idx, h, u, y_pred):\n",
        "        \"\"\"Backward pass with gradient descent\"\"\"\n",
        "        # Create one-hot vector for true context word\n",
        "        y_true = np.zeros(self.vocab_size)\n",
        "        y_true[context_word_idx] = 1\n",
        "        \n",
        "        # Compute gradients\n",
        "        dL_du = y_pred - y_true  # (vocab_size,)\n",
        "        dL_dW_out = np.outer(h, dL_du)  # (embedding_dim, vocab_size)\n",
        "        dL_dh = np.dot(self.W_out, dL_du)  # (embedding_dim,)\n",
        "        \n",
        "        # Update weights\n",
        "        self.W_out -= self.learning_rate * dL_dW_out.T\n",
        "        self.W_in[center_word_idx] -= self.learning_rate * dL_dh\n",
        "    \n",
        "    def generate_training_pairs(self, corpus, vocab):\n",
        "        \"\"\"Generate (center_word, context_word) pairs\"\"\"\n",
        "        pairs = []\n",
        "        \n",
        "        for text in corpus:\n",
        "            words = preprocess_text(text)\n",
        "            # Convert words to indices\n",
        "            word_indices = [vocab[word] for word in words if word in vocab]\n",
        "            \n",
        "            # Generate context pairs\n",
        "            for i, center_idx in enumerate(word_indices):\n",
        "                # Define context window\n",
        "                start = max(0, i - self.window_size)\n",
        "                end = min(len(word_indices), i + self.window_size + 1)\n",
        "                \n",
        "                for j in range(start, end):\n",
        "                    if i != j:  # Skip center word itself\n",
        "                        context_idx = word_indices[j]\n",
        "                        pairs.append((center_idx, context_idx))\n",
        "        \n",
        "        return pairs\n",
        "    \n",
        "    def train(self, corpus, vocab, epochs=100):\n",
        "        \"\"\"Train the Word2Vec model\"\"\"\n",
        "        print(f\"Training Word2Vec on {len(corpus)} documents...\")\n",
        "        \n",
        "        # Generate training pairs\n",
        "        training_pairs = self.generate_training_pairs(corpus, vocab)\n",
        "        print(f\"Generated {len(training_pairs)} training pairs\")\n",
        "        \n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            np.random.shuffle(training_pairs)\n",
        "            \n",
        "            for center_idx, context_idx in training_pairs:\n",
        "                # Forward pass\n",
        "                h, u, y_pred, loss = self.forward_pass(center_idx, context_idx)\n",
        "                total_loss += loss\n",
        "                \n",
        "                # Backward pass\n",
        "                self.backward_pass(center_idx, context_idx, h, u, y_pred)\n",
        "            \n",
        "            avg_loss = total_loss / len(training_pairs)\n",
        "            self.loss_history.append(avg_loss)\n",
        "            \n",
        "            if (epoch + 1) % 20 == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "        \n",
        "        print(\"Training completed!\")\n",
        "    \n",
        "    def get_word_vector(self, word_idx):\n",
        "        \"\"\"Get word embedding vector\"\"\"\n",
        "        return self.W_in[word_idx]\n",
        "    \n",
        "    def find_similar_words(self, word, vocab, idx_to_word, top_k=5):\n",
        "        \"\"\"Find most similar words using cosine similarity\"\"\"\n",
        "        if word not in vocab:\n",
        "            return []\n",
        "        \n",
        "        word_idx = vocab[word]\n",
        "        word_vec = self.get_word_vector(word_idx)\n",
        "        \n",
        "        # Compute similarities with all words\n",
        "        similarities = []\n",
        "        for idx in range(self.vocab_size):\n",
        "            if idx != word_idx:\n",
        "                other_vec = self.get_word_vector(idx)\n",
        "                similarity = np.dot(word_vec, other_vec) / (\n",
        "                    np.linalg.norm(word_vec) * np.linalg.norm(other_vec) + 1e-10\n",
        "                )\n",
        "                similarities.append((idx, similarity))\n",
        "        \n",
        "        # Sort by similarity\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        # Return top k similar words\n",
        "        similar_words = [(idx_to_word[idx], sim) for idx, sim in similarities[:top_k]]\n",
        "        return similar_words\n",
        "\n",
        "# Train Word2Vec model\n",
        "print(\"\\\\n=== Training Word2Vec Model ===\")\n",
        "w2v_model = SimpleWord2Vec(vocab_size=len(vocab), embedding_dim=50, \n",
        "                          window_size=2, learning_rate=0.01)\n",
        "\n",
        "w2v_model.train(corpus, vocab, epochs=100)\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(w2v_model.loss_history)\n",
        "plt.title('Word2Vec Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final training loss: {w2v_model.loss_history[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simplified GloVe implementation\n",
        "class SimpleGloVe:\n",
        "    \"\"\"Simplified GloVe implementation for educational purposes\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim=100, learning_rate=0.01, \n",
        "                 x_max=100, alpha=0.75):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.x_max = x_max\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        # Initialize word vectors and biases\n",
        "        self.W = np.random.normal(0, 0.1, (vocab_size, embedding_dim))\n",
        "        self.W_tilde = np.random.normal(0, 0.1, (vocab_size, embedding_dim))\n",
        "        self.b = np.random.normal(0, 0.1, vocab_size)\n",
        "        self.b_tilde = np.random.normal(0, 0.1, vocab_size)\n",
        "        \n",
        "        self.loss_history = []\n",
        "        self.cooccurrence_matrix = None\n",
        "    \n",
        "    def build_cooccurrence_matrix(self, corpus, vocab, window_size=5):\n",
        "        \"\"\"Build word co-occurrence matrix\"\"\"\n",
        "        print(\"Building co-occurrence matrix...\")\n",
        "        \n",
        "        # Initialize co-occurrence counts\n",
        "        cooccur_counts = defaultdict(float)\n",
        "        \n",
        "        for text in corpus:\n",
        "            words = preprocess_text(text)\n",
        "            word_indices = [vocab[word] for word in words if word in vocab]\n",
        "            \n",
        "            # Count co-occurrences within window\n",
        "            for i, center_idx in enumerate(word_indices):\n",
        "                for j in range(max(0, i - window_size), \n",
        "                             min(len(word_indices), i + window_size + 1)):\n",
        "                    if i != j:\n",
        "                        context_idx = word_indices[j]\n",
        "                        distance = abs(i - j)\n",
        "                        # Weight by inverse distance\n",
        "                        weight = 1.0 / distance\n",
        "                        cooccur_counts[(center_idx, context_idx)] += weight\n",
        "        \n",
        "        # Convert to matrix format\n",
        "        rows, cols, data = [], [], []\n",
        "        for (i, j), count in cooccur_counts.items():\n",
        "            rows.append(i)\n",
        "            cols.append(j)\n",
        "            data.append(count)\n",
        "        \n",
        "        self.cooccurrence_matrix = coo_matrix((data, (rows, cols)), \n",
        "                                            shape=(self.vocab_size, self.vocab_size))\n",
        "        \n",
        "        print(f\"Co-occurrence matrix: {self.cooccurrence_matrix.nnz} non-zero entries\")\n",
        "        return self.cooccurrence_matrix\n",
        "    \n",
        "    def weighting_function(self, x):\n",
        "        \"\"\"GloVe weighting function f(X_ij)\"\"\"\n",
        "        return np.where(x < self.x_max, (x / self.x_max) ** self.alpha, 1.0)\n",
        "    \n",
        "    def train(self, corpus, vocab, epochs=100, window_size=5):\n",
        "        \"\"\"Train GloVe model\"\"\"\n",
        "        print(f\"Training GloVe on {len(corpus)} documents...\")\n",
        "        \n",
        "        # Build co-occurrence matrix\n",
        "        cooccur_matrix = self.build_cooccurrence_matrix(corpus, vocab, window_size)\n",
        "        \n",
        "        # Convert to dense format for training (in practice, use sparse operations)\n",
        "        X = cooccur_matrix.toarray()\n",
        "        \n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            num_pairs = 0\n",
        "            \n",
        "            # Iterate over non-zero co-occurrences\n",
        "            for i in range(self.vocab_size):\n",
        "                for j in range(self.vocab_size):\n",
        "                    if X[i, j] > 0:\n",
        "                        # Compute current prediction\n",
        "                        prediction = (np.dot(self.W[i], self.W_tilde[j]) + \n",
        "                                    self.b[i] + self.b_tilde[j])\n",
        "                        \n",
        "                        # Compute weighted squared error\n",
        "                        diff = prediction - np.log(X[i, j] + 1e-10)\n",
        "                        weight = self.weighting_function(X[i, j])\n",
        "                        loss = weight * (diff ** 2)\n",
        "                        total_loss += loss\n",
        "                        num_pairs += 1\n",
        "                        \n",
        "                        # Compute gradients\n",
        "                        grad_factor = 2 * weight * diff\n",
        "                        \n",
        "                        # Update parameters\n",
        "                        self.W[i] -= self.learning_rate * grad_factor * self.W_tilde[j]\n",
        "                        self.W_tilde[j] -= self.learning_rate * grad_factor * self.W[i]\n",
        "                        self.b[i] -= self.learning_rate * grad_factor\n",
        "                        self.b_tilde[j] -= self.learning_rate * grad_factor\n",
        "            \n",
        "            avg_loss = total_loss / max(num_pairs, 1)\n",
        "            self.loss_history.append(avg_loss)\n",
        "            \n",
        "            if (epoch + 1) % 20 == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "        \n",
        "        print(\"GloVe training completed!\")\n",
        "    \n",
        "    def get_word_vector(self, word_idx):\n",
        "        \"\"\"Get final word embedding (average of W and W_tilde)\"\"\"\n",
        "        return (self.W[word_idx] + self.W_tilde[word_idx]) / 2\n",
        "    \n",
        "    def find_similar_words(self, word, vocab, idx_to_word, top_k=5):\n",
        "        \"\"\"Find most similar words using cosine similarity\"\"\"\n",
        "        if word not in vocab:\n",
        "            return []\n",
        "        \n",
        "        word_idx = vocab[word]\n",
        "        word_vec = self.get_word_vector(word_idx)\n",
        "        \n",
        "        # Compute similarities with all words\n",
        "        similarities = []\n",
        "        for idx in range(self.vocab_size):\n",
        "            if idx != word_idx:\n",
        "                other_vec = self.get_word_vector(idx)\n",
        "                similarity = np.dot(word_vec, other_vec) / (\n",
        "                    np.linalg.norm(word_vec) * np.linalg.norm(other_vec) + 1e-10\n",
        "                )\n",
        "                similarities.append((idx, similarity))\n",
        "        \n",
        "        # Sort by similarity\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        # Return top k similar words\n",
        "        similar_words = [(idx_to_word[idx], sim) for idx, sim in similarities[:top_k]]\n",
        "        return similar_words\n",
        "\n",
        "# Train GloVe model\n",
        "print(\"\\\\n=== Training GloVe Model ===\")\n",
        "glove_model = SimpleGloVe(vocab_size=len(vocab), embedding_dim=50, \n",
        "                         learning_rate=0.01)\n",
        "\n",
        "glove_model.train(corpus, vocab, epochs=50, window_size=5)\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(glove_model.loss_history)\n",
        "plt.title('GloVe Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final training loss: {glove_model.loss_history[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Semantic similarity and analogy analysis\n",
        "def analyze_word_similarities():\n",
        "    \"\"\"Analyze semantic similarities in both models\"\"\"\n",
        "    \n",
        "    print(\"\\\\n=== Word Similarity Analysis ===\")\n",
        "    \n",
        "    # Test words for similarity analysis\n",
        "    test_words = ['learning', 'data', 'research', 'language', 'business', 'mathematical']\n",
        "    \n",
        "    for word in test_words:\n",
        "        if word in vocab:\n",
        "            print(f\"\\\\n--- Similar words to '{word}' ---\")\n",
        "            \n",
        "            # Word2Vec similarities\n",
        "            w2v_similar = w2v_model.find_similar_words(word, vocab, idx_to_word, top_k=5)\n",
        "            print(\"Word2Vec:\")\n",
        "            for similar_word, similarity in w2v_similar:\n",
        "                print(f\"  {similar_word}: {similarity:.3f}\")\n",
        "            \n",
        "            # GloVe similarities\n",
        "            glove_similar = glove_model.find_similar_words(word, vocab, idx_to_word, top_k=5)\n",
        "            print(\"GloVe:\")\n",
        "            for similar_word, similarity in glove_similar:\n",
        "                print(f\"  {similar_word}: {similarity:.3f}\")\n",
        "\n",
        "def simple_analogy_test(model, word_a, word_b, word_c, vocab, idx_to_word, model_name):\n",
        "    \"\"\"Simple analogy test: word_a is to word_b as word_c is to ?\"\"\"\n",
        "    \n",
        "    if not all(word in vocab for word in [word_a, word_b, word_c]):\n",
        "        return None\n",
        "    \n",
        "    # Get word vectors\n",
        "    vec_a = model.get_word_vector(vocab[word_a])\n",
        "    vec_b = model.get_word_vector(vocab[word_b])\n",
        "    vec_c = model.get_word_vector(vocab[word_c])\n",
        "    \n",
        "    # Compute analogy vector: vec_b - vec_a + vec_c\n",
        "    analogy_vec = vec_b - vec_a + vec_c\n",
        "    \n",
        "    # Find closest word to analogy vector\n",
        "    best_similarity = -1\n",
        "    best_word = None\n",
        "    \n",
        "    for idx in range(len(vocab)):\n",
        "        word = idx_to_word[idx]\n",
        "        if word not in [word_a, word_b, word_c]:  # Exclude input words\n",
        "            vec = model.get_word_vector(idx)\n",
        "            similarity = np.dot(analogy_vec, vec) / (\n",
        "                np.linalg.norm(analogy_vec) * np.linalg.norm(vec) + 1e-10\n",
        "            )\n",
        "            \n",
        "            if similarity > best_similarity:\n",
        "                best_similarity = similarity\n",
        "                best_word = word\n",
        "    \n",
        "    print(f\"{model_name}: {word_a} : {word_b} :: {word_c} : {best_word} (similarity: {best_similarity:.3f})\")\n",
        "    return best_word, best_similarity\n",
        "\n",
        "# Run similarity analysis\n",
        "analyze_word_similarities()\n",
        "\n",
        "# Test simple analogies\n",
        "print(\"\\\\n=== Simple Analogy Tests ===\")\n",
        "analogy_tests = [\n",
        "    ('data', 'analysis', 'research', 'what should be the fourth word?'),\n",
        "    ('learning', 'education', 'training', 'what should be the fourth word?'),\n",
        "    ('machine', 'artificial', 'human', 'what should be the fourth word?'),\n",
        "]\n",
        "\n",
        "for word_a, word_b, word_c, description in analogy_tests:\n",
        "    print(f\"\\\\nAnalogy: {word_a} : {word_b} :: {word_c} : ?\")\n",
        "    simple_analogy_test(w2v_model, word_a, word_b, word_c, vocab, idx_to_word, \"Word2Vec\")\n",
        "    simple_analogy_test(glove_model, word_a, word_b, word_c, vocab, idx_to_word, \"GloVe\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization of word embeddings\n",
        "def visualize_embeddings():\n",
        "    \"\"\"Visualize word embeddings using dimensionality reduction\"\"\"\n",
        "    \n",
        "    print(\"\\\\n=== Embedding Visualization ===\")\n",
        "    \n",
        "    # Get all word vectors for both models\n",
        "    w2v_vectors = np.array([w2v_model.get_word_vector(i) for i in range(len(vocab))])\n",
        "    glove_vectors = np.array([glove_model.get_word_vector(i) for i in range(len(vocab))])\n",
        "    \n",
        "    # Select subset of words for visualization\n",
        "    common_words = [word for word, count in word_counts.most_common(30) if word in vocab]\n",
        "    word_indices = [vocab[word] for word in common_words]\n",
        "    \n",
        "    # Get vectors for selected words\n",
        "    w2v_subset = w2v_vectors[word_indices]\n",
        "    glove_subset = glove_vectors[word_indices]\n",
        "    \n",
        "    # Apply t-SNE for 2D visualization\n",
        "    print(\"Applying t-SNE for visualization...\")\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(15, len(common_words)-1))\n",
        "    \n",
        "    w2v_2d = tsne.fit_transform(w2v_subset)\n",
        "    \n",
        "    # Use same t-SNE for GloVe (refit for fair comparison)\n",
        "    tsne_glove = TSNE(n_components=2, random_state=42, perplexity=min(15, len(common_words)-1))\n",
        "    glove_2d = tsne_glove.fit_transform(glove_subset)\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
        "    \n",
        "    # Word2Vec visualization\n",
        "    axes[0].scatter(w2v_2d[:, 0], w2v_2d[:, 1], alpha=0.7, s=50)\n",
        "    for i, word in enumerate(common_words):\n",
        "        axes[0].annotate(word, (w2v_2d[i, 0], w2v_2d[i, 1]), \n",
        "                        xytext=(3, 3), textcoords='offset points', \n",
        "                        fontsize=9, alpha=0.8)\n",
        "    axes[0].set_title('Word2Vec Embeddings (t-SNE)')\n",
        "    axes[0].set_xlabel('t-SNE 1')\n",
        "    axes[0].set_ylabel('t-SNE 2')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # GloVe visualization\n",
        "    axes[1].scatter(glove_2d[:, 0], glove_2d[:, 1], alpha=0.7, s=50, color='orange')\n",
        "    for i, word in enumerate(common_words):\n",
        "        axes[1].annotate(word, (glove_2d[i, 0], glove_2d[i, 1]), \n",
        "                        xytext=(3, 3), textcoords='offset points', \n",
        "                        fontsize=9, alpha=0.8)\n",
        "    axes[1].set_title('GloVe Embeddings (t-SNE)')\n",
        "    axes[1].set_xlabel('t-SNE 1')\n",
        "    axes[1].set_ylabel('t-SNE 2')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return w2v_vectors, glove_vectors\n",
        "\n",
        "def compare_embedding_quality():\n",
        "    \"\"\"Compare embedding quality using various metrics\"\"\"\n",
        "    \n",
        "    print(\"\\\\n=== Embedding Quality Comparison ===\")\n",
        "    \n",
        "    # Get all embeddings\n",
        "    w2v_vectors = np.array([w2v_model.get_word_vector(i) for i in range(len(vocab))])\n",
        "    glove_vectors = np.array([glove_model.get_word_vector(i) for i in range(len(vocab))])\n",
        "    \n",
        "    # Compute average vector norms\n",
        "    w2v_avg_norm = np.mean([np.linalg.norm(vec) for vec in w2v_vectors])\n",
        "    glove_avg_norm = np.mean([np.linalg.norm(vec) for vec in glove_vectors])\n",
        "    \n",
        "    print(f\"Average vector norms:\")\n",
        "    print(f\"  Word2Vec: {w2v_avg_norm:.3f}\")\n",
        "    print(f\"  GloVe: {glove_avg_norm:.3f}\")\n",
        "    \n",
        "    # Compute similarity distributions\n",
        "    print(\"\\\\nComputing similarity distributions...\")\n",
        "    \n",
        "    # Sample pairs for efficiency\n",
        "    sample_size = min(100, len(vocab))\n",
        "    sample_indices = np.random.choice(len(vocab), sample_size, replace=False)\n",
        "    \n",
        "    w2v_similarities = []\n",
        "    glove_similarities = []\n",
        "    \n",
        "    for i in range(sample_size):\n",
        "        for j in range(i+1, sample_size):\n",
        "            idx1, idx2 = sample_indices[i], sample_indices[j]\n",
        "            \n",
        "            # Word2Vec similarity\n",
        "            vec1_w2v = w2v_vectors[idx1]\n",
        "            vec2_w2v = w2v_vectors[idx2]\n",
        "            sim_w2v = np.dot(vec1_w2v, vec2_w2v) / (\n",
        "                np.linalg.norm(vec1_w2v) * np.linalg.norm(vec2_w2v) + 1e-10\n",
        "            )\n",
        "            w2v_similarities.append(sim_w2v)\n",
        "            \n",
        "            # GloVe similarity\n",
        "            vec1_glove = glove_vectors[idx1]\n",
        "            vec2_glove = glove_vectors[idx2]\n",
        "            sim_glove = np.dot(vec1_glove, vec2_glove) / (\n",
        "                np.linalg.norm(vec1_glove) * np.linalg.norm(vec2_glove) + 1e-10\n",
        "            )\n",
        "            glove_similarities.append(sim_glove)\n",
        "    \n",
        "    # Plot similarity distributions\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(w2v_similarities, bins=30, alpha=0.7, label='Word2Vec', density=True)\n",
        "    plt.hist(glove_similarities, bins=30, alpha=0.7, label='GloVe', density=True)\n",
        "    plt.xlabel('Cosine Similarity')\n",
        "    plt.ylabel('Density')\n",
        "    plt.title('Distribution of Pairwise Similarities')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Statistics comparison\n",
        "    plt.subplot(1, 2, 2)\n",
        "    stats_comparison = {\n",
        "        'Mean Similarity': [np.mean(w2v_similarities), np.mean(glove_similarities)],\n",
        "        'Std Similarity': [np.std(w2v_similarities), np.std(glove_similarities)],\n",
        "        'Min Similarity': [np.min(w2v_similarities), np.min(glove_similarities)],\n",
        "        'Max Similarity': [np.max(w2v_similarities), np.max(glove_similarities)]\n",
        "    }\n",
        "    \n",
        "    x = np.arange(len(stats_comparison))\n",
        "    width = 0.35\n",
        "    \n",
        "    w2v_stats = [stats_comparison[key][0] for key in stats_comparison.keys()]\n",
        "    glove_stats = [stats_comparison[key][1] for key in stats_comparison.keys()]\n",
        "    \n",
        "    plt.bar(x - width/2, w2v_stats, width, label='Word2Vec', alpha=0.7)\n",
        "    plt.bar(x + width/2, glove_stats, width, label='GloVe', alpha=0.7)\n",
        "    \n",
        "    plt.xlabel('Statistics')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title('Similarity Statistics Comparison')\n",
        "    plt.xticks(x, stats_comparison.keys(), rotation=45)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print numerical comparison\n",
        "    print(\"\\\\nNumerical Comparison:\")\n",
        "    for key, values in stats_comparison.items():\n",
        "        print(f\"  {key}:\")\n",
        "        print(f\"    Word2Vec: {values[0]:.4f}\")\n",
        "        print(f\"    GloVe: {values[1]:.4f}\")\n",
        "        print(f\"    Difference: {abs(values[0] - values[1]):.4f}\")\n",
        "\n",
        "# Run visualization and comparison\n",
        "embedding_vectors = visualize_embeddings()\n",
        "compare_embedding_quality()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clustering analysis using word embeddings\n",
        "def clustering_with_embeddings():\n",
        "    \"\"\"Perform clustering analysis using word embeddings\"\"\"\n",
        "    \n",
        "    print(\"\\\\n=== Clustering Analysis with Word Embeddings ===\")\n",
        "    \n",
        "    from sklearn.cluster import KMeans\n",
        "    from sklearn.metrics import silhouette_score\n",
        "    \n",
        "    # Get embeddings\n",
        "    w2v_vectors = np.array([w2v_model.get_word_vector(i) for i in range(len(vocab))])\n",
        "    glove_vectors = np.array([glove_model.get_word_vector(i) for i in range(len(vocab))])\n",
        "    \n",
        "    # Determine optimal number of clusters\n",
        "    n_clusters_range = range(3, min(8, len(vocab)//3))\n",
        "    \n",
        "    w2v_silhouette_scores = []\n",
        "    glove_silhouette_scores = []\n",
        "    \n",
        "    print(\"Finding optimal number of clusters...\")\n",
        "    for n_clusters in n_clusters_range:\n",
        "        # Word2Vec clustering\n",
        "        kmeans_w2v = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        w2v_labels = kmeans_w2v.fit_predict(w2v_vectors)\n",
        "        w2v_silhouette = silhouette_score(w2v_vectors, w2v_labels)\n",
        "        w2v_silhouette_scores.append(w2v_silhouette)\n",
        "        \n",
        "        # GloVe clustering\n",
        "        kmeans_glove = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        glove_labels = kmeans_glove.fit_predict(glove_vectors)\n",
        "        glove_silhouette = silhouette_score(glove_vectors, glove_labels)\n",
        "        glove_silhouette_scores.append(glove_silhouette)\n",
        "        \n",
        "        print(f\"  k={n_clusters}: Word2Vec silhouette={w2v_silhouette:.3f}, GloVe silhouette={glove_silhouette:.3f}\")\n",
        "    \n",
        "    # Find optimal k\n",
        "    optimal_k_w2v = n_clusters_range[np.argmax(w2v_silhouette_scores)]\n",
        "    optimal_k_glove = n_clusters_range[np.argmax(glove_silhouette_scores)]\n",
        "    \n",
        "    print(f\"\\\\nOptimal clusters: Word2Vec k={optimal_k_w2v}, GloVe k={optimal_k_glove}\")\n",
        "    \n",
        "    # Perform final clustering with optimal k\n",
        "    final_k = optimal_k_w2v  # Use Word2Vec's optimal for comparison\n",
        "    \n",
        "    kmeans_w2v = KMeans(n_clusters=final_k, random_state=42)\n",
        "    w2v_clusters = kmeans_w2v.fit_predict(w2v_vectors)\n",
        "    \n",
        "    kmeans_glove = KMeans(n_clusters=final_k, random_state=42)\n",
        "    glove_clusters = kmeans_glove.fit_predict(glove_vectors)\n",
        "    \n",
        "    # Analyze clusters\n",
        "    print(f\"\\\\n--- Word2Vec Clusters (k={final_k}) ---\")\n",
        "    for cluster_id in range(final_k):\n",
        "        cluster_words = [idx_to_word[i] for i, label in enumerate(w2v_clusters) if label == cluster_id]\n",
        "        print(f\"Cluster {cluster_id}: {cluster_words}\")\n",
        "    \n",
        "    print(f\"\\\\n--- GloVe Clusters (k={final_k}) ---\")\n",
        "    for cluster_id in range(final_k):\n",
        "        cluster_words = [idx_to_word[i] for i, label in enumerate(glove_clusters) if label == cluster_id]\n",
        "        print(f\"Cluster {cluster_id}: {cluster_words}\")\n",
        "    \n",
        "    # Plot silhouette scores\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(n_clusters_range, w2v_silhouette_scores, 'o-', label='Word2Vec', linewidth=2)\n",
        "    plt.plot(n_clusters_range, glove_silhouette_scores, 's-', label='GloVe', linewidth=2)\n",
        "    plt.xlabel('Number of Clusters')\n",
        "    plt.ylabel('Silhouette Score')\n",
        "    plt.title('Clustering Quality vs Number of Clusters')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Mark optimal points\n",
        "    plt.axvline(x=optimal_k_w2v, color='blue', linestyle='--', alpha=0.5, label=f'Word2Vec optimal (k={optimal_k_w2v})')\n",
        "    plt.axvline(x=optimal_k_glove, color='orange', linestyle='--', alpha=0.5, label=f'GloVe optimal (k={optimal_k_glove})')\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    return w2v_clusters, glove_clusters\n",
        "\n",
        "def document_similarity_analysis():\n",
        "    \"\"\"Analyze document similarity using word embeddings\"\"\"\n",
        "    \n",
        "    print(\"\\\\n=== Document Similarity Analysis ===\")\n",
        "    \n",
        "    # Create document vectors by averaging word embeddings\n",
        "    def get_document_vector(text, model, vocab):\n",
        "        words = preprocess_text(text)\n",
        "        word_vectors = []\n",
        "        \n",
        "        for word in words:\n",
        "            if word in vocab:\n",
        "                word_idx = vocab[word]\n",
        "                word_vec = model.get_word_vector(word_idx)\n",
        "                word_vectors.append(word_vec)\n",
        "        \n",
        "        if word_vectors:\n",
        "            return np.mean(word_vectors, axis=0)\n",
        "        else:\n",
        "            return np.zeros(model.embedding_dim)\n",
        "    \n",
        "    # Get document vectors for both models\n",
        "    w2v_doc_vectors = []\n",
        "    glove_doc_vectors = []\n",
        "    \n",
        "    for text in corpus[:10]:  # Use first 10 documents for analysis\n",
        "        w2v_doc_vec = get_document_vector(text, w2v_model, vocab)\n",
        "        glove_doc_vec = get_document_vector(text, glove_model, vocab)\n",
        "        \n",
        "        w2v_doc_vectors.append(w2v_doc_vec)\n",
        "        glove_doc_vectors.append(glove_doc_vec)\n",
        "    \n",
        "    w2v_doc_vectors = np.array(w2v_doc_vectors)\n",
        "    glove_doc_vectors = np.array(glove_doc_vectors)\n",
        "    \n",
        "    # Compute document similarity matrices\n",
        "    w2v_doc_sim = cosine_similarity(w2v_doc_vectors)\n",
        "    glove_doc_sim = cosine_similarity(glove_doc_vectors)\n",
        "    \n",
        "    # Visualize similarity matrices\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    im1 = axes[0].imshow(w2v_doc_sim, cmap='viridis', aspect='auto')\n",
        "    axes[0].set_title('Document Similarity (Word2Vec)')\n",
        "    axes[0].set_xlabel('Document Index')\n",
        "    axes[0].set_ylabel('Document Index')\n",
        "    plt.colorbar(im1, ax=axes[0])\n",
        "    \n",
        "    im2 = axes[1].imshow(glove_doc_sim, cmap='viridis', aspect='auto')\n",
        "    axes[1].set_title('Document Similarity (GloVe)')\n",
        "    axes[1].set_xlabel('Document Index')\n",
        "    axes[1].set_ylabel('Document Index')\n",
        "    plt.colorbar(im2, ax=axes[1])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Find most similar document pairs\n",
        "    print(\"\\\\nMost similar document pairs:\")\n",
        "    \n",
        "    # Get upper triangle indices (excluding diagonal)\n",
        "    triu_indices = np.triu_indices_from(w2v_doc_sim, k=1)\n",
        "    \n",
        "    # Word2Vec similarities\n",
        "    w2v_similarities = w2v_doc_sim[triu_indices]\n",
        "    w2v_top_pairs = np.argsort(w2v_similarities)[-3:][::-1]\n",
        "    \n",
        "    print(\"\\\\nWord2Vec top similar pairs:\")\n",
        "    for idx in w2v_top_pairs:\n",
        "        i, j = triu_indices[0][idx], triu_indices[1][idx]\n",
        "        similarity = w2v_similarities[idx]\n",
        "        print(f\"  Documents {i} & {j}: {similarity:.3f}\")\n",
        "        print(f\"    Doc {i}: {corpus[i][:100]}...\")\n",
        "        print(f\"    Doc {j}: {corpus[j][:100]}...\")\n",
        "        print()\n",
        "    \n",
        "    # GloVe similarities\n",
        "    glove_similarities = glove_doc_sim[triu_indices]\n",
        "    glove_top_pairs = np.argsort(glove_similarities)[-3:][::-1]\n",
        "    \n",
        "    print(\"GloVe top similar pairs:\")\n",
        "    for idx in glove_top_pairs:\n",
        "        i, j = triu_indices[0][idx], triu_indices[1][idx]\n",
        "        similarity = glove_similarities[idx]\n",
        "        print(f\"  Documents {i} & {j}: {similarity:.3f}\")\n",
        "        print(f\"    Doc {i}: {corpus[i][:100]}...\")\n",
        "        print(f\"    Doc {j}: {corpus[j][:100]}...\")\n",
        "        print()\n",
        "\n",
        "# Run clustering and document analysis\n",
        "cluster_results = clustering_with_embeddings()\n",
        "document_similarity_analysis()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Key Insights and Learning Reflections\n",
        "\n",
        "### Word Embeddings Understanding:\n",
        "\n",
        "1. **Distributional Hypothesis**:\n",
        "   - **Core Principle**: Words appearing in similar contexts have similar meanings\n",
        "   - **Vector Representation**: Maps discrete words to continuous vector space\n",
        "   - **Semantic Relationships**: Captured through geometric relationships in embedding space\n",
        "   - **Context Window**: Size affects local vs global context capture\n",
        "\n",
        "2. **Word2Vec vs GloVe Comparison**:\n",
        "   - **Word2Vec (Skip-gram)**:\n",
        "     - Prediction-based approach\n",
        "     - Local context windows\n",
        "     - Neural network training\n",
        "     - Good at capturing syntactic relationships\n",
        "   - **GloVe (Global Vectors)**:\n",
        "     - Count-based approach with prediction benefits\n",
        "     - Global co-occurrence statistics\n",
        "     - Matrix factorization foundation\n",
        "     - Better at capturing semantic relationships\n",
        "\n",
        "3. **Implementation Insights**:\n",
        "   - **Training Complexity**: Word2Vec computationally intensive due to softmax\n",
        "   - **Convergence**: Both models show steady loss reduction\n",
        "   - **Hyperparameters**: Embedding dimension, learning rate, window size critical\n",
        "   - **Vocabulary Size**: Affects computational requirements and embedding quality\n",
        "\n",
        "### Practical Applications:\n",
        "\n",
        "1. **Semantic Similarity**: Both models capture word relationships effectively\n",
        "2. **Document Representation**: Averaging word vectors creates meaningful document embeddings\n",
        "3. **Clustering**: Embeddings enable semantic clustering of words and documents\n",
        "4. **Analogy Tasks**: Vector arithmetic reveals semantic relationships\n",
        "\n",
        "### Technical Observations:\n",
        "\n",
        "1. **Vector Norms**: Different models produce different magnitude distributions\n",
        "2. **Similarity Distributions**: Reveal how models organize semantic space\n",
        "3. **Clustering Quality**: Measured through silhouette scores\n",
        "4. **Visualization**: t-SNE reveals semantic neighborhoods\n",
        "\n",
        "### Key Takeaways:\n",
        "- Word embeddings transform discrete text into continuous, meaningful representations\n",
        "- Different training objectives (prediction vs reconstruction) lead to different embedding characteristics\n",
        "- Quality depends on corpus size, diversity, and training parameters\n",
        "- Embeddings enable powerful downstream NLP applications\n",
        "- Vector arithmetic reveals fascinating linguistic relationships\n",
        "\n",
        "### Limitations and Considerations:\n",
        "- Small corpus limits embedding quality\n",
        "- Context window size affects relationship capture\n",
        "- Computational requirements scale with vocabulary size\n",
        "- Evaluation requires both intrinsic and extrinsic measures\n",
        "\n",
        "### Next Steps:\n",
        "This completes our Week 2 exploration of advanced ML techniques, from traditional methods to modern NLP!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
