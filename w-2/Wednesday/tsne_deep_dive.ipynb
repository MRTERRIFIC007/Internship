{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# t-SNE Deep Dive: Implementation Details\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the crowding problem in high-dimensional embeddings\n",
        "- Implement early exaggeration in t-SNE\n",
        "- Explore the role of Cauchy distribution vs Gaussian distribution\n",
        "- Analyze perplexity effects and parameter sensitivity\n",
        "- Compare t-SNE variants and optimizations\n",
        "- Understand the mathematical foundations deeply\n",
        "\n",
        "## Advanced t-SNE Theory\n",
        "\n",
        "### The Crowding Problem\n",
        "High-dimensional data points that are moderately distant become indistinguishable in lower dimensions due to volume constraints.\n",
        "\n",
        "### Early Exaggeration\n",
        "Multiply P (high-dim probabilities) by a factor > 1 in early iterations to create clusters before fine-tuning.\n",
        "\n",
        "### Distribution Choice\n",
        "- **High-dimensional**: Gaussian distribution (heavy tails)\n",
        "- **Low-dimensional**: Student's t-distribution (heavier tails)\n",
        "- **Why t-distribution**: Addresses crowding problem by allowing distant points to be placed further apart\n",
        "\n",
        "### Key Parameters:\n",
        "- **Perplexity**: Effective number of neighbors, typically 5-50\n",
        "- **Learning rate**: Controls step size, typically 100-1000\n",
        "- **Early exaggeration**: Multiplier for early iterations, typically 4-12\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_digits, make_swiss_roll, make_s_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import trustworthiness\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style and random seed\n",
        "plt.style.use('seaborn-v0_8')\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Advanced t-SNE Implementation with Early Exaggeration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdvancedTSNE:\n",
        "    \"\"\"Advanced t-SNE implementation with detailed analysis capabilities\"\"\"\n",
        "    \n",
        "    def __init__(self, n_components=2, perplexity=30, learning_rate=200, \n",
        "                 n_iter=1000, early_exaggeration=12, min_grad_norm=1e-7):\n",
        "        self.n_components = n_components\n",
        "        self.perplexity = perplexity\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iter = n_iter\n",
        "        self.early_exaggeration = early_exaggeration\n",
        "        self.min_grad_norm = min_grad_norm\n",
        "        \n",
        "        # Training history\n",
        "        self.costs = []\n",
        "        self.grad_norms = []\n",
        "        self.embedding_history = []\n",
        "        self.P_matrix = None\n",
        "        \n",
        "    def _pairwise_distances(self, X):\n",
        "        \"\"\"Compute pairwise squared Euclidean distances efficiently\"\"\"\n",
        "        sum_X = np.sum(X**2, axis=1)\n",
        "        distances = sum_X[:, np.newaxis] + sum_X[np.newaxis, :] - 2 * np.dot(X, X.T)\n",
        "        return np.maximum(distances, 0)\n",
        "    \n",
        "    def _search_optimal_sigma(self, distances_i, target_perplexity, tol=1e-5, max_iter=50):\n",
        "        \"\"\"Binary search for optimal sigma for given perplexity\"\"\"\n",
        "        beta_min = -np.inf\n",
        "        beta_max = np.inf\n",
        "        beta = 1.0\n",
        "        \n",
        "        for _ in range(max_iter):\n",
        "            # Compute conditional probabilities\n",
        "            Pi = np.exp(-beta * distances_i)\n",
        "            sum_Pi = np.sum(Pi)\n",
        "            \n",
        "            if sum_Pi == 0:\n",
        "                Pi = np.ones_like(Pi) / len(Pi)\n",
        "                sum_Pi = 1.0\n",
        "            else:\n",
        "                Pi = Pi / sum_Pi\n",
        "            \n",
        "            # Compute perplexity\n",
        "            H = -np.sum(Pi * np.log2(Pi + 1e-12))\n",
        "            perplexity = 2**H\n",
        "            \n",
        "            # Check convergence\n",
        "            perp_diff = perplexity - target_perplexity\n",
        "            if abs(perp_diff) < tol:\n",
        "                break\n",
        "            \n",
        "            # Update beta\n",
        "            if perp_diff > 0:\n",
        "                beta_min = beta\n",
        "                if beta_max == np.inf:\n",
        "                    beta = beta * 2\n",
        "                else:\n",
        "                    beta = (beta + beta_max) / 2\n",
        "            else:\n",
        "                beta_max = beta\n",
        "                if beta_min == -np.inf:\n",
        "                    beta = beta / 2\n",
        "                else:\n",
        "                    beta = (beta + beta_min) / 2\n",
        "        \n",
        "        return beta\n",
        "    \n",
        "    def _compute_joint_probabilities(self, X):\n",
        "        \"\"\"Compute joint probabilities in high-dimensional space with detailed logging\"\"\"\n",
        "        n = X.shape[0]\n",
        "        distances = self._pairwise_distances(X)\n",
        "        \n",
        "        # Set diagonal to infinity to exclude self-distances\n",
        "        np.fill_diagonal(distances, np.inf)\n",
        "        \n",
        "        # Compute conditional probabilities for each point\n",
        "        P = np.zeros((n, n))\n",
        "        sigmas = np.zeros(n)\n",
        "        \n",
        "        print(\"Computing optimal sigmas for each point...\")\n",
        "        for i in range(n):\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Processing point {i}/{n}\")\n",
        "            \n",
        "            # Binary search for optimal sigma\n",
        "            beta = self._search_optimal_sigma(distances[i], self.perplexity)\n",
        "            sigmas[i] = 1.0 / np.sqrt(2.0 * beta)\n",
        "            \n",
        "            # Compute conditional probabilities\n",
        "            Pi = np.exp(-beta * distances[i])\n",
        "            Pi[i] = 0  # Exclude self\n",
        "            Pi = Pi / np.sum(Pi)\n",
        "            P[i] = Pi\n",
        "        \n",
        "        # Symmetrize to get joint probabilities\n",
        "        P_joint = (P + P.T) / (2.0 * n)\n",
        "        P_joint = np.maximum(P_joint, 1e-12)\n",
        "        \n",
        "        print(f\"Sigma statistics: mean={np.mean(sigmas):.4f}, std={np.std(sigmas):.4f}\")\n",
        "        print(f\"Perplexity range achieved: {2**(-np.sum(P * np.log2(P + 1e-12), axis=1)).min():.2f} - {2**(-np.sum(P * np.log2(P + 1e-12), axis=1)).max():.2f}\")\n",
        "        \n",
        "        return P_joint, sigmas\n",
        "    \n",
        "    def _compute_low_dim_probabilities(self, Y):\n",
        "        \"\"\"Compute probabilities in low-dimensional space using Student's t-distribution\"\"\"\n",
        "        distances = self._pairwise_distances(Y)\n",
        "        \n",
        "        # Student's t-distribution with 1 degree of freedom\n",
        "        Q = 1.0 / (1.0 + distances)\n",
        "        np.fill_diagonal(Q, 0)\n",
        "        \n",
        "        # Normalize\n",
        "        Q = Q / np.sum(Q)\n",
        "        Q = np.maximum(Q, 1e-12)\n",
        "        \n",
        "        return Q\n",
        "    \n",
        "    def _compute_gradient(self, P, Q, Y):\n",
        "        \"\"\"Compute gradient with detailed force analysis\"\"\"\n",
        "        n = Y.shape[0]\n",
        "        \n",
        "        # Attractive and repulsive forces\n",
        "        PQ_diff = P - Q\n",
        "        distances = self._pairwise_distances(Y)\n",
        "        inv_distances = 1.0 / (1.0 + distances)\n",
        "        \n",
        "        # Compute gradient\n",
        "        gradient = np.zeros_like(Y)\n",
        "        \n",
        "        for i in range(n):\n",
        "            diff = Y[i] - Y\n",
        "            forces = (PQ_diff[i] * inv_distances[i])[:, np.newaxis] * diff\n",
        "            gradient[i] = 4 * np.sum(forces, axis=0)\n",
        "        \n",
        "        return gradient\n",
        "    \n",
        "    def _analyze_forces(self, P, Q, Y, iteration):\n",
        "        \"\"\"Analyze attractive vs repulsive forces\"\"\"\n",
        "        n = Y.shape[0]\n",
        "        distances = self._pairwise_distances(Y)\n",
        "        \n",
        "        # Compute forces\n",
        "        attractive_force = np.sum(P * distances)\n",
        "        repulsive_force = np.sum(Q * distances)\n",
        "        \n",
        "        # Compute KL divergence components\n",
        "        kl_div = np.sum(P * np.log(P / Q))\n",
        "        \n",
        "        if iteration % 100 == 0:\n",
        "            print(f\"Iteration {iteration}: KL={kl_div:.4f}, Attractive={attractive_force:.4f}, Repulsive={repulsive_force:.4f}\")\n",
        "        \n",
        "        return {\n",
        "            'kl_divergence': kl_div,\n",
        "            'attractive_force': attractive_force,\n",
        "            'repulsive_force': repulsive_force\n",
        "        }\n",
        "    \n",
        "    def fit_transform(self, X):\n",
        "        \"\"\"Fit t-SNE with early exaggeration and detailed monitoring\"\"\"\n",
        "        n, d = X.shape\n",
        "        print(f\"Fitting t-SNE on {n} points with {d} dimensions...\")\n",
        "        \n",
        "        # Compute joint probabilities\n",
        "        P, sigmas = self._compute_joint_probabilities(X)\n",
        "        self.P_matrix = P.copy()\n",
        "        \n",
        "        # Initialize embedding\n",
        "        Y = np.random.normal(0, 1e-4, (n, self.n_components))\n",
        "        \n",
        "        # Momentum terms for optimization\n",
        "        Y_momentum = np.zeros_like(Y)\n",
        "        \n",
        "        print(\"Starting optimization...\")\n",
        "        for iteration in range(self.n_iter):\n",
        "            # Apply early exaggeration\n",
        "            if iteration < 250:\n",
        "                P_current = P * self.early_exaggeration\n",
        "            else:\n",
        "                P_current = P.copy()\n",
        "            \n",
        "            # Compute low-dimensional probabilities\n",
        "            Q = self._compute_low_dim_probabilities(Y)\n",
        "            \n",
        "            # Analyze forces\n",
        "            force_analysis = self._analyze_forces(P_current, Q, Y, iteration)\n",
        "            \n",
        "            # Compute cost (KL divergence)\n",
        "            cost = np.sum(P_current * np.log(P_current / Q))\n",
        "            self.costs.append(cost)\n",
        "            \n",
        "            # Compute gradient\n",
        "            gradient = self._compute_gradient(P_current, Q, Y)\n",
        "            grad_norm = np.linalg.norm(gradient)\n",
        "            self.grad_norms.append(grad_norm)\n",
        "            \n",
        "            # Early stopping if gradient is too small\n",
        "            if grad_norm < self.min_grad_norm:\n",
        "                print(f\"Stopping early at iteration {iteration} due to small gradient norm\")\n",
        "                break\n",
        "            \n",
        "            # Update with momentum\n",
        "            if iteration < 250:\n",
        "                momentum = 0.5\n",
        "                lr = self.learning_rate\n",
        "            else:\n",
        "                momentum = 0.8\n",
        "                lr = self.learning_rate\n",
        "            \n",
        "            Y_momentum = momentum * Y_momentum - lr * gradient\n",
        "            Y = Y + Y_momentum\n",
        "            \n",
        "            # Center the embedding\n",
        "            Y = Y - np.mean(Y, axis=0)\n",
        "            \n",
        "            # Store embedding history (every 50 iterations to save memory)\n",
        "            if iteration % 50 == 0:\n",
        "                self.embedding_history.append(Y.copy())\n",
        "            \n",
        "        print(f\"Optimization completed after {len(self.costs)} iterations\")\n",
        "        return Y\n",
        "\n",
        "# Analyze perplexity effects\n",
        "def analyze_perplexity_effects():\n",
        "    \"\"\"Analyze how different perplexity values affect t-SNE embeddings\"\"\"\n",
        "    \n",
        "    # Load digits dataset for analysis\n",
        "    digits = load_digits()\n",
        "    X = digits.data\n",
        "    y = digits.target\n",
        "    \n",
        "    # Use subset for faster computation\n",
        "    n_samples = 500\n",
        "    indices = np.random.choice(len(X), n_samples, replace=False)\n",
        "    X_subset = X[indices]\n",
        "    y_subset = y[indices]\n",
        "    \n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_subset)\n",
        "    \n",
        "    # Test different perplexity values\n",
        "    perplexities = [5, 15, 30, 50, 100]\n",
        "    \n",
        "    results = {}\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, perp in enumerate(perplexities):\n",
        "        print(f\"\\\\nTesting perplexity = {perp}\")\n",
        "        \n",
        "        # Fit t-SNE\n",
        "        tsne = AdvancedTSNE(perplexity=perp, n_iter=500, learning_rate=200)\n",
        "        Y = tsne.fit_transform(X_scaled)\n",
        "        \n",
        "        # Calculate trustworthiness\n",
        "        trust_score = trustworthiness(X_scaled, Y, n_neighbors=min(10, perp//2))\n",
        "        \n",
        "        results[perp] = {\n",
        "            'embedding': Y,\n",
        "            'trustworthiness': trust_score,\n",
        "            'final_cost': tsne.costs[-1],\n",
        "            'tsne': tsne\n",
        "        }\n",
        "        \n",
        "        # Plot embedding\n",
        "        scatter = axes[i].scatter(Y[:, 0], Y[:, 1], c=y_subset, cmap='tab10', alpha=0.7)\n",
        "        axes[i].set_title(f'Perplexity = {perp}\\\\nTrust: {trust_score:.3f}')\n",
        "        axes[i].set_xlabel('t-SNE 1')\n",
        "        axes[i].set_ylabel('t-SNE 2')\n",
        "    \n",
        "    # Plot perplexity comparison\n",
        "    axes[5].set_visible(False)\n",
        "    \n",
        "    # Create comparison plot\n",
        "    fig2, axes2 = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    \n",
        "    # Trustworthiness vs perplexity\n",
        "    perps = list(results.keys())\n",
        "    trusts = [results[p]['trustworthiness'] for p in perps]\n",
        "    costs = [results[p]['final_cost'] for p in perps]\n",
        "    \n",
        "    axes2[0].plot(perps, trusts, 'o-', linewidth=2, markersize=8)\n",
        "    axes2[0].set_xlabel('Perplexity')\n",
        "    axes2[0].set_ylabel('Trustworthiness')\n",
        "    axes2[0].set_title('Trustworthiness vs Perplexity')\n",
        "    axes2[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Final cost vs perplexity\n",
        "    axes2[1].plot(perps, costs, 'o-', linewidth=2, markersize=8, color='red')\n",
        "    axes2[1].set_xlabel('Perplexity')\n",
        "    axes2[1].set_ylabel('Final KL Divergence')\n",
        "    axes2[1].set_title('Final Cost vs Perplexity')\n",
        "    axes2[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Cost evolution for different perplexities\n",
        "    for perp in [5, 30, 100]:\n",
        "        costs_evolution = results[perp]['tsne'].costs\n",
        "        axes2[2].plot(costs_evolution, label=f'Perplexity {perp}', alpha=0.8)\n",
        "    \n",
        "    axes2[2].set_xlabel('Iteration')\n",
        "    axes2[2].set_ylabel('KL Divergence')\n",
        "    axes2[2].set_title('Cost Evolution')\n",
        "    axes2[2].legend()\n",
        "    axes2[2].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run perplexity analysis\n",
        "print(\"=== Perplexity Analysis ===\")\n",
        "perplexity_results = analyze_perplexity_effects()\n",
        "\n",
        "print(\"\\\\n=== Perplexity Results Summary ===\")\n",
        "for perp, result in perplexity_results.items():\n",
        "    print(f\"\\\\nPerplexity {perp}:\")\n",
        "    print(f\"  Trustworthiness: {result['trustworthiness']:.4f}\")\n",
        "    print(f\"  Final Cost: {result['final_cost']:.4f}\")\n",
        "    print(f\"  Convergence iterations: {len(result['tsne'].costs)}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Early Exaggeration Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare early exaggeration effects\n",
        "def analyze_early_exaggeration():\n",
        "    \"\"\"Analyze the effects of early exaggeration on t-SNE convergence\"\"\"\n",
        "    \n",
        "    # Create a challenging dataset (Swiss roll)\n",
        "    X, color = make_swiss_roll(n_samples=300, noise=0.1, random_state=42)\n",
        "    \n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    \n",
        "    # Test different early exaggeration values\n",
        "    exaggeration_values = [1, 4, 12, 20, 50]  # 1 means no exaggeration\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, exag in enumerate(exaggeration_values):\n",
        "        print(f\"\\\\nTesting early exaggeration = {exag}\")\n",
        "        \n",
        "        # Fit t-SNE with different exaggeration\n",
        "        tsne = AdvancedTSNE(perplexity=30, n_iter=600, early_exaggeration=exag, learning_rate=200)\n",
        "        Y = tsne.fit_transform(X_scaled)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        trust_score = trustworthiness(X_scaled, Y, n_neighbors=10)\n",
        "        final_cost = tsne.costs[-1]\n",
        "        \n",
        "        results[exag] = {\n",
        "            'embedding': Y,\n",
        "            'trustworthiness': trust_score,\n",
        "            'final_cost': final_cost,\n",
        "            'costs': tsne.costs,\n",
        "            'grad_norms': tsne.grad_norms\n",
        "        }\n",
        "        \n",
        "        # Plot embedding\n",
        "        scatter = axes[i].scatter(Y[:, 0], Y[:, 1], c=color, cmap='viridis', alpha=0.7)\n",
        "        axes[i].set_title(f'Early Exag = {exag}\\\\nTrust: {trust_score:.3f}')\n",
        "        axes[i].set_xlabel('t-SNE 1')\n",
        "        axes[i].set_ylabel('t-SNE 2')\n",
        "        \n",
        "        if i >= len(exaggeration_values):\n",
        "            axes[i].set_visible(False)\n",
        "    \n",
        "    # Additional analysis plots\n",
        "    fig2, axes2 = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Cost evolution\n",
        "    axes2[0, 0].set_title('Cost Evolution')\n",
        "    for exag in exaggeration_values:\n",
        "        axes2[0, 0].plot(results[exag]['costs'], label=f'Exag {exag}', alpha=0.8)\n",
        "    axes2[0, 0].set_xlabel('Iteration')\n",
        "    axes2[0, 0].set_ylabel('KL Divergence')\n",
        "    axes2[0, 0].legend()\n",
        "    axes2[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Gradient norm evolution\n",
        "    axes2[0, 1].set_title('Gradient Norm Evolution')\n",
        "    for exag in exaggeration_values:\n",
        "        grad_norms = results[exag]['grad_norms']\n",
        "        axes2[0, 1].plot(grad_norms, label=f'Exag {exag}', alpha=0.8)\n",
        "    axes2[0, 1].set_xlabel('Iteration')\n",
        "    axes2[0, 1].set_ylabel('Gradient Norm')\n",
        "    axes2[0, 1].set_yscale('log')\n",
        "    axes2[0, 1].legend()\n",
        "    axes2[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Final metrics comparison\n",
        "    exag_vals = list(results.keys())\n",
        "    trusts = [results[e]['trustworthiness'] for e in exag_vals]\n",
        "    costs = [results[e]['final_cost'] for e in exag_vals]\n",
        "    \n",
        "    axes2[1, 0].bar(range(len(exag_vals)), trusts, alpha=0.7, color='skyblue')\n",
        "    axes2[1, 0].set_xlabel('Early Exaggeration')\n",
        "    axes2[1, 0].set_ylabel('Trustworthiness')\n",
        "    axes2[1, 0].set_title('Final Trustworthiness')\n",
        "    axes2[1, 0].set_xticks(range(len(exag_vals)))\n",
        "    axes2[1, 0].set_xticklabels(exag_vals)\n",
        "    \n",
        "    # Add values on bars\n",
        "    for i, trust in enumerate(trusts):\n",
        "        axes2[1, 0].text(i, trust + 0.01, f'{trust:.3f}', ha='center')\n",
        "    \n",
        "    axes2[1, 1].bar(range(len(exag_vals)), costs, alpha=0.7, color='lightcoral')\n",
        "    axes2[1, 1].set_xlabel('Early Exaggeration')\n",
        "    axes2[1, 1].set_ylabel('Final KL Divergence')\n",
        "    axes2[1, 1].set_title('Final Cost')\n",
        "    axes2[1, 1].set_xticks(range(len(exag_vals)))\n",
        "    axes2[1, 1].set_xticklabels(exag_vals)\n",
        "    \n",
        "    # Add values on bars\n",
        "    for i, cost in enumerate(costs):\n",
        "        axes2[1, 1].text(i, cost + 0.05, f'{cost:.2f}', ha='center')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run early exaggeration analysis\n",
        "print(\"=== Early Exaggeration Analysis ===\")\n",
        "exaggeration_results = analyze_early_exaggeration()\n",
        "\n",
        "print(\"\\\\n=== Early Exaggeration Results Summary ===\")\n",
        "for exag, result in exaggeration_results.items():\n",
        "    print(f\"\\\\nEarly Exaggeration {exag}:\")\n",
        "    print(f\"  Trustworthiness: {result['trustworthiness']:.4f}\")\n",
        "    print(f\"  Final Cost: {result['final_cost']:.4f}\")\n",
        "    print(f\"  Final Gradient Norm: {result['grad_norms'][-1]:.6f}\")\n",
        "    \n",
        "    # Analyze convergence speed\n",
        "    costs = np.array(result['costs'])\n",
        "    initial_cost = costs[0]\n",
        "    final_cost = costs[-1]\n",
        "    improvement = initial_cost - final_cost\n",
        "    print(f\"  Total Improvement: {improvement:.4f}\")\n",
        "    print(f\"  Convergence Rate: {improvement / len(costs):.6f} per iteration\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Crowding Problem Demonstration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate the crowding problem and how different distributions address it\n",
        "def demonstrate_crowding_problem():\n",
        "    \"\"\"Demonstrate the crowding problem and distribution effects\"\"\"\n",
        "    \n",
        "    # Create high-dimensional dataset with known structure\n",
        "    np.random.seed(42)\n",
        "    n_clusters = 5\n",
        "    n_points_per_cluster = 50\n",
        "    dims = 100\n",
        "    \n",
        "    # Generate clusters in high-dimensional space\n",
        "    X = []\n",
        "    y = []\n",
        "    cluster_centers = np.random.randn(n_clusters, dims) * 3\n",
        "    \n",
        "    for i in range(n_clusters):\n",
        "        cluster_points = cluster_centers[i] + np.random.randn(n_points_per_cluster, dims) * 0.5\n",
        "        X.append(cluster_points)\n",
        "        y.extend([i] * n_points_per_cluster)\n",
        "    \n",
        "    X = np.vstack(X)\n",
        "    y = np.array(y)\n",
        "    \n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    \n",
        "    print(f\"Dataset: {X_scaled.shape[0]} points in {X_scaled.shape[1]} dimensions\")\n",
        "    \n",
        "    # Analyze distance distributions in high-dimensional space\n",
        "    distances_hd = []\n",
        "    n_samples = X_scaled.shape[0]\n",
        "    \n",
        "    # Sample distances to avoid memory issues\n",
        "    sample_size = min(1000, n_samples // 2)\n",
        "    indices = np.random.choice(n_samples, sample_size, replace=False)\n",
        "    \n",
        "    for i in indices:\n",
        "        for j in range(i+1, min(i+50, n_samples)):  # Limit to avoid too many distances\n",
        "            dist = np.linalg.norm(X_scaled[i] - X_scaled[j])\n",
        "            distances_hd.append(dist)\n",
        "    \n",
        "    distances_hd = np.array(distances_hd)\n",
        "    \n",
        "    # Compare different embedding methods\n",
        "    methods = {\n",
        "        'PCA': PCA(n_components=2),\n",
        "        't-SNE (Gaussian)': 't-sne-gaussian',  # We'll implement this\n",
        "        't-SNE (Student-t)': TSNE(n_components=2, perplexity=30, random_state=42)\n",
        "    }\n",
        "    \n",
        "    embeddings = {}\n",
        "    \n",
        "    # Fit embeddings\n",
        "    for name, method in methods.items():\n",
        "        print(f\"\\\\nFitting {name}...\")\n",
        "        \n",
        "        if name == 'PCA':\n",
        "            embedding = method.fit_transform(X_scaled)\n",
        "        elif name == 't-SNE (Gaussian)':\n",
        "            # We'll use a modified version that uses Gaussian in low-dim space\n",
        "            embedding = fit_tsne_gaussian(X_scaled)\n",
        "        else:\n",
        "            embedding = method.fit_transform(X_scaled)\n",
        "        \n",
        "        embeddings[name] = embedding\n",
        "    \n",
        "    # Analyze distance preservation\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "    \n",
        "    # Plot embeddings\n",
        "    for i, (name, embedding) in enumerate(embeddings.items()):\n",
        "        ax = axes[0, i]\n",
        "        scatter = ax.scatter(embedding[:, 0], embedding[:, 1], c=y, cmap='tab10', alpha=0.7)\n",
        "        ax.set_title(f'{name}')\n",
        "        ax.set_xlabel('Component 1')\n",
        "        ax.set_ylabel('Component 2')\n",
        "    \n",
        "    # Plot distance distributions\n",
        "    ax_dist = axes[0, 3]\n",
        "    ax_dist.hist(distances_hd, bins=50, alpha=0.7, density=True, label='High-dim distances')\n",
        "    ax_dist.set_xlabel('Distance')\n",
        "    ax_dist.set_ylabel('Density')\n",
        "    ax_dist.set_title('High-Dim Distance Distribution')\n",
        "    ax_dist.legend()\n",
        "    \n",
        "    # Analyze distance preservation for each method\n",
        "    for i, (name, embedding) in enumerate(embeddings.items()):\n",
        "        ax = axes[1, i]\n",
        "        \n",
        "        # Compute distances in embedding space\n",
        "        distances_ld = []\n",
        "        for idx1 in indices[:min(len(indices), 100)]:  # Limit for computation\n",
        "            for idx2 in range(idx1+1, min(idx1+50, n_samples)):\n",
        "                if idx2 < len(embedding):\n",
        "                    dist = np.linalg.norm(embedding[idx1] - embedding[idx2])\n",
        "                    distances_ld.append(dist)\n",
        "        \n",
        "        distances_ld = np.array(distances_ld)\n",
        "        \n",
        "        # Plot distance comparison\n",
        "        ax.scatter(distances_hd[:len(distances_ld)], distances_ld, alpha=0.3)\n",
        "        ax.plot([0, distances_hd.max()], [0, distances_hd.max()], 'r--', alpha=0.5)\n",
        "        ax.set_xlabel('High-dim Distance')\n",
        "        ax.set_ylabel('Low-dim Distance')\n",
        "        ax.set_title(f'{name}\\\\nDistance Preservation')\n",
        "        \n",
        "        # Calculate correlation\n",
        "        if len(distances_ld) > 0:\n",
        "            corr = np.corrcoef(distances_hd[:len(distances_ld)], distances_ld)[0, 1]\n",
        "            ax.text(0.05, 0.95, f'Corr: {corr:.3f}', transform=ax.transAxes, \n",
        "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "    \n",
        "    # Distance distribution comparison\n",
        "    ax_compare = axes[1, 3]\n",
        "    \n",
        "    # Plot Gaussian vs Student-t distributions\n",
        "    x = np.linspace(-4, 4, 1000)\n",
        "    gaussian = (1/np.sqrt(2*np.pi)) * np.exp(-0.5 * x**2)\n",
        "    student_t = (1/np.pi) * (1 / (1 + x**2))  # Cauchy distribution (t with 1 df)\n",
        "    \n",
        "    ax_compare.plot(x, gaussian, label='Gaussian', linewidth=2)\n",
        "    ax_compare.plot(x, student_t, label='Student-t (df=1)', linewidth=2)\n",
        "    ax_compare.set_xlabel('Value')\n",
        "    ax_compare.set_ylabel('Probability Density')\n",
        "    ax_compare.set_title('Distribution Comparison')\n",
        "    ax_compare.legend()\n",
        "    ax_compare.set_yscale('log')\n",
        "    ax_compare.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print analysis\n",
        "    print(\"\\\\n=== Crowding Problem Analysis ===\")\n",
        "    print(f\"High-dimensional distance statistics:\")\n",
        "    print(f\"  Mean: {np.mean(distances_hd):.4f}\")\n",
        "    print(f\"  Std: {np.std(distances_hd):.4f}\")\n",
        "    print(f\"  Min: {np.min(distances_hd):.4f}\")\n",
        "    print(f\"  Max: {np.max(distances_hd):.4f}\")\n",
        "    \n",
        "    print(\"\\\\nKey Insights:\")\n",
        "    print(\"- High-dimensional distances become more uniform (curse of dimensionality)\")\n",
        "    print(\"- Student-t distribution has heavier tails than Gaussian\")\n",
        "    print(\"- Heavy tails allow distant points to be placed further apart in 2D\")\n",
        "    print(\"- This helps address the crowding problem in t-SNE\")\n",
        "    \n",
        "    return embeddings, distances_hd\n",
        "\n",
        "def fit_tsne_gaussian(X, perplexity=30, n_iter=300):\n",
        "    \"\"\"Simplified t-SNE using Gaussian distribution in low-dimensional space\"\"\"\n",
        "    n = X.shape[0]\n",
        "    \n",
        "    # Compute high-dimensional probabilities (same as regular t-SNE)\n",
        "    distances = np.sum((X[:, np.newaxis] - X[np.newaxis, :]) ** 2, axis=2)\n",
        "    np.fill_diagonal(distances, np.inf)\n",
        "    \n",
        "    # Simple approach: use fixed sigma for all points\n",
        "    sigma = np.sqrt(perplexity / 2)\n",
        "    P = np.exp(-distances / (2 * sigma**2))\n",
        "    np.fill_diagonal(P, 0)\n",
        "    P = P / np.sum(P, axis=1, keepdims=True)\n",
        "    P = (P + P.T) / (2 * n)\n",
        "    P = np.maximum(P, 1e-12)\n",
        "    \n",
        "    # Initialize embedding\n",
        "    Y = np.random.normal(0, 1e-4, (n, 2))\n",
        "    \n",
        "    # Optimize using Gaussian distribution in low-dimensional space\n",
        "    for i in range(n_iter):\n",
        "        # Compute low-dimensional probabilities using Gaussian\n",
        "        distances_ld = np.sum((Y[:, np.newaxis] - Y[np.newaxis, :]) ** 2, axis=2)\n",
        "        Q = np.exp(-distances_ld)\n",
        "        np.fill_diagonal(Q, 0)\n",
        "        Q = Q / np.sum(Q)\n",
        "        Q = np.maximum(Q, 1e-12)\n",
        "        \n",
        "        # Compute gradient\n",
        "        PQ_diff = P - Q\n",
        "        gradient = np.zeros_like(Y)\n",
        "        \n",
        "        for j in range(n):\n",
        "            diff = Y[j] - Y\n",
        "            gradient[j] = 4 * np.sum((PQ_diff[j] * np.exp(-distances_ld[j]))[:, np.newaxis] * diff, axis=0)\n",
        "        \n",
        "        # Update with simple gradient descent\n",
        "        Y -= 100 * gradient\n",
        "        Y = Y - np.mean(Y, axis=0)  # Center\n",
        "    \n",
        "    return Y\n",
        "\n",
        "# Run crowding problem demonstration\n",
        "print(\"=== Crowding Problem Demonstration ===\")\n",
        "crowding_results, hd_distances = demonstrate_crowding_problem()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Key Insights and Deep Understanding\n",
        "\n",
        "### Understanding the Crowding Problem:\n",
        "\n",
        "1. **The Problem**: In high dimensions, most points are roughly equidistant due to curse of dimensionality\n",
        "2. **The Challenge**: Mapping these similar distances to 2D creates crowding in the center\n",
        "3. **The Solution**: Student-t distribution's heavy tails allow distant points to be placed further apart\n",
        "\n",
        "### Early Exaggeration Benefits:\n",
        "\n",
        "1. **Cluster Formation**: Early exaggeration (multiply P by 4-12) creates tight clusters initially\n",
        "2. **Fine-Tuning**: Later iterations without exaggeration refine local structure\n",
        "3. **Convergence**: Helps escape poor local minima and improves final embedding quality\n",
        "\n",
        "### Perplexity Insights:\n",
        "\n",
        "1. **Small Perplexity (5-15)**: \n",
        "   - Focuses on very local structure\n",
        "   - Creates tight, well-separated clusters\n",
        "   - May miss global structure\n",
        "\n",
        "2. **Medium Perplexity (30-50)**:\n",
        "   - Good balance between local and global structure\n",
        "   - Generally recommended default range\n",
        "   - Works well for most datasets\n",
        "\n",
        "3. **Large Perplexity (100+)**:\n",
        "   - Emphasizes global structure\n",
        "   - May lose fine local details\n",
        "   - Can become computationally expensive\n",
        "\n",
        "### Distribution Choice Impact:\n",
        "\n",
        "1. **Gaussian in Both Spaces**:\n",
        "   - Suffers from crowding problem\n",
        "   - Points tend to cluster in center\n",
        "   - Poor distance preservation\n",
        "\n",
        "2. **Gaussian â†’ Student-t**:\n",
        "   - Heavy tails in low-dimensional space\n",
        "   - Better separation of distant clusters\n",
        "   - Addresses crowding effectively\n",
        "\n",
        "### Practical Recommendations:\n",
        "\n",
        "1. **Parameter Selection**:\n",
        "   - Start with perplexity = 30 for most datasets\n",
        "   - Use early exaggeration = 12 for first 250 iterations\n",
        "   - Learning rate = 200 works well for most cases\n",
        "\n",
        "2. **Convergence Monitoring**:\n",
        "   - Watch KL divergence decrease\n",
        "   - Monitor gradient norm for convergence\n",
        "   - Early stopping when gradient norm < 1e-7\n",
        "\n",
        "3. **Interpretation Guidelines**:\n",
        "   - Cluster separations are meaningful\n",
        "   - Distances within clusters less reliable\n",
        "   - Global structure may be distorted\n",
        "   - Multiple runs recommended for stability\n",
        "\n",
        "### Tomorrow's Exploration:\n",
        "We'll explore UMAP as an alternative to t-SNE, transfer learning techniques, and advanced computer vision methods!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
