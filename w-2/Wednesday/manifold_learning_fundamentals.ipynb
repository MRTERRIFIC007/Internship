{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Manifold Learning Fundamentals\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the manifold hypothesis in machine learning\n",
        "- Explore different manifold learning techniques\n",
        "- Implement t-SNE from scratch\n",
        "- Compare linear vs non-linear dimensionality reduction\n",
        "- Visualize high-dimensional data in 2D/3D space\n",
        "- Understand the mathematical foundations of manifold learning\n",
        "\n",
        "## Theory Overview\n",
        "\n",
        "### The Manifold Hypothesis\n",
        "High-dimensional data often lies on or near a lower-dimensional manifold embedded in the high-dimensional space.\n",
        "\n",
        "### Key Concepts:\n",
        "- **Manifold**: A topological space that locally resembles Euclidean space\n",
        "- **Intrinsic Dimensionality**: The true dimensionality of the data\n",
        "- **Embedding**: Mapping from high-dimensional to low-dimensional space\n",
        "- **Neighborhood Preservation**: Maintaining local structure during dimensionality reduction\n",
        "\n",
        "### Common Techniques:\n",
        "1. **Linear**: PCA, MDS\n",
        "2. **Non-linear**: t-SNE, UMAP, Isomap, LLE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_swiss_roll, make_s_curve, load_digits, fetch_openml\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE, MDS, Isomap, LocallyLinearEmbedding\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import trustworthiness\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Creating Manifold Datasets\n",
        "\n",
        "Let's create various datasets that lie on known manifolds to test our techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create manifold datasets\n",
        "def create_manifold_datasets():\n",
        "    \"\"\"Create various manifold datasets for testing\"\"\"\n",
        "    \n",
        "    # 1. Swiss Roll - 2D manifold in 3D space\n",
        "    X_swiss, color_swiss = make_swiss_roll(n_samples=1000, noise=0.1, random_state=42)\n",
        "    \n",
        "    # 2. S-Curve - 2D manifold in 3D space\n",
        "    X_s_curve, color_s_curve = make_s_curve(n_samples=1000, noise=0.1, random_state=42)\n",
        "    \n",
        "    # 3. Create a 2D spiral that we'll embed in higher dimensions\n",
        "    t = np.linspace(0, 4*np.pi, 1000)\n",
        "    X_spiral_2d = np.column_stack([\n",
        "        t * np.cos(t),\n",
        "        t * np.sin(t)\n",
        "    ])\n",
        "    \n",
        "    # Embed spiral in higher dimensions with noise\n",
        "    X_spiral_hd = np.column_stack([\n",
        "        X_spiral_2d,\n",
        "        np.random.normal(0, 0.1, (1000, 8))  # Add 8 noise dimensions\n",
        "    ])\n",
        "    \n",
        "    # 4. Load digits dataset (high-dimensional real data)\n",
        "    digits = load_digits()\n",
        "    X_digits = digits.data\n",
        "    y_digits = digits.target\n",
        "    \n",
        "    # Standardize datasets\n",
        "    scaler_swiss = StandardScaler()\n",
        "    X_swiss_scaled = scaler_swiss.fit_transform(X_swiss)\n",
        "    \n",
        "    scaler_spiral = StandardScaler()\n",
        "    X_spiral_scaled = scaler_spiral.fit_transform(X_spiral_hd)\n",
        "    \n",
        "    scaler_digits = StandardScaler()\n",
        "    X_digits_scaled = scaler_digits.fit_transform(X_digits)\n",
        "    \n",
        "    return {\n",
        "        'swiss_roll': (X_swiss_scaled, color_swiss),\n",
        "        's_curve': (X_s_curve, color_s_curve),\n",
        "        'spiral': (X_spiral_scaled, t),\n",
        "        'digits': (X_digits_scaled, y_digits)\n",
        "    }\n",
        "\n",
        "# Create datasets\n",
        "datasets = create_manifold_datasets()\n",
        "\n",
        "# Visualize the 3D manifolds\n",
        "fig = plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Swiss Roll\n",
        "ax1 = fig.add_subplot(141, projection='3d')\n",
        "X_swiss, color_swiss = datasets['swiss_roll']\n",
        "ax1.scatter(X_swiss[:, 0], X_swiss[:, 1], X_swiss[:, 2], c=color_swiss, cmap='viridis')\n",
        "ax1.set_title('Swiss Roll Manifold')\n",
        "ax1.set_xlabel('X')\n",
        "ax1.set_ylabel('Y')\n",
        "ax1.set_zlabel('Z')\n",
        "\n",
        "# S-Curve\n",
        "ax2 = fig.add_subplot(142, projection='3d')\n",
        "X_s_curve, color_s_curve = datasets['s_curve']\n",
        "ax2.scatter(X_s_curve[:, 0], X_s_curve[:, 1], X_s_curve[:, 2], c=color_s_curve, cmap='viridis')\n",
        "ax2.set_title('S-Curve Manifold')\n",
        "ax2.set_xlabel('X')\n",
        "ax2.set_ylabel('Y')\n",
        "ax2.set_zlabel('Z')\n",
        "\n",
        "# High-dimensional spiral (show first 3 dimensions)\n",
        "ax3 = fig.add_subplot(143, projection='3d')\n",
        "X_spiral, t_spiral = datasets['spiral']\n",
        "ax3.scatter(X_spiral[:, 0], X_spiral[:, 1], X_spiral[:, 2], c=t_spiral, cmap='viridis')\n",
        "ax3.set_title('Spiral in High Dimensions')\n",
        "ax3.set_xlabel('X')\n",
        "ax3.set_ylabel('Y')\n",
        "ax3.set_zlabel('Z')\n",
        "\n",
        "# Digits (show some sample images)\n",
        "ax4 = fig.add_subplot(144)\n",
        "X_digits, y_digits = datasets['digits']\n",
        "# Reshape first digit back to 8x8 image\n",
        "digit_img = X_digits[0].reshape(8, 8)\n",
        "ax4.imshow(digit_img, cmap='gray')\n",
        "ax4.set_title(f'Sample Digit: {y_digits[0]}')\n",
        "ax4.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Manifold datasets created and visualized!\")\n",
        "print(f\"Swiss Roll: {X_swiss.shape} -> 2D manifold in 3D space\")\n",
        "print(f\"S-Curve: {X_s_curve.shape} -> 2D manifold in 3D space\")  \n",
        "print(f\"Spiral: {X_spiral.shape} -> 2D manifold in 10D space\")\n",
        "print(f\"Digits: {X_digits.shape} -> Manifold in 64D space\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Comparing Manifold Learning Techniques\n",
        "\n",
        "Let's compare different manifold learning techniques on our datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare manifold learning techniques\n",
        "def compare_manifold_techniques(X, colors, title, n_components=2):\n",
        "    \"\"\"Compare different manifold learning techniques\"\"\"\n",
        "    \n",
        "    # Reduce sample size for computational efficiency\n",
        "    if X.shape[0] > 1000:\n",
        "        indices = np.random.choice(X.shape[0], 1000, replace=False)\n",
        "        X_sample = X[indices]\n",
        "        colors_sample = colors[indices]\n",
        "    else:\n",
        "        X_sample = X\n",
        "        colors_sample = colors\n",
        "    \n",
        "    # Define techniques\n",
        "    techniques = {\n",
        "        'PCA': PCA(n_components=n_components),\n",
        "        't-SNE': TSNE(n_components=n_components, random_state=42, perplexity=30),\n",
        "        'MDS': MDS(n_components=n_components, random_state=42),\n",
        "        'Isomap': Isomap(n_components=n_components, n_neighbors=10),\n",
        "        'LLE': LocallyLinearEmbedding(n_components=n_components, n_neighbors=10, random_state=42)\n",
        "    }\n",
        "    \n",
        "    # Apply techniques and measure performance\n",
        "    results = {}\n",
        "    trustworthiness_scores = {}\n",
        "    \n",
        "    fig, axes = plt.subplots(1, len(techniques), figsize=(20, 4))\n",
        "    \n",
        "    for i, (name, technique) in enumerate(techniques.items()):\n",
        "        try:\n",
        "            # Fit and transform\n",
        "            X_embedded = technique.fit_transform(X_sample)\n",
        "            \n",
        "            # Calculate trustworthiness (higher is better)\n",
        "            trust_score = trustworthiness(X_sample, X_embedded, n_neighbors=10)\n",
        "            trustworthiness_scores[name] = trust_score\n",
        "            \n",
        "            # Plot\n",
        "            scatter = axes[i].scatter(X_embedded[:, 0], X_embedded[:, 1], \n",
        "                                    c=colors_sample, cmap='viridis', alpha=0.7)\n",
        "            axes[i].set_title(f'{name}\\\\nTrustworthiness: {trust_score:.3f}')\n",
        "            axes[i].set_xlabel('Component 1')\n",
        "            axes[i].set_ylabel('Component 2')\n",
        "            \n",
        "            results[name] = X_embedded\n",
        "            \n",
        "        except Exception as e:\n",
        "            axes[i].text(0.5, 0.5, f'{name}\\\\nFailed: {str(e)[:20]}...', \n",
        "                        ha='center', va='center', transform=axes[i].transAxes)\n",
        "            axes[i].set_title(f'{name}\\\\nFailed')\n",
        "            trustworthiness_scores[name] = 0\n",
        "    \n",
        "    plt.suptitle(f'Manifold Learning Comparison: {title}', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return results, trustworthiness_scores\n",
        "\n",
        "# Test on Swiss Roll\n",
        "print(\"=== Swiss Roll Analysis ===\")\n",
        "X_swiss, color_swiss = datasets['swiss_roll']\n",
        "swiss_results, swiss_trust = compare_manifold_techniques(X_swiss, color_swiss, \"Swiss Roll\")\n",
        "\n",
        "# Test on Spiral\n",
        "print(\"\\\\n=== High-Dimensional Spiral Analysis ===\")\n",
        "X_spiral, t_spiral = datasets['spiral']\n",
        "spiral_results, spiral_trust = compare_manifold_techniques(X_spiral, t_spiral, \"High-D Spiral\")\n",
        "\n",
        "# Test on Digits (subset for speed)\n",
        "print(\"\\\\n=== Digits Dataset Analysis ===\")\n",
        "X_digits, y_digits = datasets['digits']\n",
        "# Use only first 500 samples for speed\n",
        "X_digits_subset = X_digits[:500]\n",
        "y_digits_subset = y_digits[:500]\n",
        "digits_results, digits_trust = compare_manifold_techniques(X_digits_subset, y_digits_subset, \"Digits\")\n",
        "\n",
        "# Summary of trustworthiness scores\n",
        "print(\"\\\\n=== Trustworthiness Summary ===\")\n",
        "summary_df = pd.DataFrame({\n",
        "    'Swiss Roll': swiss_trust,\n",
        "    'High-D Spiral': spiral_trust,\n",
        "    'Digits': digits_trust\n",
        "}).T\n",
        "\n",
        "print(summary_df)\n",
        "\n",
        "# Plot trustworthiness comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "summary_df.plot(kind='bar', ax=plt.gca())\n",
        "plt.title('Trustworthiness Scores by Dataset and Method')\n",
        "plt.ylabel('Trustworthiness Score')\n",
        "plt.xlabel('Dataset')\n",
        "plt.legend(title='Method', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Simple t-SNE Implementation\n",
        "\n",
        "Let's implement a basic version of t-SNE to understand its mechanics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleTSNE:\n",
        "    \"\"\"Simplified t-SNE implementation for educational purposes\"\"\"\n",
        "    \n",
        "    def __init__(self, n_components=2, perplexity=30, learning_rate=200, n_iter=1000):\n",
        "        self.n_components = n_components\n",
        "        self.perplexity = perplexity\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iter = n_iter\n",
        "        self.costs = []\n",
        "        \n",
        "    def _pairwise_distances(self, X):\n",
        "        \"\"\"Compute pairwise squared Euclidean distances\"\"\"\n",
        "        sum_X = np.sum(X**2, axis=1)\n",
        "        D = sum_X[:, np.newaxis] + sum_X[np.newaxis, :] - 2 * np.dot(X, X.T)\n",
        "        return np.maximum(D, 0)  # Ensure non-negative\n",
        "    \n",
        "    def _compute_conditional_probabilities(self, distances, sigmas):\n",
        "        \"\"\"Compute conditional probabilities p_{j|i}\"\"\"\n",
        "        n = distances.shape[0]\n",
        "        P = np.zeros((n, n))\n",
        "        \n",
        "        for i in range(n):\n",
        "            # Compute probabilities for row i\n",
        "            beta = 1.0 / (2.0 * sigmas[i]**2)\n",
        "            \n",
        "            # Exclude self-distance\n",
        "            Di = distances[i].copy()\n",
        "            Di[i] = np.inf\n",
        "            \n",
        "            # Compute probabilities\n",
        "            Pi = np.exp(-beta * Di)\n",
        "            Pi[i] = 0\n",
        "            Pi = Pi / np.sum(Pi)\n",
        "            \n",
        "            P[i] = Pi\n",
        "            \n",
        "        return P\n",
        "    \n",
        "    def _binary_search_perplexity(self, distances, target_perplexity, tol=1e-5):\n",
        "        \"\"\"Binary search to find sigma that gives target perplexity\"\"\"\n",
        "        n = distances.shape[0]\n",
        "        sigmas = np.ones(n)\n",
        "        \n",
        "        for i in range(n):\n",
        "            # Binary search for sigma_i\n",
        "            beta_min = -np.inf\n",
        "            beta_max = np.inf\n",
        "            beta = 1.0\n",
        "            \n",
        "            # Exclude self-distance\n",
        "            Di = distances[i].copy()\n",
        "            Di[i] = np.inf\n",
        "            \n",
        "            for _ in range(50):  # Max iterations\n",
        "                # Compute probabilities\n",
        "                Pi = np.exp(-beta * Di)\n",
        "                Pi[i] = 0\n",
        "                sum_Pi = np.sum(Pi)\n",
        "                \n",
        "                if sum_Pi == 0:\n",
        "                    Pi = np.ones_like(Pi) / (n - 1)\n",
        "                    Pi[i] = 0\n",
        "                else:\n",
        "                    Pi = Pi / sum_Pi\n",
        "                \n",
        "                # Compute perplexity\n",
        "                H = -np.sum(Pi * np.log2(Pi + 1e-12))\n",
        "                perplexity = 2**H\n",
        "                \n",
        "                # Check convergence\n",
        "                perp_diff = perplexity - target_perplexity\n",
        "                if abs(perp_diff) < tol:\n",
        "                    break\n",
        "                \n",
        "                # Update beta\n",
        "                if perp_diff > 0:\n",
        "                    beta_min = beta\n",
        "                    if beta_max == np.inf:\n",
        "                        beta = beta * 2\n",
        "                    else:\n",
        "                        beta = (beta + beta_max) / 2\n",
        "                else:\n",
        "                    beta_max = beta\n",
        "                    if beta_min == -np.inf:\n",
        "                        beta = beta / 2\n",
        "                    else:\n",
        "                        beta = (beta + beta_min) / 2\n",
        "            \n",
        "            sigmas[i] = 1.0 / np.sqrt(2.0 * beta)\n",
        "        \n",
        "        return sigmas\n",
        "    \n",
        "    def _compute_joint_probabilities(self, X):\n",
        "        \"\"\"Compute joint probabilities in high-dimensional space\"\"\"\n",
        "        distances = self._pairwise_distances(X)\n",
        "        n = X.shape[0]\n",
        "        \n",
        "        # Binary search for sigmas\n",
        "        sigmas = self._binary_search_perplexity(distances, self.perplexity)\n",
        "        \n",
        "        # Compute conditional probabilities\n",
        "        P = self._compute_conditional_probabilities(distances, sigmas)\n",
        "        \n",
        "        # Symmetrize to get joint probabilities\n",
        "        P_joint = (P + P.T) / (2.0 * n)\n",
        "        P_joint = np.maximum(P_joint, 1e-12)  # Avoid numerical issues\n",
        "        \n",
        "        return P_joint\n",
        "    \n",
        "    def _compute_low_dim_probabilities(self, Y):\n",
        "        \"\"\"Compute probabilities in low-dimensional space using Student's t-distribution\"\"\"\n",
        "        distances = self._pairwise_distances(Y)\n",
        "        \n",
        "        # Student's t-distribution with 1 degree of freedom (Cauchy distribution)\n",
        "        Q = 1.0 / (1.0 + distances)\n",
        "        \n",
        "        # Set diagonal to zero\n",
        "        np.fill_diagonal(Q, 0)\n",
        "        \n",
        "        # Normalize\n",
        "        Q = Q / np.sum(Q)\n",
        "        Q = np.maximum(Q, 1e-12)  # Avoid numerical issues\n",
        "        \n",
        "        return Q\n",
        "    \n",
        "    def _compute_gradient(self, P, Q, Y):\n",
        "        \"\"\"Compute gradient of KL divergence\"\"\"\n",
        "        n = Y.shape[0]\n",
        "        \n",
        "        # Compute forces\n",
        "        PQ_diff = P - Q\n",
        "        distances = self._pairwise_distances(Y)\n",
        "        inv_distances = 1.0 / (1.0 + distances)\n",
        "        \n",
        "        # Gradient computation\n",
        "        gradient = np.zeros_like(Y)\n",
        "        for i in range(n):\n",
        "            diff = Y[i] - Y\n",
        "            gradient[i] = 4 * np.sum(\n",
        "                (PQ_diff[i] * inv_distances[i])[:, np.newaxis] * diff, axis=0\n",
        "            )\n",
        "        \n",
        "        return gradient\n",
        "    \n",
        "    def fit_transform(self, X):\n",
        "        \"\"\"Fit t-SNE and return embedded coordinates\"\"\"\n",
        "        n, d = X.shape\n",
        "        \n",
        "        # Compute joint probabilities in high-dimensional space\n",
        "        print(\"Computing high-dimensional probabilities...\")\n",
        "        P = self._compute_joint_probabilities(X)\n",
        "        \n",
        "        # Initialize low-dimensional embedding\n",
        "        Y = np.random.normal(0, 1e-4, (n, self.n_components))\n",
        "        \n",
        "        # Optimization loop\n",
        "        print(\"Optimizing embedding...\")\n",
        "        for iteration in range(self.n_iter):\n",
        "            # Compute low-dimensional probabilities\n",
        "            Q = self._compute_low_dim_probabilities(Y)\n",
        "            \n",
        "            # Compute cost (KL divergence)\n",
        "            cost = np.sum(P * np.log(P / Q))\n",
        "            self.costs.append(cost)\n",
        "            \n",
        "            # Compute gradient\n",
        "            gradient = self._compute_gradient(P, Q, Y)\n",
        "            \n",
        "            # Update Y using gradient descent\n",
        "            Y = Y - self.learning_rate * gradient\n",
        "            \n",
        "            # Center the embedding\n",
        "            Y = Y - np.mean(Y, axis=0)\n",
        "            \n",
        "            if iteration % 100 == 0:\n",
        "                print(f\"Iteration {iteration}, Cost: {cost:.4f}\")\n",
        "        \n",
        "        return Y\n",
        "\n",
        "# Test simple t-SNE implementation\n",
        "print(\"=== Testing Simple t-SNE Implementation ===\")\n",
        "\n",
        "# Use a small subset of Swiss Roll for demonstration\n",
        "X_test = datasets['swiss_roll'][0][:200]  # Use only 200 points for speed\n",
        "colors_test = datasets['swiss_roll'][1][:200]\n",
        "\n",
        "# Apply our simple t-SNE\n",
        "simple_tsne = SimpleTSNE(n_components=2, perplexity=15, learning_rate=100, n_iter=500)\n",
        "Y_simple = simple_tsne.fit_transform(X_test)\n",
        "\n",
        "# Compare with sklearn's t-SNE\n",
        "tsne_sklearn = TSNE(n_components=2, perplexity=15, random_state=42)\n",
        "Y_sklearn = tsne_sklearn.fit_transform(X_test)\n",
        "\n",
        "# Plot results\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Original data (first 2 dimensions)\n",
        "axes[0].scatter(X_test[:, 0], X_test[:, 1], c=colors_test, cmap='viridis')\n",
        "axes[0].set_title('Original Data (First 2 Dims)')\n",
        "\n",
        "# Our implementation\n",
        "axes[1].scatter(Y_simple[:, 0], Y_simple[:, 1], c=colors_test, cmap='viridis')\n",
        "axes[1].set_title('Our Simple t-SNE')\n",
        "\n",
        "# Sklearn implementation\n",
        "axes[2].scatter(Y_sklearn[:, 0], Y_sklearn[:, 1], c=colors_test, cmap='viridis')\n",
        "axes[2].set_title('Sklearn t-SNE')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot cost function\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(simple_tsne.costs)\n",
        "plt.title('t-SNE Cost Function (KL Divergence)')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cost')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"Simple t-SNE implementation completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Key Insights and Reflections\n",
        "\n",
        "### What I Learned About Manifold Learning:\n",
        "\n",
        "1. **Manifold Hypothesis**: High-dimensional data often has lower intrinsic dimensionality\n",
        "2. **Technique Selection**: Different methods work better for different types of manifolds\n",
        "3. **Non-linear Power**: t-SNE and UMAP excel at preserving local neighborhoods\n",
        "4. **Trustworthiness**: Important metric for evaluating embedding quality\n",
        "5. **t-SNE Mechanics**: Probability matching between high and low dimensions\n",
        "\n",
        "### Key Observations:\n",
        "- **PCA**: Good for linear relationships, poor for non-linear manifolds\n",
        "- **t-SNE**: Excellent for local structure, can distort global structure\n",
        "- **MDS**: Preserves global distances, may miss local structure\n",
        "- **Isomap**: Good for manifolds with geodesic structure\n",
        "- **LLE**: Effective for locally linear manifolds\n",
        "\n",
        "### Tomorrow's Preview:\n",
        "We'll dive deeper into t-SNE details including the crowding problem and early exaggeration!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
