{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Loss Functions & Regularization Deep Dive\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand different types of loss functions and their properties\n",
        "- Implement various loss functions from scratch\n",
        "- Explore L1 and L2 regularization effects\n",
        "- Understand dropout as a regularization technique\n",
        "- Analyze the mathematical properties of loss functions\n",
        "- Compare regularization techniques empirically\n",
        "\n",
        "## Theory Overview\n",
        "\n",
        "### Loss Functions\n",
        "Loss functions measure the difference between predicted and actual values, guiding the optimization process.\n",
        "\n",
        "### Common Loss Functions:\n",
        "1. **Mean Squared Error (MSE)**: For regression, sensitive to outliers\n",
        "2. **Mean Absolute Error (MAE)**: Robust to outliers, non-differentiable at zero\n",
        "3. **Huber Loss**: Combines MSE and MAE benefits\n",
        "4. **Cross-Entropy**: For classification, probabilistic interpretation\n",
        "5. **Hinge Loss**: For SVMs, margin-based\n",
        "\n",
        "### Regularization\n",
        "Prevents overfitting by adding penalty terms to the loss function.\n",
        "\n",
        "### Types:\n",
        "1. **L1 (Lasso)**: Promotes sparsity, feature selection\n",
        "2. **L2 (Ridge)**: Shrinks coefficients, handles collinearity\n",
        "3. **Elastic Net**: Combines L1 and L2\n",
        "4. **Dropout**: Randomly sets neurons to zero during training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_regression, make_classification, load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, log_loss\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style and random seed\n",
        "plt.style.use('seaborn-v0_8')\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Implementing Loss Functions from Scratch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LossFunctions:\n",
        "    \"\"\"Collection of loss functions implemented from scratch\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def mse_loss(y_true, y_pred):\n",
        "        \"\"\"Mean Squared Error\"\"\"\n",
        "        return np.mean((y_true - y_pred) ** 2)\n",
        "    \n",
        "    @staticmethod\n",
        "    def mae_loss(y_true, y_pred):\n",
        "        \"\"\"Mean Absolute Error\"\"\"\n",
        "        return np.mean(np.abs(y_true - y_pred))\n",
        "    \n",
        "    @staticmethod\n",
        "    def huber_loss(y_true, y_pred, delta=1.0):\n",
        "        \"\"\"Huber Loss - combines MSE and MAE\"\"\"\n",
        "        residual = np.abs(y_true - y_pred)\n",
        "        return np.mean(np.where(\n",
        "            residual <= delta,\n",
        "            0.5 * residual ** 2,\n",
        "            delta * (residual - 0.5 * delta)\n",
        "        ))\n",
        "    \n",
        "    @staticmethod\n",
        "    def cross_entropy_loss(y_true, y_pred):\n",
        "        \"\"\"Binary Cross-Entropy Loss\"\"\"\n",
        "        # Clip predictions to prevent log(0)\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "    \n",
        "    @staticmethod\n",
        "    def hinge_loss(y_true, y_pred):\n",
        "        \"\"\"Hinge Loss for SVMs\"\"\"\n",
        "        # Convert labels to {-1, 1}\n",
        "        y_true_signed = 2 * y_true - 1\n",
        "        return np.mean(np.maximum(0, 1 - y_true_signed * y_pred))\n",
        "    \n",
        "    @staticmethod\n",
        "    def log_cosh_loss(y_true, y_pred):\n",
        "        \"\"\"Logarithm of hyperbolic cosine\"\"\"\n",
        "        return np.mean(np.log(np.cosh(y_pred - y_true)))\n",
        "\n",
        "# Test loss functions with synthetic data\n",
        "def test_loss_functions():\n",
        "    \"\"\"Test and visualize different loss functions\"\"\"\n",
        "    \n",
        "    # Generate regression data with outliers\n",
        "    np.random.seed(42)\n",
        "    n_samples = 100\n",
        "    X = np.linspace(-3, 3, n_samples)\n",
        "    y_true = 2 * X + 1 + np.random.normal(0, 0.5, n_samples)\n",
        "    \n",
        "    # Add some outliers\n",
        "    outlier_indices = np.random.choice(n_samples, 5, replace=False)\n",
        "    y_true[outlier_indices] += np.random.normal(0, 5, 5)\n",
        "    \n",
        "    # Generate predictions (with some error)\n",
        "    y_pred = 2.1 * X + 0.9\n",
        "    \n",
        "    # Calculate losses\n",
        "    losses = {\n",
        "        'MSE': LossFunctions.mse_loss(y_true, y_pred),\n",
        "        'MAE': LossFunctions.mae_loss(y_true, y_pred),\n",
        "        'Huber (δ=1)': LossFunctions.huber_loss(y_true, y_pred, delta=1.0),\n",
        "        'Huber (δ=2)': LossFunctions.huber_loss(y_true, y_pred, delta=2.0),\n",
        "        'Log-Cosh': LossFunctions.log_cosh_loss(y_true, y_pred)\n",
        "    }\n",
        "    \n",
        "    # Visualize data and predictions\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    # Plot 1: Data and predictions\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.scatter(X, y_true, alpha=0.6, label='True values')\n",
        "    plt.plot(X, y_pred, 'r-', label='Predictions')\n",
        "    plt.scatter(X[outlier_indices], y_true[outlier_indices], \n",
        "                color='red', s=100, alpha=0.8, label='Outliers')\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('y')\n",
        "    plt.title('Data with Outliers')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Loss comparison\n",
        "    plt.subplot(2, 3, 2)\n",
        "    loss_names = list(losses.keys())\n",
        "    loss_values = list(losses.values())\n",
        "    bars = plt.bar(loss_names, loss_values, color='skyblue', alpha=0.7)\n",
        "    plt.title('Loss Function Comparison')\n",
        "    plt.ylabel('Loss Value')\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars, loss_values):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
        "                f'{value:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    # Plot 3-6: Individual loss landscapes\n",
        "    residuals = np.linspace(-5, 5, 100)\n",
        "    loss_landscapes = {\n",
        "        'MSE': residuals ** 2,\n",
        "        'MAE': np.abs(residuals),\n",
        "        'Huber (δ=1)': np.where(np.abs(residuals) <= 1, \n",
        "                               0.5 * residuals ** 2,\n",
        "                               np.abs(residuals) - 0.5),\n",
        "        'Log-Cosh': np.log(np.cosh(residuals))\n",
        "    }\n",
        "    \n",
        "    for i, (name, landscape) in enumerate(loss_landscapes.items(), 3):\n",
        "        plt.subplot(2, 3, i)\n",
        "        plt.plot(residuals, landscape, linewidth=2)\n",
        "        plt.xlabel('Residual (y_true - y_pred)')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title(f'{name} Loss Landscape')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return losses\n",
        "\n",
        "# Run tests\n",
        "print(\"=== Testing Loss Functions ===\")\n",
        "loss_results = test_loss_functions()\n",
        "\n",
        "print(\"\\\\nLoss Values:\")\n",
        "for name, value in loss_results.items():\n",
        "    print(f\"{name}: {value:.4f}\")\n",
        "\n",
        "print(\"\\\\nKey Observations:\")\n",
        "print(\"- MSE is sensitive to outliers (squared error)\")\n",
        "print(\"- MAE is more robust to outliers\")  \n",
        "print(\"- Huber loss combines benefits of both\")\n",
        "print(\"- Log-Cosh is smooth and robust\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Regularization Techniques Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RegularizedLinearRegression:\n",
        "    \"\"\"Linear regression with different regularization techniques\"\"\"\n",
        "    \n",
        "    def __init__(self, regularization='none', alpha=0.01, l1_ratio=0.5):\n",
        "        self.regularization = regularization\n",
        "        self.alpha = alpha  # Regularization strength\n",
        "        self.l1_ratio = l1_ratio  # For elastic net\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.cost_history = []\n",
        "    \n",
        "    def _add_regularization(self, loss, weights):\n",
        "        \"\"\"Add regularization term to loss\"\"\"\n",
        "        if self.regularization == 'l1':\n",
        "            return loss + self.alpha * np.sum(np.abs(weights))\n",
        "        elif self.regularization == 'l2':\n",
        "            return loss + self.alpha * np.sum(weights ** 2)\n",
        "        elif self.regularization == 'elastic_net':\n",
        "            l1_term = self.alpha * self.l1_ratio * np.sum(np.abs(weights))\n",
        "            l2_term = self.alpha * (1 - self.l1_ratio) * np.sum(weights ** 2)\n",
        "            return loss + l1_term + l2_term\n",
        "        else:\n",
        "            return loss\n",
        "    \n",
        "    def _compute_gradients(self, X, y, predictions):\n",
        "        \"\"\"Compute gradients with regularization\"\"\"\n",
        "        m = X.shape[0]\n",
        "        \n",
        "        # Base gradients\n",
        "        dw = (2/m) * np.dot(X.T, (predictions - y))\n",
        "        db = (2/m) * np.sum(predictions - y)\n",
        "        \n",
        "        # Add regularization to weight gradients\n",
        "        if self.regularization == 'l1':\n",
        "            dw += self.alpha * np.sign(self.weights)\n",
        "        elif self.regularization == 'l2':\n",
        "            dw += 2 * self.alpha * self.weights\n",
        "        elif self.regularization == 'elastic_net':\n",
        "            dw += self.alpha * (self.l1_ratio * np.sign(self.weights) + \n",
        "                               2 * (1 - self.l1_ratio) * self.weights)\n",
        "        \n",
        "        return dw, db\n",
        "    \n",
        "    def fit(self, X, y, learning_rate=0.01, n_iterations=1000):\n",
        "        \"\"\"Fit the model using gradient descent\"\"\"\n",
        "        m, n = X.shape\n",
        "        \n",
        "        # Initialize parameters\n",
        "        self.weights = np.random.normal(0, 0.01, n)\n",
        "        self.bias = 0\n",
        "        self.cost_history = []\n",
        "        \n",
        "        for i in range(n_iterations):\n",
        "            # Forward pass\n",
        "            predictions = X.dot(self.weights) + self.bias\n",
        "            \n",
        "            # Compute loss\n",
        "            base_loss = np.mean((predictions - y) ** 2)\n",
        "            total_loss = self._add_regularization(base_loss, self.weights)\n",
        "            self.cost_history.append(total_loss)\n",
        "            \n",
        "            # Compute gradients\n",
        "            dw, db = self._compute_gradients(X, y, predictions)\n",
        "            \n",
        "            # Update parameters\n",
        "            self.weights -= learning_rate * dw\n",
        "            self.bias -= learning_rate * db\n",
        "            \n",
        "            # Apply soft thresholding for L1 regularization\n",
        "            if self.regularization == 'l1':\n",
        "                self.weights = np.sign(self.weights) * np.maximum(\n",
        "                    np.abs(self.weights) - learning_rate * self.alpha, 0\n",
        "                )\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        return X.dot(self.weights) + self.bias\n",
        "\n",
        "# Generate dataset for regularization comparison\n",
        "def create_regularization_dataset():\n",
        "    \"\"\"Create dataset with multicollinearity for regularization demo\"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Create base features\n",
        "    n_samples = 100\n",
        "    X_base = np.random.randn(n_samples, 3)\n",
        "    \n",
        "    # Add highly correlated features to create multicollinearity\n",
        "    X_corr1 = X_base[:, 0:1] + 0.1 * np.random.randn(n_samples, 1)\n",
        "    X_corr2 = X_base[:, 1:2] + 0.1 * np.random.randn(n_samples, 1)\n",
        "    X_noise = np.random.randn(n_samples, 5)  # Irrelevant features\n",
        "    \n",
        "    # Combine all features\n",
        "    X = np.hstack([X_base, X_corr1, X_corr2, X_noise])\n",
        "    \n",
        "    # True relationship (only first 3 features are relevant)\n",
        "    true_weights = np.array([2, -1.5, 1, 0, 0, 0, 0, 0])\n",
        "    y = X.dot(true_weights) + 0.5 * np.random.randn(n_samples)\n",
        "    \n",
        "    return X, y, true_weights\n",
        "\n",
        "# Compare regularization techniques\n",
        "def compare_regularization():\n",
        "    \"\"\"Compare different regularization techniques\"\"\"\n",
        "    \n",
        "    # Create dataset\n",
        "    X, y, true_weights = create_regularization_dataset()\n",
        "    \n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    \n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Define models\n",
        "    models = {\n",
        "        'No Regularization': RegularizedLinearRegression('none'),\n",
        "        'L1 (α=0.1)': RegularizedLinearRegression('l1', alpha=0.1),\n",
        "        'L2 (α=0.1)': RegularizedLinearRegression('l2', alpha=0.1),\n",
        "        'Elastic Net': RegularizedLinearRegression('elastic_net', alpha=0.1, l1_ratio=0.5)\n",
        "    }\n",
        "    \n",
        "    # Train models and collect results\n",
        "    results = {}\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    \n",
        "    for i, (name, model) in enumerate(models.items()):\n",
        "        # Train model\n",
        "        model.fit(X_train_scaled, y_train, learning_rate=0.1, n_iterations=1000)\n",
        "        \n",
        "        # Make predictions\n",
        "        train_pred = model.predict(X_train_scaled)\n",
        "        test_pred = model.predict(X_test_scaled)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        train_mse = mean_squared_error(y_train, train_pred)\n",
        "        test_mse = mean_squared_error(y_test, test_pred)\n",
        "        \n",
        "        results[name] = {\n",
        "            'weights': model.weights,\n",
        "            'train_mse': train_mse,\n",
        "            'test_mse': test_mse,\n",
        "            'cost_history': model.cost_history\n",
        "        }\n",
        "        \n",
        "        # Plot cost history\n",
        "        ax = axes[0, i] if i < 2 else axes[1, i-2]\n",
        "        ax.plot(model.cost_history)\n",
        "        ax.set_title(f'{name}\\\\nTrain MSE: {train_mse:.3f}, Test MSE: {test_mse:.3f}')\n",
        "        ax.set_xlabel('Iteration')\n",
        "        ax.set_ylabel('Cost')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot weight comparison\n",
        "    ax = axes[0, 2]\n",
        "    feature_names = [f'Feature {i}' for i in range(len(true_weights))]\n",
        "    x_pos = np.arange(len(feature_names))\n",
        "    \n",
        "    width = 0.15\n",
        "    for i, (name, result) in enumerate(results.items()):\n",
        "        ax.bar(x_pos + i * width, result['weights'], width, \n",
        "               label=name, alpha=0.7)\n",
        "    \n",
        "    ax.bar(x_pos + len(results) * width, true_weights, width, \n",
        "           label='True Weights', alpha=0.7, color='black')\n",
        "    \n",
        "    ax.set_xlabel('Features')\n",
        "    ax.set_ylabel('Weight Value')\n",
        "    ax.set_title('Weight Comparison')\n",
        "    ax.set_xticks(x_pos + width * len(results) / 2)\n",
        "    ax.set_xticklabels(feature_names, rotation=45)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot sparsity comparison\n",
        "    ax = axes[1, 2]\n",
        "    sparsity_data = []\n",
        "    model_names = []\n",
        "    \n",
        "    for name, result in results.items():\n",
        "        sparsity = np.sum(np.abs(result['weights']) < 0.01) / len(result['weights'])\n",
        "        sparsity_data.append(sparsity)\n",
        "        model_names.append(name)\n",
        "    \n",
        "    bars = ax.bar(model_names, sparsity_data, alpha=0.7, color='lightcoral')\n",
        "    ax.set_ylabel('Sparsity (fraction of near-zero weights)')\n",
        "    ax.set_title('Model Sparsity Comparison')\n",
        "    ax.set_xticklabels(model_names, rotation=45)\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, sparsity_data):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{value:.2f}', ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run regularization comparison\n",
        "print(\"=== Regularization Comparison ===\")\n",
        "reg_results = compare_regularization()\n",
        "\n",
        "# Print summary\n",
        "print(\"\\\\n=== Results Summary ===\")\n",
        "for name, result in reg_results.items():\n",
        "    print(f\"\\\\n{name}:\")\n",
        "    print(f\"  Train MSE: {result['train_mse']:.4f}\")\n",
        "    print(f\"  Test MSE: {result['test_mse']:.4f}\")\n",
        "    print(f\"  Non-zero weights: {np.sum(np.abs(result['weights']) > 0.01)}/8\")\n",
        "    print(f\"  Weight magnitudes: {np.linalg.norm(result['weights']):.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Dropout as Regularization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleNeuralNetworkWithDropout:\n",
        "    \"\"\"Simple neural network with dropout regularization\"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.5):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "        \n",
        "        # Initialize weights\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.1\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.1\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "        \n",
        "        self.train_history = []\n",
        "        self.val_history = []\n",
        "    \n",
        "    def _sigmoid(self, x):\n",
        "        \"\"\"Sigmoid activation function\"\"\"\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n",
        "    \n",
        "    def _sigmoid_derivative(self, x):\n",
        "        \"\"\"Derivative of sigmoid\"\"\"\n",
        "        s = self._sigmoid(x)\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    def _apply_dropout(self, x, training=True):\n",
        "        \"\"\"Apply dropout during training\"\"\"\n",
        "        if training and self.dropout_rate > 0:\n",
        "            # Create dropout mask\n",
        "            mask = np.random.binomial(1, 1 - self.dropout_rate, x.shape) / (1 - self.dropout_rate)\n",
        "            return x * mask, mask\n",
        "        else:\n",
        "            return x, np.ones_like(x)\n",
        "    \n",
        "    def forward(self, X, training=True):\n",
        "        \"\"\"Forward propagation with optional dropout\"\"\"\n",
        "        # Hidden layer\n",
        "        z1 = np.dot(X, self.W1) + self.b1\n",
        "        a1 = self._sigmoid(z1)\n",
        "        \n",
        "        # Apply dropout to hidden layer\n",
        "        a1_dropout, dropout_mask = self._apply_dropout(a1, training)\n",
        "        \n",
        "        # Output layer\n",
        "        z2 = np.dot(a1_dropout, self.W2) + self.b2\n",
        "        a2 = self._sigmoid(z2)\n",
        "        \n",
        "        return a2, a1, a1_dropout, z1, z2, dropout_mask\n",
        "    \n",
        "    def backward(self, X, y, a2, a1, a1_dropout, z1, z2, dropout_mask):\n",
        "        \"\"\"Backward propagation\"\"\"\n",
        "        m = X.shape[0]\n",
        "        \n",
        "        # Output layer gradients\n",
        "        dz2 = a2 - y\n",
        "        dW2 = (1/m) * np.dot(a1_dropout.T, dz2)\n",
        "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
        "        \n",
        "        # Hidden layer gradients\n",
        "        da1 = np.dot(dz2, self.W2.T)\n",
        "        # Apply dropout mask to gradients\n",
        "        da1 = da1 * dropout_mask\n",
        "        dz1 = da1 * self._sigmoid_derivative(z1)\n",
        "        dW1 = (1/m) * np.dot(X.T, dz1)\n",
        "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
        "        \n",
        "        return dW1, db1, dW2, db2\n",
        "    \n",
        "    def fit(self, X_train, y_train, X_val, y_val, epochs=1000, learning_rate=0.1):\n",
        "        \"\"\"Train the network\"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass with dropout\n",
        "            a2, a1, a1_dropout, z1, z2, dropout_mask = self.forward(X_train, training=True)\n",
        "            \n",
        "            # Compute training loss\n",
        "            train_loss = -np.mean(y_train * np.log(a2 + 1e-15) + \n",
        "                                 (1 - y_train) * np.log(1 - a2 + 1e-15))\n",
        "            \n",
        "            # Backward pass\n",
        "            dW1, db1, dW2, db2 = self.backward(X_train, y_train, a2, a1, a1_dropout, z1, z2, dropout_mask)\n",
        "            \n",
        "            # Update parameters\n",
        "            self.W1 -= learning_rate * dW1\n",
        "            self.b1 -= learning_rate * db1\n",
        "            self.W2 -= learning_rate * dW2\n",
        "            self.b2 -= learning_rate * db2\n",
        "            \n",
        "            # Validation loss (without dropout)\n",
        "            val_pred, _, _, _, _, _ = self.forward(X_val, training=False)\n",
        "            val_loss = -np.mean(y_val * np.log(val_pred + 1e-15) + \n",
        "                               (1 - y_val) * np.log(1 - val_pred + 1e-15))\n",
        "            \n",
        "            # Store history\n",
        "            self.train_history.append(train_loss)\n",
        "            self.val_history.append(val_loss)\n",
        "            \n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions (without dropout)\"\"\"\n",
        "        pred, _, _, _, _, _ = self.forward(X, training=False)\n",
        "        return pred\n",
        "\n",
        "# Compare networks with and without dropout\n",
        "def compare_dropout_effects():\n",
        "    \"\"\"Compare neural networks with and without dropout\"\"\"\n",
        "    \n",
        "    # Generate classification dataset\n",
        "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, \n",
        "                              n_redundant=10, n_clusters_per_class=1, random_state=42)\n",
        "    \n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Reshape targets\n",
        "    y_train = y_train.reshape(-1, 1)\n",
        "    y_val = y_val.reshape(-1, 1)\n",
        "    y_test = y_test.reshape(-1, 1)\n",
        "    \n",
        "    # Create networks\n",
        "    network_no_dropout = SimpleNeuralNetworkWithDropout(20, 50, 1, dropout_rate=0.0)\n",
        "    network_with_dropout = SimpleNeuralNetworkWithDropout(20, 50, 1, dropout_rate=0.3)\n",
        "    \n",
        "    print(\"Training network without dropout...\")\n",
        "    network_no_dropout.fit(X_train_scaled, y_train, X_val_scaled, y_val, epochs=500, learning_rate=0.1)\n",
        "    \n",
        "    print(\"\\\\nTraining network with dropout...\")\n",
        "    network_with_dropout.fit(X_train_scaled, y_train, X_val_scaled, y_val, epochs=500, learning_rate=0.1)\n",
        "    \n",
        "    # Make predictions\n",
        "    pred_no_dropout = network_no_dropout.predict(X_test_scaled)\n",
        "    pred_with_dropout = network_with_dropout.predict(X_test_scaled)\n",
        "    \n",
        "    # Calculate accuracies\n",
        "    acc_no_dropout = accuracy_score(y_test, (pred_no_dropout > 0.5).astype(int))\n",
        "    acc_with_dropout = accuracy_score(y_test, (pred_with_dropout > 0.5).astype(int))\n",
        "    \n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    \n",
        "    # Training curves\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(network_no_dropout.train_history, label='No Dropout - Train', alpha=0.7)\n",
        "    plt.plot(network_no_dropout.val_history, label='No Dropout - Val', alpha=0.7)\n",
        "    plt.plot(network_with_dropout.train_history, label='With Dropout - Train', alpha=0.7)\n",
        "    plt.plot(network_with_dropout.val_history, label='With Dropout - Val', alpha=0.7)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Curves Comparison')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Overfitting analysis\n",
        "    plt.subplot(1, 3, 2)\n",
        "    overfitting_no_dropout = np.array(network_no_dropout.val_history) - np.array(network_no_dropout.train_history)\n",
        "    overfitting_with_dropout = np.array(network_with_dropout.val_history) - np.array(network_with_dropout.train_history)\n",
        "    \n",
        "    plt.plot(overfitting_no_dropout, label='No Dropout', alpha=0.7)\n",
        "    plt.plot(overfitting_with_dropout, label='With Dropout', alpha=0.7)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Validation - Training Loss')\n",
        "    plt.title('Overfitting Analysis')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Accuracy comparison\n",
        "    plt.subplot(1, 3, 3)\n",
        "    accuracies = [acc_no_dropout, acc_with_dropout]\n",
        "    labels = ['No Dropout', 'With Dropout']\n",
        "    bars = plt.bar(labels, accuracies, alpha=0.7, color=['coral', 'skyblue'])\n",
        "    plt.ylabel('Test Accuracy')\n",
        "    plt.title('Test Accuracy Comparison')\n",
        "    plt.ylim(0, 1)\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, acc in zip(bars, accuracies):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{acc:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return {\n",
        "        'no_dropout': {\n",
        "            'accuracy': acc_no_dropout,\n",
        "            'final_train_loss': network_no_dropout.train_history[-1],\n",
        "            'final_val_loss': network_no_dropout.val_history[-1]\n",
        "        },\n",
        "        'with_dropout': {\n",
        "            'accuracy': acc_with_dropout,\n",
        "            'final_train_loss': network_with_dropout.train_history[-1],\n",
        "            'final_val_loss': network_with_dropout.val_history[-1]\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Run dropout comparison\n",
        "print(\"=== Dropout Regularization Analysis ===\")\n",
        "dropout_results = compare_dropout_effects()\n",
        "\n",
        "print(\"\\\\n=== Dropout Results Summary ===\")\n",
        "for name, result in dropout_results.items():\n",
        "    print(f\"\\\\n{name.replace('_', ' ').title()}:\")\n",
        "    print(f\"  Test Accuracy: {result['accuracy']:.4f}\")\n",
        "    print(f\"  Final Train Loss: {result['final_train_loss']:.4f}\")\n",
        "    print(f\"  Final Val Loss: {result['final_val_loss']:.4f}\")\n",
        "    print(f\"  Overfitting: {result['final_val_loss'] - result['final_train_loss']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Key Insights and Learning Reflections\n",
        "\n",
        "### Loss Functions Understanding:\n",
        "\n",
        "1. **MSE vs MAE**: MSE penalizes large errors more heavily, MAE is more robust to outliers\n",
        "2. **Huber Loss**: Best of both worlds - quadratic for small errors, linear for large ones\n",
        "3. **Cross-Entropy**: Natural choice for classification, has probabilistic interpretation\n",
        "4. **Loss Landscapes**: Shape affects optimization difficulty and convergence\n",
        "\n",
        "### Regularization Insights:\n",
        "\n",
        "1. **L1 Regularization**: \n",
        "   - Promotes sparsity (feature selection)\n",
        "   - Can make some weights exactly zero\n",
        "   - Good for feature selection in high-dimensional data\n",
        "\n",
        "2. **L2 Regularization**:\n",
        "   - Shrinks weights towards zero but doesn't make them exactly zero\n",
        "   - Helps with multicollinearity\n",
        "   - Generally provides smoother solutions\n",
        "\n",
        "3. **Dropout**:\n",
        "   - Effective regularization for neural networks\n",
        "   - Prevents co-adaptation of neurons\n",
        "   - Acts like ensemble method during training\n",
        "\n",
        "### Practical Takeaways:\n",
        "- Choose loss function based on problem characteristics and robustness needs\n",
        "- Use L1 for feature selection, L2 for general regularization\n",
        "- Dropout is essential for deep networks to prevent overfitting\n",
        "- Regularization strength (α) needs careful tuning\n",
        "\n",
        "### Tomorrow's Focus:\n",
        "We'll explore advanced training strategies, batch processing, and learning rate scheduling!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
