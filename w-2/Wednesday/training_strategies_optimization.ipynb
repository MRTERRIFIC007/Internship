{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Advanced Training Strategies & Optimization\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand batch processing and its effects on training\n",
        "- Implement different batch sizes and analyze their impact\n",
        "- Explore learning rate scheduling strategies\n",
        "- Implement momentum and adaptive learning rate methods\n",
        "- Understand early stopping and model checkpointing\n",
        "- Compare different optimization algorithms\n",
        "\n",
        "## Theory Overview\n",
        "\n",
        "### Batch Processing\n",
        "- **Batch Gradient Descent**: Uses entire dataset per update\n",
        "- **Stochastic Gradient Descent (SGD)**: Uses one sample per update\n",
        "- **Mini-batch Gradient Descent**: Uses small subsets of data\n",
        "\n",
        "### Learning Rate Scheduling\n",
        "- **Step Decay**: Reduce LR by factor at specific epochs\n",
        "- **Exponential Decay**: Exponentially decrease LR over time\n",
        "- **Cosine Annealing**: LR follows cosine curve\n",
        "- **Learning Rate Warmup**: Gradually increase LR initially\n",
        "\n",
        "### Optimization Algorithms\n",
        "- **SGD**: Basic gradient descent\n",
        "- **Momentum**: Accelerates gradients in right directions\n",
        "- **Adam**: Adaptive learning rates per parameter\n",
        "- **RMSprop**: Adaptive learning rate method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style and random seed\n",
        "plt.style.use('seaborn-v0_8')\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Batch Processing Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BatchGradientDescent:\n",
        "    \"\"\"Neural Network with configurable batch processing\"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, batch_size=32):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        # Initialize weights with Xavier initialization\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "        \n",
        "        # Training history\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.learning_rates = []\n",
        "        \n",
        "    def _relu(self, x):\n",
        "        \"\"\"ReLU activation function\"\"\"\n",
        "        return np.maximum(0, x)\n",
        "    \n",
        "    def _relu_derivative(self, x):\n",
        "        \"\"\"Derivative of ReLU\"\"\"\n",
        "        return (x > 0).astype(float)\n",
        "    \n",
        "    def _sigmoid(self, x):\n",
        "        \"\"\"Sigmoid activation function\"\"\"\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward propagation\"\"\"\n",
        "        z1 = np.dot(X, self.W1) + self.b1\n",
        "        a1 = self._relu(z1)\n",
        "        z2 = np.dot(a1, self.W2) + self.b2\n",
        "        a2 = self._sigmoid(z2)\n",
        "        return a2, a1, z1, z2\n",
        "    \n",
        "    def backward(self, X, y, a2, a1, z1):\n",
        "        \"\"\"Backward propagation\"\"\"\n",
        "        m = X.shape[0]\n",
        "        \n",
        "        # Output layer\n",
        "        dz2 = a2 - y\n",
        "        dW2 = (1/m) * np.dot(a1.T, dz2)\n",
        "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
        "        \n",
        "        # Hidden layer\n",
        "        da1 = np.dot(dz2, self.W2.T)\n",
        "        dz1 = da1 * self._relu_derivative(z1)\n",
        "        dW1 = (1/m) * np.dot(X.T, dz1)\n",
        "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
        "        \n",
        "        return dW1, db1, dW2, db2\n",
        "    \n",
        "    def create_batches(self, X, y):\n",
        "        \"\"\"Create mini-batches from data\"\"\"\n",
        "        m = X.shape[0]\n",
        "        if self.batch_size >= m:\n",
        "            # Full batch\n",
        "            return [(X, y)]\n",
        "        \n",
        "        # Shuffle data\n",
        "        indices = np.random.permutation(m)\n",
        "        X_shuffled = X[indices]\n",
        "        y_shuffled = y[indices]\n",
        "        \n",
        "        # Create batches\n",
        "        batches = []\n",
        "        for i in range(0, m, self.batch_size):\n",
        "            end_idx = min(i + self.batch_size, m)\n",
        "            batch_X = X_shuffled[i:end_idx]\n",
        "            batch_y = y_shuffled[i:end_idx]\n",
        "            batches.append((batch_X, batch_y))\n",
        "        \n",
        "        return batches\n",
        "    \n",
        "    def compute_loss(self, X, y):\n",
        "        \"\"\"Compute binary cross-entropy loss\"\"\"\n",
        "        a2, _, _, _ = self.forward(X)\n",
        "        loss = -np.mean(y * np.log(a2 + 1e-15) + (1 - y) * np.log(1 - a2 + 1e-15))\n",
        "        return loss\n",
        "    \n",
        "    def fit(self, X_train, y_train, X_val, y_val, epochs=100, learning_rate=0.01, \n",
        "            lr_scheduler=None, early_stopping_patience=None):\n",
        "        \"\"\"Train the network with various strategies\"\"\"\n",
        "        \n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Adjust learning rate if scheduler provided\n",
        "            if lr_scheduler:\n",
        "                current_lr = lr_scheduler(epoch, learning_rate)\n",
        "            else:\n",
        "                current_lr = learning_rate\n",
        "            \n",
        "            self.learning_rates.append(current_lr)\n",
        "            \n",
        "            # Create batches\n",
        "            batches = self.create_batches(X_train, y_train)\n",
        "            \n",
        "            # Train on batches\n",
        "            epoch_losses = []\n",
        "            for batch_X, batch_y in batches:\n",
        "                # Forward pass\n",
        "                a2, a1, z1, z2 = self.forward(batch_X)\n",
        "                \n",
        "                # Compute loss\n",
        "                batch_loss = self.compute_loss(batch_X, batch_y)\n",
        "                epoch_losses.append(batch_loss)\n",
        "                \n",
        "                # Backward pass\n",
        "                dW1, db1, dW2, db2 = self.backward(batch_X, batch_y, a2, a1, z1)\n",
        "                \n",
        "                # Update parameters\n",
        "                self.W1 -= current_lr * dW1\n",
        "                self.b1 -= current_lr * db1\n",
        "                self.W2 -= current_lr * dW2\n",
        "                self.b2 -= current_lr * db2\n",
        "            \n",
        "            # Compute epoch losses\n",
        "            train_loss = np.mean(epoch_losses)\n",
        "            val_loss = self.compute_loss(X_val, y_val)\n",
        "            \n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "            \n",
        "            # Early stopping\n",
        "            if early_stopping_patience:\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience_counter >= early_stopping_patience:\n",
        "                        print(f\"Early stopping at epoch {epoch}\")\n",
        "                        break\n",
        "            \n",
        "            if epoch % 20 == 0:\n",
        "                print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {current_lr:.6f}\")\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        a2, _, _, _ = self.forward(X)\n",
        "        return a2\n",
        "\n",
        "# Compare different batch sizes\n",
        "def compare_batch_sizes():\n",
        "    \"\"\"Compare training with different batch sizes\"\"\"\n",
        "    \n",
        "    # Generate dataset\n",
        "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n",
        "                              n_redundant=5, n_clusters_per_class=1, random_state=42)\n",
        "    \n",
        "    # Split and scale data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Reshape targets\n",
        "    y_train = y_train.reshape(-1, 1)\n",
        "    y_val = y_val.reshape(-1, 1)\n",
        "    y_test = y_test.reshape(-1, 1)\n",
        "    \n",
        "    # Test different batch sizes\n",
        "    batch_sizes = [1, 8, 32, 128, len(X_train)]  # SGD, small, medium, large, full batch\n",
        "    batch_names = ['SGD (1)', 'Small (8)', 'Medium (32)', 'Large (128)', 'Full Batch']\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    plt.figure(figsize=(20, 12))\n",
        "    \n",
        "    for i, (batch_size, name) in enumerate(zip(batch_sizes, batch_names)):\n",
        "        print(f\"\\\\nTraining with {name}...\")\n",
        "        \n",
        "        # Create and train model\n",
        "        model = BatchGradientDescent(20, 50, 1, batch_size=batch_size)\n",
        "        model.fit(X_train_scaled, y_train, X_val_scaled, y_val, \n",
        "                 epochs=100, learning_rate=0.01)\n",
        "        \n",
        "        # Make predictions\n",
        "        test_pred = model.predict(X_test_scaled)\n",
        "        accuracy = accuracy_score(y_test, (test_pred > 0.5).astype(int))\n",
        "        \n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'accuracy': accuracy,\n",
        "            'final_train_loss': model.train_losses[-1],\n",
        "            'final_val_loss': model.val_losses[-1]\n",
        "        }\n",
        "        \n",
        "        # Plot training curves\n",
        "        plt.subplot(2, 3, i+1)\n",
        "        plt.plot(model.train_losses, label='Train Loss', alpha=0.8)\n",
        "        plt.plot(model.val_losses, label='Val Loss', alpha=0.8)\n",
        "        plt.title(f'{name}\\\\nAccuracy: {accuracy:.3f}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Summary comparison\n",
        "    plt.subplot(2, 3, 6)\n",
        "    names = list(results.keys())\n",
        "    accuracies = [results[name]['accuracy'] for name in names]\n",
        "    train_losses = [results[name]['final_train_loss'] for name in names]\n",
        "    val_losses = [results[name]['final_val_loss'] for name in names]\n",
        "    \n",
        "    x_pos = np.arange(len(names))\n",
        "    width = 0.25\n",
        "    \n",
        "    plt.bar(x_pos - width, accuracies, width, label='Test Accuracy', alpha=0.7)\n",
        "    plt.bar(x_pos, train_losses, width, label='Final Train Loss', alpha=0.7)\n",
        "    plt.bar(x_pos + width, val_losses, width, label='Final Val Loss', alpha=0.7)\n",
        "    \n",
        "    plt.xlabel('Batch Size')\n",
        "    plt.ylabel('Metric Value')\n",
        "    plt.title('Batch Size Comparison')\n",
        "    plt.xticks(x_pos, names, rotation=45)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run batch size comparison\n",
        "print(\"=== Batch Size Analysis ===\")\n",
        "batch_results = compare_batch_sizes()\n",
        "\n",
        "print(\"\\\\n=== Batch Size Results Summary ===\")\n",
        "for name, result in batch_results.items():\n",
        "    print(f\"\\\\n{name}:\")\n",
        "    print(f\"  Test Accuracy: {result['accuracy']:.4f}\")\n",
        "    print(f\"  Final Train Loss: {result['final_train_loss']:.4f}\")\n",
        "    print(f\"  Final Val Loss: {result['final_val_loss']:.4f}\")\n",
        "    print(f\"  Overfitting: {result['final_val_loss'] - result['final_train_loss']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Learning Rate Scheduling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LearningRateSchedulers:\n",
        "    \"\"\"Collection of learning rate scheduling strategies\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def constant_lr(epoch, initial_lr):\n",
        "        \"\"\"Constant learning rate\"\"\"\n",
        "        return initial_lr\n",
        "    \n",
        "    @staticmethod\n",
        "    def step_decay(epoch, initial_lr, drop_rate=0.5, epochs_drop=20):\n",
        "        \"\"\"Step decay - reduce LR by factor every N epochs\"\"\"\n",
        "        return initial_lr * (drop_rate ** (epoch // epochs_drop))\n",
        "    \n",
        "    @staticmethod\n",
        "    def exponential_decay(epoch, initial_lr, decay_rate=0.95):\n",
        "        \"\"\"Exponential decay\"\"\"\n",
        "        return initial_lr * (decay_rate ** epoch)\n",
        "    \n",
        "    @staticmethod\n",
        "    def cosine_annealing(epoch, initial_lr, max_epochs=100):\n",
        "        \"\"\"Cosine annealing\"\"\"\n",
        "        return initial_lr * (1 + np.cos(np.pi * epoch / max_epochs)) / 2\n",
        "    \n",
        "    @staticmethod\n",
        "    def linear_warmup_cosine_decay(epoch, initial_lr, warmup_epochs=10, max_epochs=100):\n",
        "        \"\"\"Linear warmup followed by cosine decay\"\"\"\n",
        "        if epoch < warmup_epochs:\n",
        "            # Linear warmup\n",
        "            return initial_lr * (epoch + 1) / warmup_epochs\n",
        "        else:\n",
        "            # Cosine decay\n",
        "            progress = (epoch - warmup_epochs) / (max_epochs - warmup_epochs)\n",
        "            return initial_lr * (1 + np.cos(np.pi * progress)) / 2\n",
        "\n",
        "# Visualize learning rate schedules\n",
        "def visualize_lr_schedules():\n",
        "    \"\"\"Visualize different learning rate schedules\"\"\"\n",
        "    \n",
        "    epochs = np.arange(100)\n",
        "    initial_lr = 0.1\n",
        "    \n",
        "    schedules = {\n",
        "        'Constant': [LearningRateSchedulers.constant_lr(e, initial_lr) for e in epochs],\n",
        "        'Step Decay': [LearningRateSchedulers.step_decay(e, initial_lr) for e in epochs],\n",
        "        'Exponential': [LearningRateSchedulers.exponential_decay(e, initial_lr, 0.98) for e in epochs],\n",
        "        'Cosine Annealing': [LearningRateSchedulers.cosine_annealing(e, initial_lr) for e in epochs],\n",
        "        'Warmup + Cosine': [LearningRateSchedulers.linear_warmup_cosine_decay(e, initial_lr) for e in epochs]\n",
        "    }\n",
        "    \n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    # Plot all schedules\n",
        "    plt.subplot(2, 3, 1)\n",
        "    for name, schedule in schedules.items():\n",
        "        plt.plot(epochs, schedule, label=name, linewidth=2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.title('Learning Rate Schedules Comparison')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot individual schedules\n",
        "    for i, (name, schedule) in enumerate(schedules.items(), 2):\n",
        "        if i <= 6:\n",
        "            plt.subplot(2, 3, i)\n",
        "            plt.plot(epochs, schedule, linewidth=2, color=f'C{i-2}')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Learning Rate')\n",
        "            plt.title(f'{name} Schedule')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return schedules\n",
        "\n",
        "# Compare training with different LR schedules\n",
        "def compare_lr_schedules():\n",
        "    \"\"\"Compare training with different learning rate schedules\"\"\"\n",
        "    \n",
        "    # Use same dataset as before\n",
        "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n",
        "                              n_redundant=5, n_clusters_per_class=1, random_state=42)\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    y_train = y_train.reshape(-1, 1)\n",
        "    y_val = y_val.reshape(-1, 1)\n",
        "    y_test = y_test.reshape(-1, 1)\n",
        "    \n",
        "    # Define schedulers\n",
        "    schedulers = {\n",
        "        'Constant': LearningRateSchedulers.constant_lr,\n",
        "        'Step Decay': lambda epoch, lr: LearningRateSchedulers.step_decay(epoch, lr, 0.7, 25),\n",
        "        'Exponential': lambda epoch, lr: LearningRateSchedulers.exponential_decay(epoch, lr, 0.98),\n",
        "        'Cosine': lambda epoch, lr: LearningRateSchedulers.cosine_annealing(epoch, lr, 100),\n",
        "        'Warmup+Cosine': lambda epoch, lr: LearningRateSchedulers.linear_warmup_cosine_decay(epoch, lr, 10, 100)\n",
        "    }\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    plt.figure(figsize=(20, 15))\n",
        "    \n",
        "    for i, (name, scheduler) in enumerate(schedulers.items()):\n",
        "        print(f\"\\\\nTraining with {name} LR schedule...\")\n",
        "        \n",
        "        # Create and train model\n",
        "        model = BatchGradientDescent(20, 50, 1, batch_size=32)\n",
        "        model.fit(X_train_scaled, y_train, X_val_scaled, y_val, \n",
        "                 epochs=100, learning_rate=0.1, lr_scheduler=scheduler)\n",
        "        \n",
        "        # Make predictions\n",
        "        test_pred = model.predict(X_test_scaled)\n",
        "        accuracy = accuracy_score(y_test, (test_pred > 0.5).astype(int))\n",
        "        \n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'accuracy': accuracy,\n",
        "            'final_train_loss': model.train_losses[-1],\n",
        "            'final_val_loss': model.val_losses[-1]\n",
        "        }\n",
        "        \n",
        "        # Plot training curves\n",
        "        plt.subplot(3, 5, i+1)\n",
        "        plt.plot(model.train_losses, label='Train Loss', alpha=0.8)\n",
        "        plt.plot(model.val_losses, label='Val Loss', alpha=0.8)\n",
        "        plt.title(f'{name}\\\\nAccuracy: {accuracy:.3f}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Plot learning rate schedule\n",
        "        plt.subplot(3, 5, i+6)\n",
        "        plt.plot(model.learning_rates, linewidth=2)\n",
        "        plt.title(f'{name} LR Schedule')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Learning Rate')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Combined loss curves\n",
        "    plt.subplot(3, 5, 11)\n",
        "    for name, result in results.items():\n",
        "        plt.plot(result['model'].train_losses, label=f'{name} Train', alpha=0.7)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Training Loss')\n",
        "    plt.title('Training Loss Comparison')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.subplot(3, 5, 12)\n",
        "    for name, result in results.items():\n",
        "        plt.plot(result['model'].val_losses, label=f'{name} Val', alpha=0.7)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Validation Loss')\n",
        "    plt.title('Validation Loss Comparison')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Summary metrics\n",
        "    plt.subplot(3, 5, 13)\n",
        "    names = list(results.keys())\n",
        "    accuracies = [results[name]['accuracy'] for name in names]\n",
        "    \n",
        "    bars = plt.bar(names, accuracies, alpha=0.7, color='skyblue')\n",
        "    plt.ylabel('Test Accuracy')\n",
        "    plt.title('Final Accuracy Comparison')\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    for bar, acc in zip(bars, accuracies):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
        "                f'{acc:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    # Convergence speed analysis\n",
        "    plt.subplot(3, 5, 14)\n",
        "    convergence_epochs = []\n",
        "    for name, result in results.items():\n",
        "        # Find when validation loss stabilizes (within 1% of final value)\n",
        "        final_val_loss = result['final_val_loss']\n",
        "        threshold = final_val_loss * 1.01\n",
        "        \n",
        "        converged_epoch = len(result['model'].val_losses)\n",
        "        for epoch, val_loss in enumerate(result['model'].val_losses):\n",
        "            if val_loss <= threshold:\n",
        "                converged_epoch = epoch\n",
        "                break\n",
        "        \n",
        "        convergence_epochs.append(converged_epoch)\n",
        "    \n",
        "    bars = plt.bar(names, convergence_epochs, alpha=0.7, color='lightgreen')\n",
        "    plt.ylabel('Convergence Epoch')\n",
        "    plt.title('Convergence Speed')\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    for bar, epoch in zip(bars, convergence_epochs):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                f'{epoch}', ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Visualize LR schedules first\n",
        "print(\"=== Learning Rate Schedules Visualization ===\")\n",
        "lr_schedules = visualize_lr_schedules()\n",
        "\n",
        "# Compare LR schedules in training\n",
        "print(\"\\\\n=== Learning Rate Schedule Comparison ===\")\n",
        "lr_results = compare_lr_schedules()\n",
        "\n",
        "print(\"\\\\n=== LR Schedule Results Summary ===\")\n",
        "for name, result in lr_results.items():\n",
        "    print(f\"\\\\n{name}:\")\n",
        "    print(f\"  Test Accuracy: {result['accuracy']:.4f}\")\n",
        "    print(f\"  Final Train Loss: {result['final_train_loss']:.4f}\")\n",
        "    print(f\"  Final Val Loss: {result['final_val_loss']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Advanced Optimization Algorithms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OptimizedNeuralNetwork:\n",
        "    \"\"\"Neural network with advanced optimization algorithms\"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, optimizer='sgd'):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "        # Initialize weights\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "        \n",
        "        # Optimizer state variables\n",
        "        self._init_optimizer_state()\n",
        "        \n",
        "        # Training history\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        \n",
        "    def _init_optimizer_state(self):\n",
        "        \"\"\"Initialize optimizer-specific state variables\"\"\"\n",
        "        if self.optimizer in ['momentum', 'adam']:\n",
        "            # Momentum terms\n",
        "            self.vW1 = np.zeros_like(self.W1)\n",
        "            self.vb1 = np.zeros_like(self.b1)\n",
        "            self.vW2 = np.zeros_like(self.W2)\n",
        "            self.vb2 = np.zeros_like(self.b2)\n",
        "        \n",
        "        if self.optimizer in ['rmsprop', 'adam']:\n",
        "            # Second moment terms\n",
        "            self.sW1 = np.zeros_like(self.W1)\n",
        "            self.sb1 = np.zeros_like(self.b1)\n",
        "            self.sW2 = np.zeros_like(self.W2)\n",
        "            self.sb2 = np.zeros_like(self.b2)\n",
        "        \n",
        "        if self.optimizer == 'adam':\n",
        "            self.t = 0  # Time step for bias correction\n",
        "    \n",
        "    def _relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "    \n",
        "    def _relu_derivative(self, x):\n",
        "        return (x > 0).astype(float)\n",
        "    \n",
        "    def _sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n",
        "    \n",
        "    def forward(self, X):\n",
        "        z1 = np.dot(X, self.W1) + self.b1\n",
        "        a1 = self._relu(z1)\n",
        "        z2 = np.dot(a1, self.W2) + self.b2\n",
        "        a2 = self._sigmoid(z2)\n",
        "        return a2, a1, z1, z2\n",
        "    \n",
        "    def backward(self, X, y, a2, a1, z1):\n",
        "        m = X.shape[0]\n",
        "        \n",
        "        dz2 = a2 - y\n",
        "        dW2 = (1/m) * np.dot(a1.T, dz2)\n",
        "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
        "        \n",
        "        da1 = np.dot(dz2, self.W2.T)\n",
        "        dz1 = da1 * self._relu_derivative(z1)\n",
        "        dW1 = (1/m) * np.dot(X.T, dz1)\n",
        "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
        "        \n",
        "        return dW1, db1, dW2, db2\n",
        "    \n",
        "    def _update_parameters_sgd(self, dW1, db1, dW2, db2, learning_rate):\n",
        "        \"\"\"Standard SGD parameter update\"\"\"\n",
        "        self.W1 -= learning_rate * dW1\n",
        "        self.b1 -= learning_rate * db1\n",
        "        self.W2 -= learning_rate * dW2\n",
        "        self.b2 -= learning_rate * db2\n",
        "    \n",
        "    def _update_parameters_momentum(self, dW1, db1, dW2, db2, learning_rate, beta=0.9):\n",
        "        \"\"\"SGD with momentum parameter update\"\"\"\n",
        "        # Update momentum terms\n",
        "        self.vW1 = beta * self.vW1 + (1 - beta) * dW1\n",
        "        self.vb1 = beta * self.vb1 + (1 - beta) * db1\n",
        "        self.vW2 = beta * self.vW2 + (1 - beta) * dW2\n",
        "        self.vb2 = beta * self.vb2 + (1 - beta) * db2\n",
        "        \n",
        "        # Update parameters\n",
        "        self.W1 -= learning_rate * self.vW1\n",
        "        self.b1 -= learning_rate * self.vb1\n",
        "        self.W2 -= learning_rate * self.vW2\n",
        "        self.b2 -= learning_rate * self.vb2\n",
        "    \n",
        "    def _update_parameters_rmsprop(self, dW1, db1, dW2, db2, learning_rate, beta=0.999, epsilon=1e-8):\n",
        "        \"\"\"RMSprop parameter update\"\"\"\n",
        "        # Update second moment estimates\n",
        "        self.sW1 = beta * self.sW1 + (1 - beta) * dW1**2\n",
        "        self.sb1 = beta * self.sb1 + (1 - beta) * db1**2\n",
        "        self.sW2 = beta * self.sW2 + (1 - beta) * dW2**2\n",
        "        self.sb2 = beta * self.sb2 + (1 - beta) * db2**2\n",
        "        \n",
        "        # Update parameters\n",
        "        self.W1 -= learning_rate * dW1 / (np.sqrt(self.sW1) + epsilon)\n",
        "        self.b1 -= learning_rate * db1 / (np.sqrt(self.sb1) + epsilon)\n",
        "        self.W2 -= learning_rate * dW2 / (np.sqrt(self.sW2) + epsilon)\n",
        "        self.b2 -= learning_rate * db2 / (np.sqrt(self.sb2) + epsilon)\n",
        "    \n",
        "    def _update_parameters_adam(self, dW1, db1, dW2, db2, learning_rate, \n",
        "                               beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        \"\"\"Adam parameter update\"\"\"\n",
        "        self.t += 1\n",
        "        \n",
        "        # Update biased first moment estimates\n",
        "        self.vW1 = beta1 * self.vW1 + (1 - beta1) * dW1\n",
        "        self.vb1 = beta1 * self.vb1 + (1 - beta1) * db1\n",
        "        self.vW2 = beta1 * self.vW2 + (1 - beta1) * dW2\n",
        "        self.vb2 = beta1 * self.vb2 + (1 - beta1) * db2\n",
        "        \n",
        "        # Update biased second moment estimates\n",
        "        self.sW1 = beta2 * self.sW1 + (1 - beta2) * dW1**2\n",
        "        self.sb1 = beta2 * self.sb1 + (1 - beta2) * db1**2\n",
        "        self.sW2 = beta2 * self.sW2 + (1 - beta2) * dW2**2\n",
        "        self.sb2 = beta2 * self.sb2 + (1 - beta2) * db2**2\n",
        "        \n",
        "        # Bias correction\n",
        "        vW1_corrected = self.vW1 / (1 - beta1**self.t)\n",
        "        vb1_corrected = self.vb1 / (1 - beta1**self.t)\n",
        "        vW2_corrected = self.vW2 / (1 - beta1**self.t)\n",
        "        vb2_corrected = self.vb2 / (1 - beta1**self.t)\n",
        "        \n",
        "        sW1_corrected = self.sW1 / (1 - beta2**self.t)\n",
        "        sb1_corrected = self.sb1 / (1 - beta2**self.t)\n",
        "        sW2_corrected = self.sW2 / (1 - beta2**self.t)\n",
        "        sb2_corrected = self.sb2 / (1 - beta2**self.t)\n",
        "        \n",
        "        # Update parameters\n",
        "        self.W1 -= learning_rate * vW1_corrected / (np.sqrt(sW1_corrected) + epsilon)\n",
        "        self.b1 -= learning_rate * vb1_corrected / (np.sqrt(sb1_corrected) + epsilon)\n",
        "        self.W2 -= learning_rate * vW2_corrected / (np.sqrt(sW2_corrected) + epsilon)\n",
        "        self.b2 -= learning_rate * vb2_corrected / (np.sqrt(sb2_corrected) + epsilon)\n",
        "    \n",
        "    def update_parameters(self, dW1, db1, dW2, db2, learning_rate):\n",
        "        \"\"\"Update parameters using selected optimizer\"\"\"\n",
        "        if self.optimizer == 'sgd':\n",
        "            self._update_parameters_sgd(dW1, db1, dW2, db2, learning_rate)\n",
        "        elif self.optimizer == 'momentum':\n",
        "            self._update_parameters_momentum(dW1, db1, dW2, db2, learning_rate)\n",
        "        elif self.optimizer == 'rmsprop':\n",
        "            self._update_parameters_rmsprop(dW1, db1, dW2, db2, learning_rate)\n",
        "        elif self.optimizer == 'adam':\n",
        "            self._update_parameters_adam(dW1, db1, dW2, db2, learning_rate)\n",
        "    \n",
        "    def compute_loss(self, X, y):\n",
        "        a2, _, _, _ = self.forward(X)\n",
        "        loss = -np.mean(y * np.log(a2 + 1e-15) + (1 - y) * np.log(1 - a2 + 1e-15))\n",
        "        return loss\n",
        "    \n",
        "    def fit(self, X_train, y_train, X_val, y_val, epochs=100, learning_rate=0.01, batch_size=32):\n",
        "        \"\"\"Train the network\"\"\"\n",
        "        m = X_train.shape[0]\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Shuffle data\n",
        "            indices = np.random.permutation(m)\n",
        "            X_shuffled = X_train[indices]\n",
        "            y_shuffled = y_train[indices]\n",
        "            \n",
        "            # Mini-batch training\n",
        "            for i in range(0, m, batch_size):\n",
        "                end_idx = min(i + batch_size, m)\n",
        "                batch_X = X_shuffled[i:end_idx]\n",
        "                batch_y = y_shuffled[i:end_idx]\n",
        "                \n",
        "                # Forward pass\n",
        "                a2, a1, z1, z2 = self.forward(batch_X)\n",
        "                \n",
        "                # Backward pass\n",
        "                dW1, db1, dW2, db2 = self.backward(batch_X, batch_y, a2, a1, z1)\n",
        "                \n",
        "                # Update parameters\n",
        "                self.update_parameters(dW1, db1, dW2, db2, learning_rate)\n",
        "            \n",
        "            # Compute losses\n",
        "            train_loss = self.compute_loss(X_train, y_train)\n",
        "            val_loss = self.compute_loss(X_val, y_val)\n",
        "            \n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "            \n",
        "            if epoch % 20 == 0:\n",
        "                print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "    \n",
        "    def predict(self, X):\n",
        "        a2, _, _, _ = self.forward(X)\n",
        "        return a2\n",
        "\n",
        "# Compare optimization algorithms\n",
        "def compare_optimizers():\n",
        "    \"\"\"Compare different optimization algorithms\"\"\"\n",
        "    \n",
        "    # Generate challenging dataset\n",
        "    X, y = make_classification(n_samples=2000, n_features=50, n_informative=30, \n",
        "                              n_redundant=20, n_clusters_per_class=2, \n",
        "                              class_sep=0.8, random_state=42)\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    y_train = y_train.reshape(-1, 1)\n",
        "    y_val = y_val.reshape(-1, 1)\n",
        "    y_test = y_test.reshape(-1, 1)\n",
        "    \n",
        "    # Test different optimizers\n",
        "    optimizers = ['sgd', 'momentum', 'rmsprop', 'adam']\n",
        "    optimizer_names = ['SGD', 'SGD + Momentum', 'RMSprop', 'Adam']\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    plt.figure(figsize=(20, 12))\n",
        "    \n",
        "    for i, (optimizer, name) in enumerate(zip(optimizers, optimizer_names)):\n",
        "        print(f\"\\\\nTraining with {name} optimizer...\")\n",
        "        \n",
        "        # Create and train model\n",
        "        model = OptimizedNeuralNetwork(50, 100, 1, optimizer=optimizer)\n",
        "        model.fit(X_train_scaled, y_train, X_val_scaled, y_val, \n",
        "                 epochs=100, learning_rate=0.01, batch_size=32)\n",
        "        \n",
        "        # Make predictions\n",
        "        test_pred = model.predict(X_test_scaled)\n",
        "        accuracy = accuracy_score(y_test, (test_pred > 0.5).astype(int))\n",
        "        \n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'accuracy': accuracy,\n",
        "            'final_train_loss': model.train_losses[-1],\n",
        "            'final_val_loss': model.val_losses[-1]\n",
        "        }\n",
        "        \n",
        "        # Plot training curves\n",
        "        plt.subplot(2, 4, i+1)\n",
        "        plt.plot(model.train_losses, label='Train Loss', alpha=0.8)\n",
        "        plt.plot(model.val_losses, label='Val Loss', alpha=0.8)\n",
        "        plt.title(f'{name}\\\\nAccuracy: {accuracy:.3f}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Combined comparison plots\n",
        "    plt.subplot(2, 4, 5)\n",
        "    for name, result in results.items():\n",
        "        plt.plot(result['model'].train_losses, label=name, alpha=0.8, linewidth=2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Training Loss')\n",
        "    plt.title('Training Loss Comparison')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.subplot(2, 4, 6)\n",
        "    for name, result in results.items():\n",
        "        plt.plot(result['model'].val_losses, label=name, alpha=0.8, linewidth=2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Validation Loss')\n",
        "    plt.title('Validation Loss Comparison')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Final metrics comparison\n",
        "    plt.subplot(2, 4, 7)\n",
        "    names = list(results.keys())\n",
        "    accuracies = [results[name]['accuracy'] for name in names]\n",
        "    train_losses = [results[name]['final_train_loss'] for name in names]\n",
        "    val_losses = [results[name]['final_val_loss'] for name in names]\n",
        "    \n",
        "    x_pos = np.arange(len(names))\n",
        "    plt.bar(x_pos, accuracies, alpha=0.7, color='skyblue')\n",
        "    plt.xlabel('Optimizer')\n",
        "    plt.ylabel('Test Accuracy')\n",
        "    plt.title('Final Accuracy Comparison')\n",
        "    plt.xticks(x_pos, names, rotation=45)\n",
        "    \n",
        "    for i, acc in enumerate(accuracies):\n",
        "        plt.text(i, acc + 0.005, f'{acc:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    # Convergence analysis\n",
        "    plt.subplot(2, 4, 8)\n",
        "    convergence_rates = []\n",
        "    for name, result in results.items():\n",
        "        # Calculate convergence rate (how fast loss decreases)\n",
        "        train_losses = np.array(result['model'].train_losses)\n",
        "        initial_loss = train_losses[0]\n",
        "        final_loss = train_losses[-1]\n",
        "        convergence_rate = (initial_loss - final_loss) / len(train_losses)\n",
        "        convergence_rates.append(convergence_rate)\n",
        "    \n",
        "    bars = plt.bar(names, convergence_rates, alpha=0.7, color='lightgreen')\n",
        "    plt.xlabel('Optimizer')\n",
        "    plt.ylabel('Convergence Rate')\n",
        "    plt.title('Convergence Speed')\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    for bar, rate in zip(bars, convergence_rates):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.0001,\n",
        "                f'{rate:.4f}', ha='center', va='bottom', fontsize=8)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run optimizer comparison\n",
        "print(\"=== Optimization Algorithms Comparison ===\")\n",
        "optimizer_results = compare_optimizers()\n",
        "\n",
        "print(\"\\\\n=== Optimizer Results Summary ===\")\n",
        "for name, result in optimizer_results.items():\n",
        "    print(f\"\\\\n{name}:\")\n",
        "    print(f\"  Test Accuracy: {result['accuracy']:.4f}\")\n",
        "    print(f\"  Final Train Loss: {result['final_train_loss']:.4f}\")\n",
        "    print(f\"  Final Val Loss: {result['final_val_loss']:.4f}\")\n",
        "    \n",
        "    # Calculate convergence metrics\n",
        "    train_losses = np.array(result['model'].train_losses)\n",
        "    improvement = train_losses[0] - train_losses[-1]\n",
        "    print(f\"  Total Loss Improvement: {improvement:.4f}\")\n",
        "    print(f\"  Convergence Rate: {improvement / len(train_losses):.6f} per epoch\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Key Insights and Learning Reflections\n",
        "\n",
        "### Batch Processing Insights:\n",
        "\n",
        "1. **Batch Size Effects**:\n",
        "   - Small batches (SGD): Noisy gradients, better generalization\n",
        "   - Large batches: Smoother gradients, faster convergence, potential overfitting\n",
        "   - Medium batches (32-128): Good balance between stability and generalization\n",
        "\n",
        "2. **Training Dynamics**:\n",
        "   - Smaller batches add beneficial noise for escaping local minima\n",
        "   - Larger batches converge faster but may get stuck in sharp minima\n",
        "   - Full batch gradient descent is most stable but computationally expensive\n",
        "\n",
        "### Learning Rate Scheduling Insights:\n",
        "\n",
        "1. **Schedule Selection**:\n",
        "   - Constant LR: Simple but may not converge optimally\n",
        "   - Step decay: Easy to implement, works well in practice\n",
        "   - Cosine annealing: Smooth decay, good for fine-tuning\n",
        "   - Warmup: Essential for large learning rates and complex models\n",
        "\n",
        "2. **Practical Tips**:\n",
        "   - Start with warmup for stability\n",
        "   - Use cosine decay for smooth convergence\n",
        "   - Monitor validation loss to adjust schedule timing\n",
        "\n",
        "### Optimization Algorithm Insights:\n",
        "\n",
        "1. **Algorithm Comparison**:\n",
        "   - **SGD**: Simple, works well with good LR scheduling\n",
        "   - **Momentum**: Accelerates convergence, reduces oscillations\n",
        "   - **RMSprop**: Adaptive per-parameter learning rates\n",
        "   - **Adam**: Combines momentum and adaptive rates, often works out-of-the-box\n",
        "\n",
        "2. **When to Use What**:\n",
        "   - Adam: Good default choice for most problems\n",
        "   - SGD + Momentum: When you want more control and stability\n",
        "   - RMSprop: Good for RNNs and non-stationary problems\n",
        "   - SGD: When you have time to tune hyperparameters carefully\n",
        "\n",
        "### Key Takeaways:\n",
        "- Batch size affects both training dynamics and generalization\n",
        "- Learning rate scheduling is crucial for optimal convergence\n",
        "- Advanced optimizers like Adam often work well with default settings\n",
        "- The choice of training strategy depends on your specific problem and constraints\n",
        "\n",
        "### Tomorrow's Focus:\n",
        "We'll dive deep into t-SNE implementation details, exploring the crowding problem and early exaggeration techniques!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
