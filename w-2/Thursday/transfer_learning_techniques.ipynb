{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Transfer Learning Techniques\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand transfer learning principles and advantages\n",
        "- Implement feature extraction vs fine-tuning strategies\n",
        "- Work with pre-trained models (ResNet, VGG, MobileNet)\n",
        "- Apply transfer learning to custom datasets\n",
        "- Compare different transfer learning approaches\n",
        "- Analyze layer freezing and unfreezing strategies\n",
        "\n",
        "## Transfer Learning Theory\n",
        "\n",
        "### Core Concepts\n",
        "**Transfer Learning** leverages knowledge gained from pre-trained models to solve related tasks:\n",
        "\n",
        "1. **Feature Extraction**: Use pre-trained features, freeze weights\n",
        "2. **Fine-tuning**: Adapt pre-trained weights to new task\n",
        "3. **Domain Adaptation**: Transfer across different domains\n",
        "4. **Multi-task Learning**: Learn multiple related tasks simultaneously\n",
        "\n",
        "### Key Advantages\n",
        "- **Reduced Training Time**: Start with learned features\n",
        "- **Better Performance**: Especially with limited data\n",
        "- **Lower Computational Cost**: Less training required\n",
        "- **Improved Generalization**: Leverage large-scale pre-training\n",
        "\n",
        "### Transfer Learning Strategies\n",
        "1. **Feature Extraction**: \n",
        "   - Freeze convolutional base\n",
        "   - Add custom classifier head\n",
        "   - Train only new layers\n",
        "\n",
        "2. **Fine-tuning**:\n",
        "   - Start with feature extraction\n",
        "   - Unfreeze some/all pre-trained layers\n",
        "   - Continue training with lower learning rate\n",
        "\n",
        "3. **Progressive Unfreezing**:\n",
        "   - Gradually unfreeze layers from top to bottom\n",
        "   - Fine-tune in stages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_digits, make_classification\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try to import deep learning libraries\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2\n",
        "    from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten\n",
        "    from tensorflow.keras.models import Model, Sequential\n",
        "    from tensorflow.keras.optimizers import Adam, SGD\n",
        "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    TF_AVAILABLE = True\n",
        "    print(\"TensorFlow/Keras available!\")\n",
        "except ImportError:\n",
        "    TF_AVAILABLE = False\n",
        "    print(\"TensorFlow/Keras not available - will use simplified demonstrations\")\n",
        "\n",
        "# Set style and random seed\n",
        "plt.style.use('seaborn-v0_8')\n",
        "np.random.seed(42)\n",
        "if TF_AVAILABLE:\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Pre-trained Model Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze pre-trained model architectures\n",
        "def analyze_pretrained_models():\n",
        "    \"\"\"Analyze different pre-trained model architectures\"\"\"\n",
        "    \n",
        "    if not TF_AVAILABLE:\n",
        "        print(\"TensorFlow not available - showing conceptual analysis\")\n",
        "        \n",
        "        # Create a conceptual demonstration\n",
        "        models_info = {\n",
        "            'VGG16': {\n",
        "                'parameters': '138M',\n",
        "                'depth': 16,\n",
        "                'key_features': ['Simple architecture', 'Small filters (3x3)', 'Deep network'],\n",
        "                'best_for': ['Feature extraction', 'Transfer learning baseline']\n",
        "            },\n",
        "            'ResNet50': {\n",
        "                'parameters': '25.6M',\n",
        "                'depth': 50,\n",
        "                'key_features': ['Residual connections', 'Skip connections', 'Deeper networks'],\n",
        "                'best_for': ['Complex tasks', 'Fine-tuning', 'High accuracy']\n",
        "            },\n",
        "            'MobileNetV2': {\n",
        "                'parameters': '3.4M',\n",
        "                'depth': 53,\n",
        "                'key_features': ['Depthwise separable conv', 'Inverted residuals', 'Linear bottlenecks'],\n",
        "                'best_for': ['Mobile deployment', 'Speed', 'Resource constraints']\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # Display model comparison\n",
        "        print(\"=== Pre-trained Model Comparison ===\\\\n\")\n",
        "        for name, info in models_info.items():\n",
        "            print(f\"{name}:\")\n",
        "            print(f\"  Parameters: {info['parameters']}\")\n",
        "            print(f\"  Depth: {info['depth']} layers\")\n",
        "            print(f\"  Key Features: {', '.join(info['key_features'])}\")\n",
        "            print(f\"  Best For: {', '.join(info['best_for'])}\")\n",
        "            print()\n",
        "        \n",
        "        # Create a simple visualization\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "        \n",
        "        models = list(models_info.keys())\n",
        "        params = [138, 25.6, 3.4]  # in millions\n",
        "        depths = [16, 50, 53]\n",
        "        \n",
        "        # Parameters comparison\n",
        "        bars1 = axes[0].bar(models, params, color=['blue', 'red', 'green'], alpha=0.7)\n",
        "        axes[0].set_ylabel('Parameters (Millions)')\n",
        "        axes[0].set_title('Model Size Comparison')\n",
        "        axes[0].tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # Add value labels\n",
        "        for bar, param in zip(bars1, params):\n",
        "            axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
        "                        f'{param}M', ha='center', va='bottom')\n",
        "        \n",
        "        # Depth comparison\n",
        "        bars2 = axes[1].bar(models, depths, color=['blue', 'red', 'green'], alpha=0.7)\n",
        "        axes[1].set_ylabel('Number of Layers')\n",
        "        axes[1].set_title('Model Depth Comparison')\n",
        "        axes[1].tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # Add value labels\n",
        "        for bar, depth in zip(bars2, depths):\n",
        "            axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                        f'{depth}', ha='center', va='bottom')\n",
        "        \n",
        "        # Efficiency plot (conceptual)\n",
        "        efficiency = [85, 92, 88]  # conceptual accuracy scores\n",
        "        speed = [1.2, 2.8, 0.8]    # conceptual relative speed\n",
        "        \n",
        "        scatter = axes[2].scatter(speed, efficiency, s=[p*5 for p in params], \n",
        "                                 c=['blue', 'red', 'green'], alpha=0.7)\n",
        "        axes[2].set_xlabel('Relative Speed')\n",
        "        axes[2].set_ylabel('Accuracy (%)')\n",
        "        axes[2].set_title('Accuracy vs Speed vs Size')\n",
        "        \n",
        "        # Add model labels\n",
        "        for i, model in enumerate(models):\n",
        "            axes[2].annotate(model, (speed[i], efficiency[i]), \n",
        "                           xytext=(5, 5), textcoords='offset points')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return models_info\n",
        "    \n",
        "    # Load and analyze actual pre-trained models\n",
        "    print(\"=== Loading Pre-trained Models ===\")\n",
        "    \n",
        "    models = {}\n",
        "    \n",
        "    # Load VGG16\n",
        "    print(\"Loading VGG16...\")\n",
        "    vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    models['VGG16'] = vgg16\n",
        "    \n",
        "    # Load ResNet50\n",
        "    print(\"Loading ResNet50...\")\n",
        "    resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    models['ResNet50'] = resnet\n",
        "    \n",
        "    # Load MobileNetV2\n",
        "    print(\"Loading MobileNetV2...\")\n",
        "    mobilenet = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    models['MobileNetV2'] = mobilenet\n",
        "    \n",
        "    # Analyze model characteristics\n",
        "    model_stats = {}\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        print(f\"\\\\n=== {name} Analysis ===\")\n",
        "        \n",
        "        # Count parameters\n",
        "        total_params = model.count_params()\n",
        "        trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
        "        \n",
        "        # Model summary info\n",
        "        model_stats[name] = {\n",
        "            'total_params': total_params,\n",
        "            'trainable_params': trainable_params,\n",
        "            'layers': len(model.layers),\n",
        "            'output_shape': model.output_shape\n",
        "        }\n",
        "        \n",
        "        print(f\"Total Parameters: {total_params:,}\")\n",
        "        print(f\"Trainable Parameters: {trainable_params:,}\")\n",
        "        print(f\"Number of Layers: {len(model.layers)}\")\n",
        "        print(f\"Output Shape: {model.output_shape}\")\n",
        "        \n",
        "        # Show first few and last few layers\n",
        "        print(\"\\\\nFirst 3 layers:\")\n",
        "        for i, layer in enumerate(model.layers[:3]):\n",
        "            print(f\"  {i+1}. {layer.name}: {layer.__class__.__name__}\")\n",
        "        \n",
        "        print(\"\\\\nLast 3 layers:\")\n",
        "        for i, layer in enumerate(model.layers[-3:]):\n",
        "            layer_idx = len(model.layers) - 3 + i + 1\n",
        "            print(f\"  {layer_idx}. {layer.name}: {layer.__class__.__name__}\")\n",
        "    \n",
        "    # Visualization of model comparison\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    model_names = list(model_stats.keys())\n",
        "    total_params = [model_stats[name]['total_params'] for name in model_names]\n",
        "    trainable_params = [model_stats[name]['trainable_params'] for name in model_names]\n",
        "    num_layers = [model_stats[name]['layers'] for name in model_names]\n",
        "    \n",
        "    # Parameters comparison\n",
        "    x = np.arange(len(model_names))\n",
        "    width = 0.35\n",
        "    \n",
        "    bars1 = axes[0,0].bar(x - width/2, np.array(total_params)/1e6, width, \n",
        "                          label='Total', alpha=0.7)\n",
        "    bars2 = axes[0,0].bar(x + width/2, np.array(trainable_params)/1e6, width, \n",
        "                          label='Trainable', alpha=0.7)\n",
        "    axes[0,0].set_ylabel('Parameters (Millions)')\n",
        "    axes[0,0].set_title('Parameter Comparison')\n",
        "    axes[0,0].set_xticks(x)\n",
        "    axes[0,0].set_xticklabels(model_names, rotation=45)\n",
        "    axes[0,0].legend()\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, param in zip(bars1, total_params):\n",
        "        axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                      f'{param/1e6:.1f}M', ha='center', va='bottom', fontsize=8)\n",
        "    \n",
        "    # Number of layers\n",
        "    bars3 = axes[0,1].bar(model_names, num_layers, color=['blue', 'red', 'green'], alpha=0.7)\n",
        "    axes[0,1].set_ylabel('Number of Layers')\n",
        "    axes[0,1].set_title('Model Depth')\n",
        "    axes[0,1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, layers in zip(bars3, num_layers):\n",
        "        axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                      f'{layers}', ha='center', va='bottom')\n",
        "    \n",
        "    # Memory usage estimation (conceptual)\n",
        "    memory_usage = [param/1e6 * 4 for param in total_params]  # Rough estimate in MB\n",
        "    bars4 = axes[1,0].bar(model_names, memory_usage, color=['blue', 'red', 'green'], alpha=0.7)\n",
        "    axes[1,0].set_ylabel('Memory Usage (MB)')\n",
        "    axes[1,0].set_title('Estimated Memory Usage')\n",
        "    axes[1,0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Efficiency plot (parameters vs layers)\n",
        "    scatter = axes[1,1].scatter(num_layers, np.array(total_params)/1e6, \n",
        "                               s=200, c=['blue', 'red', 'green'], alpha=0.7)\n",
        "    axes[1,1].set_xlabel('Number of Layers')\n",
        "    axes[1,1].set_ylabel('Parameters (Millions)')\n",
        "    axes[1,1].set_title('Architecture Efficiency')\n",
        "    \n",
        "    # Add model labels\n",
        "    for i, name in enumerate(model_names):\n",
        "        axes[1,1].annotate(name, (num_layers[i], total_params[i]/1e6), \n",
        "                          xytext=(5, 5), textcoords='offset points')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return models, model_stats\n",
        "\n",
        "# Run model analysis\n",
        "print(\"=== Pre-trained Model Analysis ===\")\n",
        "if TF_AVAILABLE:\n",
        "    pretrained_models, model_statistics = analyze_pretrained_models()\n",
        "else:\n",
        "    model_info = analyze_pretrained_models()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Transfer Learning Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transfer Learning Implementation\n",
        "def implement_transfer_learning():\n",
        "    \"\"\"Implement different transfer learning strategies\"\"\"\n",
        "    \n",
        "    if not TF_AVAILABLE:\n",
        "        print(\"TensorFlow not available - showing conceptual implementation\")\n",
        "        \n",
        "        # Create a conceptual demonstration with traditional ML\n",
        "        print(\"=== Conceptual Transfer Learning with Traditional ML ===\")\n",
        "        \n",
        "        # Generate sample high-dimensional data\n",
        "        X_source, y_source = make_classification(n_samples=2000, n_features=50, \n",
        "                                                n_informative=30, n_redundant=10,\n",
        "                                                n_classes=5, random_state=42)\n",
        "        \n",
        "        X_target, y_target = make_classification(n_samples=500, n_features=50,\n",
        "                                                n_informative=25, n_redundant=15,\n",
        "                                                n_classes=3, random_state=84)\n",
        "        \n",
        "        # Simulate \"pre-trained\" features using PCA on source data\n",
        "        from sklearn.decomposition import PCA\n",
        "        \n",
        "        # \"Pre-train\" feature extractor on source domain\n",
        "        print(\"1. Pre-training feature extractor on source domain...\")\n",
        "        pca_pretrained = PCA(n_components=20)\n",
        "        X_source_features = pca_pretrained.fit_transform(X_source)\n",
        "        \n",
        "        # Train classifier on source domain\n",
        "        source_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        source_classifier.fit(X_source_features, y_source)\n",
        "        source_accuracy = source_classifier.score(X_source_features, y_source)\n",
        "        print(f\"Source domain accuracy: {source_accuracy:.3f}\")\n",
        "        \n",
        "        # Apply transfer learning to target domain\n",
        "        print(\"\\\\n2. Applying transfer learning to target domain...\")\n",
        "        \n",
        "        # Strategy 1: Feature extraction only\n",
        "        X_target_features = pca_pretrained.transform(X_target)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_target_features, y_target, test_size=0.3, random_state=42)\n",
        "        \n",
        "        # Train new classifier on extracted features\n",
        "        transfer_classifier = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "        transfer_classifier.fit(X_train, y_train)\n",
        "        transfer_accuracy = transfer_classifier.score(X_test, y_test)\n",
        "        \n",
        "        # Strategy 2: Train from scratch for comparison\n",
        "        X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "            X_target, y_target, test_size=0.3, random_state=42)\n",
        "        \n",
        "        scratch_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        scratch_classifier.fit(X_train_raw, y_train)\n",
        "        scratch_accuracy = scratch_classifier.score(X_test_raw, y_test)\n",
        "        \n",
        "        print(f\"Transfer Learning Accuracy: {transfer_accuracy:.3f}\")\n",
        "        print(f\"From Scratch Accuracy: {scratch_accuracy:.3f}\")\n",
        "        print(f\"Transfer Learning Advantage: {transfer_accuracy - scratch_accuracy:.3f}\")\n",
        "        \n",
        "        # Visualize results\n",
        "        methods = ['Source Domain', 'Transfer Learning', 'From Scratch']\n",
        "        accuracies = [source_accuracy, transfer_accuracy, scratch_accuracy]\n",
        "        colors = ['blue', 'green', 'red']\n",
        "        \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        bars = plt.bar(methods, accuracies, color=colors, alpha=0.7)\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title('Transfer Learning Comparison (Conceptual)')\n",
        "        plt.ylim(0, 1)\n",
        "        \n",
        "        # Add value labels\n",
        "        for bar, acc in zip(bars, accuracies):\n",
        "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "                    f'{acc:.3f}', ha='center', va='bottom')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return {'transfer': transfer_accuracy, 'scratch': scratch_accuracy}\n",
        "    \n",
        "    # Real transfer learning implementation with deep learning\n",
        "    print(\"=== Implementing Transfer Learning with Deep Networks ===\")\n",
        "    \n",
        "    # Create sample image-like data (simulated since we don't have real image dataset)\n",
        "    print(\"Creating synthetic image-like dataset...\")\n",
        "    \n",
        "    # Generate synthetic data that mimics image classification\n",
        "    np.random.seed(42)\n",
        "    n_samples = 1000\n",
        "    n_classes = 3\n",
        "    img_size = 32  # Smaller for demonstration\n",
        "    \n",
        "    # Create synthetic \"images\" with some structure\n",
        "    X_synthetic = []\n",
        "    y_synthetic = []\n",
        "    \n",
        "    for class_idx in range(n_classes):\n",
        "        for _ in range(n_samples // n_classes):\n",
        "            # Create synthetic image with class-specific patterns\n",
        "            img = np.random.randn(img_size, img_size, 3) * 0.1\n",
        "            \n",
        "            # Add class-specific patterns\n",
        "            if class_idx == 0:  # Class 0: horizontal stripes\n",
        "                for i in range(0, img_size, 4):\n",
        "                    img[i:i+2, :, :] += 0.5\n",
        "            elif class_idx == 1:  # Class 1: vertical stripes  \n",
        "                for j in range(0, img_size, 4):\n",
        "                    img[:, j:j+2, :] += 0.5\n",
        "            else:  # Class 2: diagonal pattern\n",
        "                for i in range(img_size):\n",
        "                    for j in range(img_size):\n",
        "                        if (i + j) % 8 < 4:\n",
        "                            img[i, j, :] += 0.5\n",
        "            \n",
        "            # Normalize to [0, 1]\n",
        "            img = np.clip(img, 0, 1)\n",
        "            X_synthetic.append(img)\n",
        "            y_synthetic.append(class_idx)\n",
        "    \n",
        "    X_synthetic = np.array(X_synthetic)\n",
        "    y_synthetic = np.array(y_synthetic)\n",
        "    \n",
        "    print(f\"Created synthetic dataset: {X_synthetic.shape}\")\n",
        "    \n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_synthetic, y_synthetic, test_size=0.3, stratify=y_synthetic, random_state=42)\n",
        "    \n",
        "    # Resize images for pre-trained models (they expect 224x224)\n",
        "    X_train_resized = tf.image.resize(X_train, [224, 224]).numpy()\n",
        "    X_test_resized = tf.image.resize(X_test, [224, 224]).numpy()\n",
        "    \n",
        "    # Convert labels to categorical\n",
        "    y_train_cat = to_categorical(y_train, n_classes)\n",
        "    y_test_cat = to_categorical(y_test, n_classes)\n",
        "    \n",
        "    print(f\"Training set: {X_train_resized.shape}\")\n",
        "    print(f\"Test set: {X_test_resized.shape}\")\n",
        "    \n",
        "    # Strategy 1: Feature Extraction with VGG16\n",
        "    print(\"\\\\n=== Strategy 1: Feature Extraction ===\")\n",
        "    \n",
        "    # Load pre-trained VGG16 without top layers\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    \n",
        "    # Freeze all layers\n",
        "    base_model.trainable = False\n",
        "    \n",
        "    # Add custom classifier\n",
        "    model_feature_extraction = Sequential([\n",
        "        base_model,\n",
        "        GlobalAveragePooling2D(),\n",
        "        Dropout(0.2),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(n_classes, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    model_feature_extraction.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    print(\"Feature extraction model summary:\")\n",
        "    print(f\"Total params: {model_feature_extraction.count_params():,}\")\n",
        "    trainable_params = sum([tf.keras.backend.count_params(w) for w in model_feature_extraction.trainable_weights])\n",
        "    print(f\"Trainable params: {trainable_params:,}\")\n",
        "    \n",
        "    # Train feature extraction model\n",
        "    print(\"Training feature extraction model...\")\n",
        "    history_fe = model_feature_extraction.fit(\n",
        "        X_train_resized, y_train_cat,\n",
        "        batch_size=32,\n",
        "        epochs=5,  # Few epochs for demonstration\n",
        "        validation_data=(X_test_resized, y_test_cat),\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    # Strategy 2: Fine-tuning\n",
        "    print(\"\\\\n=== Strategy 2: Fine-tuning ===\")\n",
        "    \n",
        "    # Create a new model for fine-tuning\n",
        "    base_model_ft = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    \n",
        "    # First, train with frozen base\n",
        "    base_model_ft.trainable = False\n",
        "    \n",
        "    model_fine_tune = Sequential([\n",
        "        base_model_ft,\n",
        "        GlobalAveragePooling2D(),\n",
        "        Dropout(0.2),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(n_classes, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    model_fine_tune.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    # Train with frozen base first\n",
        "    print(\"Step 1: Training with frozen base...\")\n",
        "    model_fine_tune.fit(\n",
        "        X_train_resized, y_train_cat,\n",
        "        batch_size=32,\n",
        "        epochs=3,\n",
        "        validation_data=(X_test_resized, y_test_cat),\n",
        "        verbose=0\n",
        "    )\n",
        "    \n",
        "    # Unfreeze some layers for fine-tuning\n",
        "    base_model_ft.trainable = True\n",
        "    \n",
        "    # Fine-tune only the last few layers\n",
        "    fine_tune_at = len(base_model_ft.layers) - 10  # Unfreeze last 10 layers\n",
        "    \n",
        "    for layer in base_model_ft.layers[:fine_tune_at]:\n",
        "        layer.trainable = False\n",
        "    \n",
        "    # Recompile with lower learning rate\n",
        "    model_fine_tune.compile(\n",
        "        optimizer=Adam(learning_rate=0.0001/10),  # Lower learning rate\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    print(\"Step 2: Fine-tuning unfrozen layers...\")\n",
        "    history_ft = model_fine_tune.fit(\n",
        "        X_train_resized, y_train_cat,\n",
        "        batch_size=32,\n",
        "        epochs=3,\n",
        "        validation_data=(X_test_resized, y_test_cat),\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    # Strategy 3: Training from scratch (for comparison)\n",
        "    print(\"\\\\n=== Strategy 3: Training from Scratch ===\")\n",
        "    \n",
        "    model_scratch = Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        GlobalAveragePooling2D(),\n",
        "        Dropout(0.5),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(n_classes, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    model_scratch.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    print(\"Training from scratch...\")\n",
        "    history_scratch = model_scratch.fit(\n",
        "        X_train_resized, y_train_cat,\n",
        "        batch_size=32,\n",
        "        epochs=8,  # More epochs since training from scratch\n",
        "        validation_data=(X_test_resized, y_test_cat),\n",
        "        verbose=0\n",
        "    )\n",
        "    \n",
        "    # Evaluate all models\n",
        "    print(\"\\\\n=== Model Evaluation ===\")\n",
        "    \n",
        "    # Get final accuracies\n",
        "    fe_accuracy = model_feature_extraction.evaluate(X_test_resized, y_test_cat, verbose=0)[1]\n",
        "    ft_accuracy = model_fine_tune.evaluate(X_test_resized, y_test_cat, verbose=0)[1]\n",
        "    scratch_accuracy = model_scratch.evaluate(X_test_resized, y_test_cat, verbose=0)[1]\n",
        "    \n",
        "    print(f\"Feature Extraction Accuracy: {fe_accuracy:.4f}\")\n",
        "    print(f\"Fine-tuning Accuracy: {ft_accuracy:.4f}\")\n",
        "    print(f\"From Scratch Accuracy: {scratch_accuracy:.4f}\")\n",
        "    \n",
        "    # Visualize training histories\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Feature extraction history\n",
        "    axes[0,0].plot(history_fe.history['accuracy'], label='Train')\n",
        "    axes[0,0].plot(history_fe.history['val_accuracy'], label='Validation')\n",
        "    axes[0,0].set_title('Feature Extraction - Accuracy')\n",
        "    axes[0,0].set_xlabel('Epoch')\n",
        "    axes[0,0].set_ylabel('Accuracy')\n",
        "    axes[0,0].legend()\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Fine-tuning history\n",
        "    axes[0,1].plot(history_ft.history['accuracy'], label='Train')\n",
        "    axes[0,1].plot(history_ft.history['val_accuracy'], label='Validation')\n",
        "    axes[0,1].set_title('Fine-tuning - Accuracy')\n",
        "    axes[0,1].set_xlabel('Epoch')\n",
        "    axes[0,1].set_ylabel('Accuracy')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # From scratch history\n",
        "    axes[1,0].plot(history_scratch.history['accuracy'], label='Train')\n",
        "    axes[1,0].plot(history_scratch.history['val_accuracy'], label='Validation')\n",
        "    axes[1,0].set_title('From Scratch - Accuracy')\n",
        "    axes[1,0].set_xlabel('Epoch')\n",
        "    axes[1,0].set_ylabel('Accuracy')\n",
        "    axes[1,0].legend()\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Comparison of final accuracies\n",
        "    methods = ['Feature\\\\nExtraction', 'Fine-tuning', 'From Scratch']\n",
        "    accuracies = [fe_accuracy, ft_accuracy, scratch_accuracy]\n",
        "    colors = ['blue', 'green', 'red']\n",
        "    \n",
        "    bars = axes[1,1].bar(methods, accuracies, color=colors, alpha=0.7)\n",
        "    axes[1,1].set_ylabel('Test Accuracy')\n",
        "    axes[1,1].set_title('Transfer Learning Comparison')\n",
        "    axes[1,1].set_ylim(0, 1)\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, acc in zip(bars, accuracies):\n",
        "        axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "                      f'{acc:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Training efficiency analysis\n",
        "    total_epochs_fe = len(history_fe.history['accuracy'])\n",
        "    total_epochs_ft = 3 + len(history_ft.history['accuracy'])  # Initial frozen + fine-tuning\n",
        "    total_epochs_scratch = len(history_scratch.history['accuracy'])\n",
        "    \n",
        "    print(\"\\\\n=== Training Efficiency Analysis ===\")\n",
        "    print(f\"Feature Extraction: {total_epochs_fe} epochs, {fe_accuracy:.4f} accuracy\")\n",
        "    print(f\"Fine-tuning: {total_epochs_ft} epochs, {ft_accuracy:.4f} accuracy\")\n",
        "    print(f\"From Scratch: {total_epochs_scratch} epochs, {scratch_accuracy:.4f} accuracy\")\n",
        "    \n",
        "    # Create efficiency plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter([total_epochs_fe], [fe_accuracy], s=200, color='blue', alpha=0.7, label='Feature Extraction')\n",
        "    plt.scatter([total_epochs_ft], [ft_accuracy], s=200, color='green', alpha=0.7, label='Fine-tuning')\n",
        "    plt.scatter([total_epochs_scratch], [scratch_accuracy], s=200, color='red', alpha=0.7, label='From Scratch')\n",
        "    \n",
        "    plt.xlabel('Training Epochs')\n",
        "    plt.ylabel('Test Accuracy')\n",
        "    plt.title('Training Efficiency: Accuracy vs Training Time')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add annotations\n",
        "    plt.annotate(f'FE: {fe_accuracy:.3f}', (total_epochs_fe, fe_accuracy), \n",
        "                xytext=(5, 5), textcoords='offset points')\n",
        "    plt.annotate(f'FT: {ft_accuracy:.3f}', (total_epochs_ft, ft_accuracy),\n",
        "                xytext=(5, 5), textcoords='offset points')\n",
        "    plt.annotate(f'Scratch: {scratch_accuracy:.3f}', (total_epochs_scratch, scratch_accuracy),\n",
        "                xytext=(5, 5), textcoords='offset points')\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    return {\n",
        "        'feature_extraction': fe_accuracy,\n",
        "        'fine_tuning': ft_accuracy,\n",
        "        'from_scratch': scratch_accuracy,\n",
        "        'models': {\n",
        "            'fe': model_feature_extraction,\n",
        "            'ft': model_fine_tune,\n",
        "            'scratch': model_scratch\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Run transfer learning implementation\n",
        "print(\"=== Transfer Learning Implementation ===\")\n",
        "transfer_results = implement_transfer_learning()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Layer Freezing Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
