{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Model Evaluation for Image Classification\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand specialized metrics for image classification\n",
        "- Implement confusion matrices and classification reports\n",
        "- Analyze ROC curves and AUC for multi-class problems\n",
        "- Apply cross-validation strategies for computer vision\n",
        "- Explore class imbalance handling techniques\n",
        "- Implement statistical significance testing for model comparison\n",
        "\n",
        "## Image Classification Evaluation Challenges\n",
        "\n",
        "### Unique Considerations\n",
        "1. **Class Imbalance**: Often prevalent in real-world image datasets\n",
        "2. **Spatial Hierarchies**: Objects at different scales and positions\n",
        "3. **Multi-label Classification**: Images containing multiple objects\n",
        "4. **Domain Shift**: Performance degradation across different domains\n",
        "5. **Computational Constraints**: Real-time inference requirements\n",
        "\n",
        "### Key Metrics for Image Classification\n",
        "- **Accuracy**: Overall correctness percentage\n",
        "- **Precision/Recall**: Class-specific performance\n",
        "- **F1-Score**: Harmonic mean of precision and recall\n",
        "- **ROC-AUC**: Area under receiver operating characteristic curve\n",
        "- **Top-k Accuracy**: Correct label in top k predictions\n",
        "- **Mean Average Precision (mAP)**: For multi-label scenarios\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, load_digits\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, label_binarize\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_curve, auc,\n",
        "    roc_auc_score, average_precision_score, precision_recall_curve\n",
        ")\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy import stats\n",
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style and random seed\n",
        "plt.style.use('seaborn-v0_8')\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive evaluation framework\n",
        "class ImageClassificationEvaluator:\n",
        "    \"\"\"Comprehensive evaluation suite for image classification models\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.results = {}\n",
        "        self.models = {}\n",
        "    \n",
        "    def prepare_data(self, imbalanced=False):\n",
        "        \"\"\"Prepare datasets for evaluation\"\"\"\n",
        "        \n",
        "        # Use digits dataset as our \"image\" data\n",
        "        digits = load_digits()\n",
        "        X, y = digits.data, digits.target\n",
        "        \n",
        "        if imbalanced:\n",
        "            # Create imbalanced dataset\n",
        "            class_counts = np.bincount(y)\n",
        "            print(\"Creating imbalanced dataset...\")\n",
        "            \n",
        "            # Keep different amounts for each class\n",
        "            keep_ratios = [1.0, 0.8, 0.6, 0.4, 0.3, 0.2, 0.15, 0.1, 0.05, 0.02]\n",
        "            \n",
        "            indices_to_keep = []\n",
        "            for class_idx in range(10):\n",
        "                class_indices = np.where(y == class_idx)[0]\n",
        "                n_keep = int(len(class_indices) * keep_ratios[class_idx])\n",
        "                indices_to_keep.extend(class_indices[:n_keep])\n",
        "            \n",
        "            X = X[indices_to_keep]\n",
        "            y = y[indices_to_keep]\n",
        "            \n",
        "            print(\"Class distribution after imbalancing:\")\n",
        "            unique, counts = np.unique(y, return_counts=True)\n",
        "            for cls, count in zip(unique, counts):\n",
        "                print(f\"  Class {cls}: {count} samples\")\n",
        "        \n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "        \n",
        "        # Standardize features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        \n",
        "        return X_train_scaled, X_test_scaled, y_train, y_test\n",
        "    \n",
        "    def train_models(self, X_train, y_train):\n",
        "        \"\"\"Train multiple models for comparison\"\"\"\n",
        "        \n",
        "        models = {\n",
        "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "            'SVM': SVC(probability=True, random_state=42),\n",
        "            'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
        "        }\n",
        "        \n",
        "        print(\"Training models...\")\n",
        "        trained_models = {}\n",
        "        \n",
        "        for name, model in models.items():\n",
        "            print(f\"  Training {name}...\")\n",
        "            model.fit(X_train, y_train)\n",
        "            trained_models[name] = model\n",
        "        \n",
        "        self.models = trained_models\n",
        "        return trained_models\n",
        "    \n",
        "    def evaluate_single_model(self, model, model_name, X_test, y_test):\n",
        "        \"\"\"Comprehensive evaluation of a single model\"\"\"\n",
        "        \n",
        "        # Basic predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_prob = model.predict_proba(X_test)\n",
        "        \n",
        "        # Basic metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted')\n",
        "        recall = recall_score(y_test, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "        \n",
        "        # Per-class metrics\n",
        "        precision_per_class = precision_score(y_test, y_pred, average=None)\n",
        "        recall_per_class = recall_score(y_test, y_pred, average=None)\n",
        "        f1_per_class = f1_score(y_test, y_pred, average=None)\n",
        "        \n",
        "        # Multi-class AUC\n",
        "        y_test_bin = label_binarize(y_test, classes=np.unique(y_test))\n",
        "        if y_test_bin.shape[1] > 1:\n",
        "            auc_score = roc_auc_score(y_test_bin, y_prob, average='weighted', multi_class='ovr')\n",
        "        else:\n",
        "            auc_score = np.nan\n",
        "        \n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        \n",
        "        # Top-k accuracy (k=3)\n",
        "        top3_accuracy = self._top_k_accuracy(y_test, y_prob, k=3)\n",
        "        \n",
        "        results = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'auc_score': auc_score,\n",
        "            'top3_accuracy': top3_accuracy,\n",
        "            'precision_per_class': precision_per_class,\n",
        "            'recall_per_class': recall_per_class,\n",
        "            'f1_per_class': f1_per_class,\n",
        "            'confusion_matrix': cm,\n",
        "            'y_pred': y_pred,\n",
        "            'y_prob': y_prob\n",
        "        }\n",
        "        \n",
        "        self.results[model_name] = results\n",
        "        return results\n",
        "    \n",
        "    def _top_k_accuracy(self, y_true, y_prob, k=3):\n",
        "        \"\"\"Calculate top-k accuracy\"\"\"\n",
        "        top_k_preds = np.argsort(y_prob, axis=1)[:, -k:]\n",
        "        correct = 0\n",
        "        for i, true_label in enumerate(y_true):\n",
        "            if true_label in top_k_preds[i]:\n",
        "                correct += 1\n",
        "        return correct / len(y_true)\n",
        "    \n",
        "    def plot_confusion_matrices(self, class_names=None):\n",
        "        \"\"\"Plot confusion matrices for all models\"\"\"\n",
        "        \n",
        "        n_models = len(self.results)\n",
        "        fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n",
        "        if n_models == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for idx, (model_name, results) in enumerate(self.results.items()):\n",
        "            cm = results['confusion_matrix']\n",
        "            \n",
        "            # Normalize confusion matrix\n",
        "            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "            \n",
        "            # Plot\n",
        "            im = axes[idx].imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "            axes[idx].set_title(f'{model_name}\\\\nAccuracy: {results[\"accuracy\"]:.3f}')\n",
        "            \n",
        "            # Add colorbar\n",
        "            plt.colorbar(im, ax=axes[idx])\n",
        "            \n",
        "            # Add text annotations\n",
        "            thresh = cm_normalized.max() / 2\n",
        "            for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "                axes[idx].text(j, i, f'{cm[i, j]}\\\\n({cm_normalized[i, j]:.2f})',\n",
        "                             horizontalalignment=\"center\",\n",
        "                             color=\"white\" if cm_normalized[i, j] > thresh else \"black\",\n",
        "                             fontsize=8)\n",
        "            \n",
        "            axes[idx].set_ylabel('True Label')\n",
        "            axes[idx].set_xlabel('Predicted Label')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def plot_roc_curves(self, X_test, y_test):\n",
        "        \"\"\"Plot ROC curves for multi-class classification\"\"\"\n",
        "        \n",
        "        n_classes = len(np.unique(y_test))\n",
        "        y_test_bin = label_binarize(y_test, classes=range(n_classes))\n",
        "        \n",
        "        # Plot for each model\n",
        "        fig, axes = plt.subplots(1, len(self.models), figsize=(6*len(self.models), 5))\n",
        "        if len(self.models) == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for model_idx, (model_name, model) in enumerate(self.models.items()):\n",
        "            y_prob = model.predict_proba(X_test)\n",
        "            \n",
        "            # Calculate ROC curve and AUC for each class\n",
        "            fpr = dict()\n",
        "            tpr = dict()\n",
        "            roc_auc = dict()\n",
        "            \n",
        "            for i in range(n_classes):\n",
        "                if y_test_bin.shape[1] > 1:\n",
        "                    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_prob[:, i])\n",
        "                    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "            \n",
        "            # Plot ROC curves\n",
        "            colors = plt.cm.Set1(np.linspace(0, 1, n_classes))\n",
        "            for i, color in enumerate(colors):\n",
        "                if i in roc_auc:\n",
        "                    axes[model_idx].plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "                                       label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
        "            \n",
        "            axes[model_idx].plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "            axes[model_idx].set_xlim([0.0, 1.0])\n",
        "            axes[model_idx].set_ylim([0.0, 1.05])\n",
        "            axes[model_idx].set_xlabel('False Positive Rate')\n",
        "            axes[model_idx].set_ylabel('True Positive Rate')\n",
        "            axes[model_idx].set_title(f'{model_name} - ROC Curves')\n",
        "            axes[model_idx].legend(loc=\"lower right\", fontsize=8)\n",
        "            axes[model_idx].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def cross_validation_analysis(self, X, y, cv_folds=5):\n",
        "        \"\"\"Perform cross-validation analysis\"\"\"\n",
        "        \n",
        "        print(f\"\\\\n=== {cv_folds}-Fold Cross-Validation Analysis ===\")\n",
        "        \n",
        "        cv_results = {}\n",
        "        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "        \n",
        "        for model_name, model in self.models.items():\n",
        "            print(f\"\\\\nEvaluating {model_name}...\")\n",
        "            \n",
        "            # Cross-validation scores\n",
        "            cv_scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "            cv_f1 = cross_val_score(model, X, y, cv=skf, scoring='f1_weighted')\n",
        "            \n",
        "            cv_results[model_name] = {\n",
        "                'accuracy_scores': cv_scores,\n",
        "                'f1_scores': cv_f1,\n",
        "                'accuracy_mean': cv_scores.mean(),\n",
        "                'accuracy_std': cv_scores.std(),\n",
        "                'f1_mean': cv_f1.mean(),\n",
        "                'f1_std': cv_f1.std()\n",
        "            }\n",
        "            \n",
        "            print(f\"  Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "            print(f\"  F1-Score: {cv_f1.mean():.4f} (+/- {cv_f1.std() * 2:.4f})\")\n",
        "        \n",
        "        # Plot cross-validation results\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "        \n",
        "        model_names = list(cv_results.keys())\n",
        "        accuracy_means = [cv_results[name]['accuracy_mean'] for name in model_names]\n",
        "        accuracy_stds = [cv_results[name]['accuracy_std'] for name in model_names]\n",
        "        f1_means = [cv_results[name]['f1_mean'] for name in model_names]\n",
        "        f1_stds = [cv_results[name]['f1_std'] for name in model_names]\n",
        "        \n",
        "        # Accuracy comparison\n",
        "        x = np.arange(len(model_names))\n",
        "        axes[0].bar(x, accuracy_means, yerr=accuracy_stds, capsize=5, alpha=0.7)\n",
        "        axes[0].set_xlabel('Models')\n",
        "        axes[0].set_ylabel('Accuracy')\n",
        "        axes[0].set_title('Cross-Validation Accuracy Comparison')\n",
        "        axes[0].set_xticks(x)\n",
        "        axes[0].set_xticklabels(model_names, rotation=45)\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # F1-score comparison\n",
        "        axes[1].bar(x, f1_means, yerr=f1_stds, capsize=5, alpha=0.7, color='orange')\n",
        "        axes[1].set_xlabel('Models')\n",
        "        axes[1].set_ylabel('F1-Score')\n",
        "        axes[1].set_title('Cross-Validation F1-Score Comparison')\n",
        "        axes[1].set_xticks(x)\n",
        "        axes[1].set_xticklabels(model_names, rotation=45)\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return cv_results\n",
        "    \n",
        "    def statistical_significance_test(self, cv_results):\n",
        "        \"\"\"Perform statistical significance testing between models\"\"\"\n",
        "        \n",
        "        print(\"\\\\n=== Statistical Significance Testing ===\")\n",
        "        \n",
        "        model_names = list(cv_results.keys())\n",
        "        \n",
        "        # Perform pairwise t-tests\n",
        "        for i in range(len(model_names)):\n",
        "            for j in range(i+1, len(model_names)):\n",
        "                model1, model2 = model_names[i], model_names[j]\n",
        "                \n",
        "                scores1 = cv_results[model1]['accuracy_scores']\n",
        "                scores2 = cv_results[model2]['accuracy_scores']\n",
        "                \n",
        "                # Paired t-test\n",
        "                t_stat, p_value = stats.ttest_rel(scores1, scores2)\n",
        "                \n",
        "                print(f\"\\\\n{model1} vs {model2}:\")\n",
        "                print(f\"  Mean difference: {scores1.mean() - scores2.mean():.4f}\")\n",
        "                print(f\"  t-statistic: {t_stat:.4f}\")\n",
        "                print(f\"  p-value: {p_value:.4f}\")\n",
        "                print(f\"  Significant (p < 0.05): {'Yes' if p_value < 0.05 else 'No'}\")\n",
        "    \n",
        "    def generate_comprehensive_report(self):\n",
        "        \"\"\"Generate a comprehensive evaluation report\"\"\"\n",
        "        \n",
        "        print(\"\\\\n\" + \"=\"*60)\n",
        "        print(\"COMPREHENSIVE MODEL EVALUATION REPORT\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        # Create summary dataframe\n",
        "        summary_data = []\n",
        "        for model_name, results in self.results.items():\n",
        "            summary_data.append({\n",
        "                'Model': model_name,\n",
        "                'Accuracy': f\"{results['accuracy']:.4f}\",\n",
        "                'Precision': f\"{results['precision']:.4f}\",\n",
        "                'Recall': f\"{results['recall']:.4f}\",\n",
        "                'F1-Score': f\"{results['f1_score']:.4f}\",\n",
        "                'AUC': f\"{results['auc_score']:.4f}\" if not np.isnan(results['auc_score']) else 'N/A',\n",
        "                'Top-3 Accuracy': f\"{results['top3_accuracy']:.4f}\"\n",
        "            })\n",
        "        \n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        print(summary_df.to_string(index=False))\n",
        "        \n",
        "        # Best performing model\n",
        "        best_model = max(self.results.items(), key=lambda x: x[1]['accuracy'])\n",
        "        print(f\"\\\\nBest Performing Model: {best_model[0]} (Accuracy: {best_model[1]['accuracy']:.4f})\")\n",
        "        \n",
        "        return summary_df\n",
        "\n",
        "# Demonstrate evaluation framework\n",
        "print(\"=== Image Classification Model Evaluation Framework ===\")\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = ImageClassificationEvaluator()\n",
        "\n",
        "# Prepare data (balanced)\n",
        "print(\"\\\\n1. Preparing balanced dataset...\")\n",
        "X_train, X_test, y_train, y_test = evaluator.prepare_data(imbalanced=False)\n",
        "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "\n",
        "# Train models\n",
        "print(\"\\\\n2. Training models...\")\n",
        "models = evaluator.train_models(X_train, y_train)\n",
        "\n",
        "# Evaluate each model\n",
        "print(\"\\\\n3. Evaluating models...\")\n",
        "for name, model in models.items():\n",
        "    print(f\"\\\\nEvaluating {name}...\")\n",
        "    evaluator.evaluate_single_model(model, name, X_test, y_test)\n",
        "\n",
        "# Generate visualizations\n",
        "print(\"\\\\n4. Generating evaluation visualizations...\")\n",
        "evaluator.plot_confusion_matrices()\n",
        "evaluator.plot_roc_curves(X_test, y_test)\n",
        "\n",
        "# Cross-validation analysis\n",
        "print(\"\\\\n5. Cross-validation analysis...\")\n",
        "X_full = np.vstack([X_train, X_test])\n",
        "y_full = np.concatenate([y_train, y_test])\n",
        "cv_results = evaluator.cross_validation_analysis(X_full, y_full)\n",
        "\n",
        "# Statistical significance testing\n",
        "evaluator.statistical_significance_test(cv_results)\n",
        "\n",
        "# Comprehensive report\n",
        "print(\"\\\\n6. Generating comprehensive report...\")\n",
        "summary_report = evaluator.generate_comprehensive_report()\n",
        "\n",
        "print(\"\\\\nEvaluation framework demonstration completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Key Insights and Learning Reflections\n",
        "\n",
        "### Image Classification Evaluation Best Practices:\n",
        "\n",
        "1. **Metric Selection Strategy**:\n",
        "   - **Balanced Datasets**: Accuracy is sufficient\n",
        "   - **Imbalanced Datasets**: Focus on F1-score, precision, recall per class\n",
        "   - **Multi-label Problems**: Use mAP (mean Average Precision)\n",
        "   - **Real-time Applications**: Consider inference speed alongside accuracy\n",
        "\n",
        "2. **Cross-Validation Considerations**:\n",
        "   - **Stratified K-Fold**: Maintain class distribution across folds\n",
        "   - **Time-Series Data**: Use time-based splits, not random\n",
        "   - **Small Datasets**: Leave-one-out or repeated cross-validation\n",
        "   - **Large Datasets**: Simple train/validation/test splits\n",
        "\n",
        "3. **Statistical Significance**:\n",
        "   - **Paired t-tests**: Compare models on same data splits\n",
        "   - **McNemar's test**: For comparing classifier performance\n",
        "   - **Bootstrap confidence intervals**: For robust uncertainty estimates\n",
        "   - **Effect size**: Not just p-values, but practical significance\n",
        "\n",
        "4. **Class Imbalance Handling**:\n",
        "   - **Sampling Techniques**: Over/under-sampling, SMOTE\n",
        "   - **Cost-sensitive Learning**: Weighted loss functions\n",
        "   - **Threshold Tuning**: Optimize decision thresholds per class\n",
        "   - **Ensemble Methods**: Combine models with different strengths\n",
        "\n",
        "### Practical Recommendations:\n",
        "\n",
        "1. **Always visualize confusion matrices** to understand error patterns\n",
        "2. **Use multiple metrics** - no single metric tells the complete story  \n",
        "3. **Perform cross-validation** to assess model stability\n",
        "4. **Test statistical significance** when comparing models\n",
        "5. **Consider domain-specific requirements** in metric selection\n",
        "6. **Monitor both training and validation metrics** to detect overfitting\n",
        "\n",
        "### Key Takeaways:\n",
        "- Comprehensive evaluation requires multiple perspectives and metrics\n",
        "- Statistical rigor is essential for valid model comparisons\n",
        "- Visual analysis of results provides intuitive understanding\n",
        "- Real-world constraints (speed, memory) must be considered alongside accuracy\n",
        "- Class imbalance requires specialized handling and evaluation approaches\n",
        "\n",
        "### Week 2 Thursday Completion:\n",
        "Today we've explored advanced manifold learning with UMAP, transfer learning techniques, data augmentation strategies, and comprehensive model evaluation for images. These techniques form the foundation for building robust computer vision systems!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
