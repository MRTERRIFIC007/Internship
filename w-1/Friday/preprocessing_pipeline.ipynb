{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Preprocessing Pipeline for Credit Card Fraud Detection\n",
        "\n",
        "This notebook implements a complete preprocessing pipeline for the credit card fraud detection project:\n",
        "- Data cleaning and normalization\n",
        "- Feature engineering\n",
        "- Handling class imbalance\n",
        "- Preprocessing transformations and pipeline construction\n",
        "\n",
        "The goal is to create a robust, reusable preprocessing workflow that prepares our data for model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# For visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('viridis')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Loading and Exploring the Dataset\n",
        "\n",
        "For this project, we'll use the Credit Card Fraud Detection dataset. Since we're working with a simulation, we'll create a sample dataset similar to the real one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sample dataset for demonstration (similar to the Credit Card Fraud Detection dataset)\n",
        "np.random.seed(42)\n",
        "n_samples = 10000\n",
        "n_features = 30\n",
        "\n",
        "# Create feature columns (V1-V28 plus Time and Amount)\n",
        "cols = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
        "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
        "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
        "\n",
        "# Generate feature data\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "\n",
        "# Generate target variable (fraud=1, normal=0) with imbalance (0.2% fraud)\n",
        "fraud_ratio = 0.002\n",
        "n_fraud = int(n_samples * fraud_ratio)\n",
        "y = np.zeros(n_samples)\n",
        "fraud_indices = np.random.choice(range(n_samples), size=n_fraud, replace=False)\n",
        "y[fraud_indices] = 1\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(X, columns=cols)\n",
        "df['Class'] = y\n",
        "\n",
        "# Make Time and Amount more realistic\n",
        "df['Time'] = np.random.uniform(0, 172800, n_samples)  # Time in seconds (2 days)\n",
        "df['Amount'] = np.exp(np.random.normal(3, 1, n_samples))  # Log-normal distribution for amounts\n",
        "\n",
        "# Display info about the dataset\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Number of fraudulent transactions: {df['Class'].sum()}\")\n",
        "print(f\"Fraud percentage: {df['Class'].mean() * 100:.3f}%\")\n",
        "\n",
        "# Display first few rows\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check basic statistics and missing values\n",
        "print(\"Basic statistics for Amount:\")\n",
        "print(df['Amount'].describe())\n",
        "\n",
        "print(\"\\nBasic statistics for Time:\")\n",
        "print(df['Time'].describe())\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(missing_values[missing_values > 0] if missing_values.sum() > 0 else \"No missing values found\")\n",
        "\n",
        "# Create some artificial missing values for demonstration purposes\n",
        "df.loc[np.random.choice(df.index, 50), 'V1'] = np.nan\n",
        "df.loc[np.random.choice(df.index, 30), 'V2'] = np.nan\n",
        "df.loc[np.random.choice(df.index, 20), 'Amount'] = np.nan\n",
        "\n",
        "print(\"\\nAfter introducing artificial missing values:\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values[missing_values > 0])\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Cleaning and Preprocessing\n",
        "\n",
        "Let's create a preprocessing pipeline that will:\n",
        "1. Handle missing values\n",
        "2. Scale numerical features\n",
        "3. Create additional features\n",
        "4. Handle class imbalance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for feature engineering\n",
        "def add_features(df):\n",
        "    \"\"\"Add engineered features to the dataframe.\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Create time-based features\n",
        "    df['Hour'] = df['Time'] // 3600  # Convert seconds to hours\n",
        "    df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)  # Cyclical encoding\n",
        "    df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)  # Cyclical encoding\n",
        "    \n",
        "    # Log transform for Amount (common for financial data)\n",
        "    df['Amount_log'] = np.log1p(df['Amount'])\n",
        "    \n",
        "    # Create interaction features between selected V features\n",
        "    df['V1_V2'] = df['V1'] * df['V2']\n",
        "    df['V1_V3'] = df['V1'] * df['V3']\n",
        "    df['V2_V3'] = df['V2'] * df['V3']\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Demonstrate feature engineering on a small sample\n",
        "sample_df = add_features(df.head(5))\n",
        "print(\"Original features:\")\n",
        "print(df.head(5)[['Time', 'Amount', 'V1', 'V2', 'V3']].to_string())\n",
        "print(\"\\nWith engineered features:\")\n",
        "print(sample_df[['Time', 'Hour', 'Hour_sin', 'Hour_cos', 'Amount', 'Amount_log', 'V1_V2', 'V1_V3', 'V2_V3']].to_string())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data before preprocessing\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples, {y_train.sum()} frauds ({y_train.mean() * 100:.3f}%)\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples, {y_test.sum()} frauds ({y_test.mean() * 100:.3f}%)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Building the Preprocessing Pipeline\n",
        "\n",
        "Now, let's build a complete preprocessing pipeline using scikit-learn's Pipeline and ColumnTransformer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define column types\n",
        "amount_columns = ['Amount']\n",
        "time_columns = ['Time']\n",
        "v_columns = [col for col in X_train.columns if col.startswith('V')]\n",
        "\n",
        "# 1. Feature Engineering Step (outside the main pipeline)\n",
        "X_train_feat = add_features(X_train)\n",
        "X_test_feat = add_features(X_test)\n",
        "\n",
        "# Get lists of the new feature columns\n",
        "hour_columns = ['Hour', 'Hour_sin', 'Hour_cos']\n",
        "amount_derived_columns = ['Amount_log']\n",
        "interaction_columns = ['V1_V2', 'V1_V3', 'V2_V3']\n",
        "\n",
        "# Define the preprocessing steps for different column types\n",
        "amount_processor = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', RobustScaler())\n",
        "])\n",
        "\n",
        "time_processor = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "v_processor = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "hour_processor = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Combine all preprocessing steps using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('amount', amount_processor, amount_columns + amount_derived_columns),\n",
        "        ('time', time_processor, time_columns),\n",
        "        ('v_features', v_processor, v_columns + interaction_columns),\n",
        "        ('hour', hour_processor, hour_columns)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply preprocessing to the training and test sets\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train_feat)\n",
        "X_test_preprocessed = preprocessor.transform(X_test_feat)\n",
        "\n",
        "print(f\"Preprocessed training data shape: {X_train_preprocessed.shape}\")\n",
        "print(f\"Preprocessed test data shape: {X_test_preprocessed.shape}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Handling Class Imbalance with SMOTE\n",
        "\n",
        "For our highly imbalanced dataset, we'll use the Synthetic Minority Over-sampling Technique (SMOTE) to generate synthetic samples of the minority class (fraud transactions).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply SMOTE to balance the training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_preprocessed, y_train)\n",
        "\n",
        "# Check the new class distribution\n",
        "print(\"Class distribution before SMOTE:\")\n",
        "print(f\"Class 0 (Normal): {(y_train == 0).sum()} ({(1 - y_train.mean()) * 100:.2f}%)\")\n",
        "print(f\"Class 1 (Fraud): {(y_train == 1).sum()} ({y_train.mean() * 100:.2f}%)\")\n",
        "\n",
        "print(\"\\nClass distribution after SMOTE:\")\n",
        "print(f\"Class 0 (Normal): {(y_train_resampled == 0).sum()} ({(1 - y_train_resampled.mean()) * 100:.2f}%)\")\n",
        "print(f\"Class 1 (Fraud): {(y_train_resampled == 1).sum()} ({y_train_resampled.mean() * 100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nResampled training data shape: {X_train_resampled.shape}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Creating a Complete End-to-End Pipeline\n",
        "\n",
        "Let's now create a complete pipeline that combines all the preprocessing steps and includes SMOTE for handling class imbalance. This will make it easy to reuse our preprocessing workflow in the model training process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a custom transformer for feature engineering\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "        \n",
        "        # Create time-based features\n",
        "        X_copy['Hour'] = X_copy['Time'] // 3600  # Convert seconds to hours\n",
        "        X_copy['Hour_sin'] = np.sin(2 * np.pi * X_copy['Hour'] / 24)  # Cyclical encoding\n",
        "        X_copy['Hour_cos'] = np.cos(2 * np.pi * X_copy['Hour'] / 24)  # Cyclical encoding\n",
        "        \n",
        "        # Log transform for Amount (common for financial data)\n",
        "        X_copy['Amount_log'] = np.log1p(X_copy['Amount'])\n",
        "        \n",
        "        # Create interaction features between selected V features\n",
        "        X_copy['V1_V2'] = X_copy['V1'] * X_copy['V2']\n",
        "        X_copy['V1_V3'] = X_copy['V1'] * X_copy['V3']\n",
        "        X_copy['V2_V3'] = X_copy['V2'] * X_copy['V3']\n",
        "        \n",
        "        return X_copy\n",
        "\n",
        "# Now we can create a full pipeline including feature engineering\n",
        "# Reset and redefine the column lists\n",
        "amount_columns = ['Amount']\n",
        "time_columns = ['Time']\n",
        "v_columns = [col for col in X.columns if col.startswith('V')]\n",
        "hour_columns = ['Hour', 'Hour_sin', 'Hour_cos']\n",
        "amount_derived_columns = ['Amount_log']\n",
        "interaction_columns = ['V1_V2', 'V1_V3', 'V2_V3']\n",
        "\n",
        "# Complete preprocessing pipeline\n",
        "full_preprocessor = Pipeline(steps=[\n",
        "    ('feature_engineer', FeatureEngineer()),\n",
        "    ('column_transformer', ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('amount', Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='median')),\n",
        "                ('scaler', RobustScaler())\n",
        "            ]), ['Amount', 'Amount_log']),\n",
        "            \n",
        "            ('time', Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='median')),\n",
        "                ('scaler', StandardScaler())\n",
        "            ]), ['Time']),\n",
        "            \n",
        "            ('hour', Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='median')),\n",
        "                ('scaler', StandardScaler())\n",
        "            ]), ['Hour', 'Hour_sin', 'Hour_cos']),\n",
        "            \n",
        "            ('v_features', Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='median')),\n",
        "                ('scaler', StandardScaler())\n",
        "            ]), v_columns + ['V1_V2', 'V1_V3', 'V2_V3'])\n",
        "        ]\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Apply the full preprocessing pipeline\n",
        "X_train_full_processed = full_preprocessor.fit_transform(X_train)\n",
        "X_test_full_processed = full_preprocessor.transform(X_test)\n",
        "\n",
        "print(f\"Fully processed training data shape: {X_train_full_processed.shape}\")\n",
        "print(f\"Fully processed test data shape: {X_test_full_processed.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete preprocessing pipeline with SMOTE\n",
        "# We'll use imbalanced-learn's Pipeline which supports SMOTE\n",
        "imb_pipeline = ImbPipeline(steps=[\n",
        "    ('preprocessor', full_preprocessor),\n",
        "    ('smote', SMOTE(random_state=42))\n",
        "])\n",
        "\n",
        "# Apply the imbalanced-learn pipeline (this will apply both preprocessing and SMOTE)\n",
        "X_train_processed_balanced, y_train_balanced = imb_pipeline.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"Final preprocessed and balanced data:\")\n",
        "print(f\"Training data shape: {X_train_processed_balanced.shape}\")\n",
        "print(f\"Class distribution in balanced training set:\")\n",
        "print(f\"Class 0 (Normal): {(y_train_balanced == 0).sum()} ({(1 - y_train_balanced.mean()) * 100:.2f}%)\")\n",
        "print(f\"Class 1 (Fraud): {(y_train_balanced == 1).sum()} ({y_train_balanced.mean() * 100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Pipeline Visualization and Summary\n",
        "\n",
        "Let's visualize our complete preprocessing workflow and summarize what we've learned.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple visualization of our pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle, FancyArrowPatch\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Helper function to add an arrow\n",
        "def add_arrow(ax, x1, y1, x2, y2, width=0.03):\n",
        "    arrow = FancyArrowPatch((x1, y1), (x2, y2), \n",
        "                           arrowstyle='->', \n",
        "                           color='black', \n",
        "                           linewidth=1, \n",
        "                           mutation_scale=20)\n",
        "    ax.add_patch(arrow)\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.set_xlim(0, 10)\n",
        "ax.set_ylim(0, 7)\n",
        "ax.axis('off')\n",
        "\n",
        "# Draw Raw Data\n",
        "rect = Rectangle((0.5, 5.5), 2, 1, facecolor='lightblue', edgecolor='black')\n",
        "ax.add_patch(rect)\n",
        "ax.text(1.5, 6, 'Raw Data', ha='center', va='center', fontsize=12)\n",
        "\n",
        "# Draw Feature Engineering\n",
        "rect = Rectangle((3.5, 5.5), 2, 1, facecolor='lightgreen', edgecolor='black')\n",
        "ax.add_patch(rect)\n",
        "ax.text(4.5, 6, 'Feature Engineering', ha='center', va='center', fontsize=12)\n",
        "\n",
        "# Draw Missing Value Imputation\n",
        "rect = Rectangle((6.5, 5.5), 2, 1, facecolor='lightcoral', edgecolor='black')\n",
        "ax.add_patch(rect)\n",
        "ax.text(7.5, 6, 'Missing Value\\nImputation', ha='center', va='center', fontsize=12)\n",
        "\n",
        "# Draw Scaling\n",
        "rect = Rectangle((3.5, 3.5), 2, 1, facecolor='lightyellow', edgecolor='black')\n",
        "ax.add_patch(rect)\n",
        "ax.text(4.5, 4, 'Feature Scaling', ha='center', va='center', fontsize=12)\n",
        "\n",
        "# Draw SMOTE\n",
        "rect = Rectangle((6.5, 3.5), 2, 1, facecolor='lavender', edgecolor='black')\n",
        "ax.add_patch(rect)\n",
        "ax.text(7.5, 4, 'SMOTE\\nResampling', ha='center', va='center', fontsize=12)\n",
        "\n",
        "# Draw Processed Data\n",
        "rect = Rectangle((4.5, 1.5), 2, 1, facecolor='lightgreen', edgecolor='black')\n",
        "ax.add_patch(rect)\n",
        "ax.text(5.5, 2, 'Processed Data\\nReady for Models', ha='center', va='center', fontsize=12)\n",
        "\n",
        "# Add arrows\n",
        "add_arrow(ax, 2.5, 6, 3.5, 6)  # Raw to Feature Engineering\n",
        "add_arrow(ax, 5.5, 6, 6.5, 6)  # Feature Engineering to Missing Value Imputation\n",
        "add_arrow(ax, 7.5, 5.5, 7.5, 4.5)  # Missing Value Imputation to SMOTE\n",
        "add_arrow(ax, 6.5, 4, 5.5, 4)  # SMOTE to Scaling\n",
        "add_arrow(ax, 4.5, 3.5, 4.5, 2.5)  # Scaling to Processed Data\n",
        "add_arrow(ax, 6.5, 3.5, 5.5, 2.5)  # SMOTE to Processed Data\n",
        "\n",
        "plt.title('Credit Card Fraud Detection Preprocessing Pipeline', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Summary and Next Steps\n",
        "\n",
        "### Key Preprocessing Steps Implemented:\n",
        "\n",
        "1. **Feature Engineering**:\n",
        "   - Created time-based features using cyclical encoding (Hour_sin, Hour_cos)\n",
        "   - Applied log transformation to Amount\n",
        "   - Generated interaction features between selected V features\n",
        "\n",
        "2. **Data Cleaning**:\n",
        "   - Identified and handled missing values using median imputation\n",
        "   - Scaled numerical features using StandardScaler and RobustScaler\n",
        "   - Organized different feature types with ColumnTransformer\n",
        "\n",
        "3. **Class Imbalance Handling**:\n",
        "   - Applied SMOTE to generate synthetic samples of the minority class\n",
        "   - Achieved balanced class distribution for model training\n",
        "\n",
        "4. **Pipeline Construction**:\n",
        "   - Created reusable, end-to-end preprocessing workflow\n",
        "   - Combined feature engineering, imputation, scaling, and resampling in a single pipeline\n",
        "   - Integrated with imbalanced-learn for SMOTE\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. Use the preprocessed data to train and evaluate various classification models\n",
        "2. Implement model comparison to identify the best performing algorithm\n",
        "3. Apply advanced optimization techniques to improve model performance\n",
        "4. Create a production-ready deployment pipeline\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
