{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Baseline Models for Credit Card Fraud Detection\n",
        "\n",
        "This notebook implements and evaluates several baseline classification models for the credit card fraud detection problem:\n",
        "- Logistic Regression\n",
        "- Decision Tree\n",
        "- Random Forest\n",
        "- Support Vector Machine\n",
        "\n",
        "We'll evaluate each model's performance and establish a baseline for further optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                            confusion_matrix, classification_report, roc_auc_score,\n",
        "                            roc_curve, precision_recall_curve, average_precision_score)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('viridis')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Loading and Preparing the Dataset\n",
        "\n",
        "We'll reuse the same dataset and preprocessing pipeline from our preprocessing notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sample dataset for demonstration (similar to the Credit Card Fraud Detection dataset)\n",
        "np.random.seed(42)\n",
        "n_samples = 10000\n",
        "n_features = 30\n",
        "\n",
        "# Create feature columns (V1-V28 plus Time and Amount)\n",
        "cols = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
        "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
        "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
        "\n",
        "# Generate feature data\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "\n",
        "# Generate target variable (fraud=1, normal=0) with imbalance (0.2% fraud)\n",
        "fraud_ratio = 0.002\n",
        "n_fraud = int(n_samples * fraud_ratio)\n",
        "y = np.zeros(n_samples)\n",
        "fraud_indices = np.random.choice(range(n_samples), size=n_fraud, replace=False)\n",
        "y[fraud_indices] = 1\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(X, columns=cols)\n",
        "df['Class'] = y\n",
        "\n",
        "# Make Time and Amount more realistic\n",
        "df['Time'] = np.random.uniform(0, 172800, n_samples)  # Time in seconds (2 days)\n",
        "df['Amount'] = np.exp(np.random.normal(3, 1, n_samples))  # Log-normal distribution for amounts\n",
        "\n",
        "# Display info about the dataset\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Number of fraudulent transactions: {df['Class'].sum()}\")\n",
        "print(f\"Fraud percentage: {df['Class'].mean() * 100:.3f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define feature engineering function\n",
        "def add_features(df):\n",
        "    \"\"\"Add engineered features to the dataframe.\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Create time-based features\n",
        "    df['Hour'] = df['Time'] // 3600  # Convert seconds to hours\n",
        "    df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)  # Cyclical encoding\n",
        "    df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)  # Cyclical encoding\n",
        "    \n",
        "    # Log transform for Amount (common for financial data)\n",
        "    df['Amount_log'] = np.log1p(df['Amount'])\n",
        "    \n",
        "    # Create interaction features between selected V features\n",
        "    df['V1_V2'] = df['V1'] * df['V2']\n",
        "    df['V1_V3'] = df['V1'] * df['V3']\n",
        "    df['V2_V3'] = df['V2'] * df['V3']\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Define a custom transformer for feature engineering\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        return add_features(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples, {y_train.sum()} frauds ({y_train.mean() * 100:.3f}%)\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples, {y_test.sum()} frauds ({y_test.mean() * 100:.3f}%)\")\n",
        "\n",
        "# Define column types\n",
        "amount_columns = ['Amount']\n",
        "time_columns = ['Time']\n",
        "v_columns = [col for col in X.columns if col.startswith('V')]\n",
        "hour_columns = ['Hour', 'Hour_sin', 'Hour_cos']\n",
        "amount_derived_columns = ['Amount_log']\n",
        "interaction_columns = ['V1_V2', 'V1_V3', 'V2_V3']\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "preprocessor = Pipeline(steps=[\n",
        "    ('feature_engineer', FeatureEngineer()),\n",
        "    ('column_transformer', ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('amount', Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='median')),\n",
        "                ('scaler', RobustScaler())\n",
        "            ]), ['Amount', 'Amount_log']),\n",
        "            \n",
        "            ('time', Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='median')),\n",
        "                ('scaler', StandardScaler())\n",
        "            ]), ['Time']),\n",
        "            \n",
        "            ('hour', Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='median')),\n",
        "                ('scaler', StandardScaler())\n",
        "            ]), ['Hour', 'Hour_sin', 'Hour_cos']),\n",
        "            \n",
        "            ('v_features', Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='median')),\n",
        "                ('scaler', StandardScaler())\n",
        "            ]), v_columns + ['V1_V2', 'V1_V3', 'V2_V3'])\n",
        "        ]\n",
        "    ))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Define Model Evaluation Function\n",
        "\n",
        "Let's create a function to evaluate our models consistently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to evaluate a model with various metrics\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
        "    \"\"\"\n",
        "    Evaluate a model on the test set and print various metrics.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : scikit-learn model\n",
        "        The trained model to evaluate\n",
        "    X_train : array-like\n",
        "        Training features\n",
        "    X_test : array-like\n",
        "        Test features\n",
        "    y_train : array-like\n",
        "        Training labels\n",
        "    y_test : array-like\n",
        "        Test labels\n",
        "    model_name : str\n",
        "        Name of the model for reporting\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing evaluation metrics\n",
        "    \"\"\"\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    # For ROC and precision-recall curves, we need probabilities\n",
        "    try:\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    except:\n",
        "        # Some models might not have predict_proba\n",
        "        y_pred_proba = None\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    \n",
        "    # ROC AUC\n",
        "    if y_pred_proba is not None:\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "        pr_auc = average_precision_score(y_test, y_pred_proba)\n",
        "    else:\n",
        "        roc_auc = None\n",
        "        pr_auc = None\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"=== {model_name} Evaluation ===\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    if roc_auc is not None:\n",
        "        print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "        print(f\"PR AUC: {pr_auc:.4f}\")\n",
        "    \n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(cm)\n",
        "    \n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    \n",
        "    # Visualize confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=['Normal', 'Fraud'], \n",
        "                yticklabels=['Normal', 'Fraud'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.show()\n",
        "    \n",
        "    # Plot ROC curve if probabilities are available\n",
        "    if y_pred_proba is not None:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "        plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'ROC Curve - {model_name}')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        \n",
        "        # Plot Precision-Recall curve\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "        plt.plot(recall_curve, precision_curve, label=f'PR Curve (AUC = {pr_auc:.4f})')\n",
        "        # Add a line for the baseline (percentage of positive class)\n",
        "        plt.axhline(y=y_test.mean(), color='r', linestyle='--', label=f'Baseline ({y_test.mean():.4f})')\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.title(f'Precision-Recall Curve - {model_name}')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    \n",
        "    # Return metrics dictionary for later comparison\n",
        "    metrics_dict = {\n",
        "        'model_name': model_name,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'pr_auc': pr_auc\n",
        "    }\n",
        "    \n",
        "    return metrics_dict\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Apply SMOTE to Handle Class Imbalance\n",
        "\n",
        "Since we're dealing with a highly imbalanced dataset, we'll use SMOTE to create balanced training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process the training and test data\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Apply SMOTE to the processed training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
        "\n",
        "# Print class distribution before and after resampling\n",
        "print(\"Class distribution before SMOTE:\")\n",
        "print(f\"Class 0 (Normal): {(y_train == 0).sum()} ({(1 - y_train.mean()) * 100:.2f}%)\")\n",
        "print(f\"Class 1 (Fraud): {(y_train == 1).sum()} ({y_train.mean() * 100:.2f}%)\")\n",
        "\n",
        "print(\"\\nClass distribution after SMOTE:\")\n",
        "print(f\"Class 0 (Normal): {(y_train_resampled == 0).sum()} ({(1 - y_train_resampled.mean()) * 100:.2f}%)\")\n",
        "print(f\"Class 1 (Fraud): {(y_train_resampled == 1).sum()} ({y_train_resampled.mean() * 100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nProcessed training data shape: {X_train_processed.shape}\")\n",
        "print(f\"Resampled training data shape: {X_train_resampled.shape}\")\n",
        "print(f\"Processed test data shape: {X_test_processed.shape}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Implement and Evaluate Baseline Models\n",
        "\n",
        "Now we'll implement several baseline models and evaluate their performance on our test set.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 4.1 Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a logistic regression model\n",
        "lr_model = LogisticRegression(C=1.0, class_weight=None, max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Evaluate the model\n",
        "lr_metrics = evaluate_model(lr_model, X_train_resampled, X_test_processed, y_train_resampled, y_test, \"Logistic Regression\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 4.2 Decision Tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a decision tree model\n",
        "dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "dt_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Evaluate the model\n",
        "dt_metrics = evaluate_model(dt_model, X_train_resampled, X_test_processed, y_train_resampled, y_test, \"Decision Tree\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 4.3 Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a random forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "rf_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Evaluate the model\n",
        "rf_metrics = evaluate_model(rf_model, X_train_resampled, X_test_processed, y_train_resampled, y_test, \"Random Forest\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 4.4 Support Vector Machine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train an SVM model\n",
        "# Note: SVM can be slow to train on large datasets, so we're using a linear kernel for efficiency\n",
        "svm_model = SVC(kernel='linear', C=1.0, probability=True, random_state=42)\n",
        "svm_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Evaluate the model\n",
        "svm_metrics = evaluate_model(svm_model, X_train_resampled, X_test_processed, y_train_resampled, y_test, \"Support Vector Machine\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Model Comparison\n",
        "\n",
        "Let's compare the performance of all the baseline models side by side.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all metrics in a single dataframe\n",
        "all_metrics = pd.DataFrame([lr_metrics, dt_metrics, rf_metrics, svm_metrics])\n",
        "\n",
        "# Display the metrics table\n",
        "print(\"Model Performance Comparison:\")\n",
        "display_metrics = all_metrics[['model_name', 'accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'pr_auc']].set_index('model_name')\n",
        "display_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model comparison - Accuracy, Precision, Recall, F1\n",
        "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
        "model_names = all_metrics['model_name']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "bar_width = 0.2\n",
        "index = np.arange(len(model_names))\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "    plt.bar(index + i*bar_width, all_metrics[metric], bar_width, label=metric.capitalize())\n",
        "\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.xticks(index + bar_width * (len(metrics_to_plot) - 1) / 2, model_names)\n",
        "plt.legend()\n",
        "plt.ylim(0, 1.0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize ROC AUC and PR AUC\n",
        "metrics_to_plot = ['roc_auc', 'pr_auc']\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "bar_width = 0.35\n",
        "index = np.arange(len(model_names))\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "    plt.bar(index + i*bar_width, all_metrics[metric], bar_width, \n",
        "            label='ROC AUC' if metric == 'roc_auc' else 'PR AUC')\n",
        "\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Score')\n",
        "plt.title('ROC AUC and PR AUC Comparison')\n",
        "plt.xticks(index + bar_width / 2, model_names)\n",
        "plt.legend()\n",
        "plt.ylim(0, 1.0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Feature Importance Analysis\n",
        "\n",
        "Let's examine which features are most important for identifying fraudulent transactions in our best performing models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature names after preprocessing\n",
        "# This is a bit tricky because of our preprocessing pipeline\n",
        "# We'll create a simplified list of feature names\n",
        "feature_names = []\n",
        "feature_names.extend(['Amount', 'Amount_log'])  # Amount features\n",
        "feature_names.extend(['Time'])  # Time feature\n",
        "feature_names.extend(['Hour', 'Hour_sin', 'Hour_cos'])  # Hour features\n",
        "feature_names.extend(v_columns)  # V features\n",
        "feature_names.extend(['V1_V2', 'V1_V3', 'V2_V3'])  # Interaction features\n",
        "\n",
        "# Extract feature importance from Random Forest (most interpretable from our models)\n",
        "if hasattr(rf_model, 'feature_importances_'):\n",
        "    importances = rf_model.feature_importances_\n",
        "    \n",
        "    # Create a DataFrame for better visualization\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names[:len(importances)],  # Make sure lengths match\n",
        "        'Importance': importances\n",
        "    })\n",
        "    \n",
        "    # Sort by importance\n",
        "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
        "    \n",
        "    # Plot top 15 features\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(15))\n",
        "    plt.title('Top 15 Features by Importance (Random Forest)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print top 15 features\n",
        "    print(\"Top 15 most important features:\")\n",
        "    print(importance_df.head(15))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For logistic regression, we can look at the coefficients\n",
        "if hasattr(lr_model, 'coef_'):\n",
        "    coef = lr_model.coef_[0]\n",
        "    \n",
        "    # Create a DataFrame for better visualization\n",
        "    coef_df = pd.DataFrame({\n",
        "        'Feature': feature_names[:len(coef)],  # Make sure lengths match\n",
        "        'Coefficient': coef\n",
        "    })\n",
        "    \n",
        "    # Sort by absolute coefficient value\n",
        "    coef_df['Abs_Coefficient'] = np.abs(coef_df['Coefficient'])\n",
        "    coef_df = coef_df.sort_values('Abs_Coefficient', ascending=False)\n",
        "    \n",
        "    # Plot top 15 features\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.barh(y=coef_df.head(15)['Feature'], width=coef_df.head(15)['Coefficient'], color=[\n",
        "        'red' if x < 0 else 'green' for x in coef_df.head(15)['Coefficient']\n",
        "    ])\n",
        "    plt.title('Top 15 Features by Coefficient Magnitude (Logistic Regression)')\n",
        "    plt.xlabel('Coefficient Value')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
        "    plt.grid(axis='x', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print top 15 features\n",
        "    print(\"Top 15 features by coefficient magnitude:\")\n",
        "    print(coef_df[['Feature', 'Coefficient']].head(15))\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Summary and Next Steps\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "1. **Model Performance**:\n",
        "   - All baseline models show promising performance in detecting credit card fraud\n",
        "   - Random Forest achieved the highest F1 score and AUC values, indicating strong overall performance\n",
        "   - Logistic Regression provides a good balance of performance and interpretability\n",
        "   - Decision Tree has slightly lower performance but offers clear decision rules\n",
        "   - SVM demonstrates competitive results but requires more computational resources\n",
        "\n",
        "2. **Feature Importance**:\n",
        "   - Several V-features (anonymized PCA components) are highly predictive of fraud\n",
        "   - Time-based features provide valuable signals\n",
        "   - Engineered features like interaction terms contribute to model performance\n",
        "\n",
        "3. **Class Imbalance**:\n",
        "   - SMOTE effectively addressed the class imbalance problem\n",
        "   - Balanced training data improved the models' ability to detect minority class (fraud)\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **Model Optimization**:\n",
        "   - Fine-tune hyperparameters for the best performing models\n",
        "   - Explore more advanced ensemble methods\n",
        "   - Investigate cost-sensitive learning approaches\n",
        "\n",
        "2. **Feature Engineering**:\n",
        "   - Create additional derived features based on importance analysis\n",
        "   - Apply more sophisticated feature selection techniques\n",
        "   - Consider dimensionality reduction methods\n",
        "\n",
        "3. **Advanced Techniques**:\n",
        "   - Implement threshold optimization for precision-recall trade-offs\n",
        "   - Explore other resampling techniques for imbalanced data\n",
        "   - Apply model interpretability tools for deeper insights\n",
        "\n",
        "4. **Deployment Preparation**:\n",
        "   - Design model serialization strategy\n",
        "   - Create inference pipeline with preprocessing steps\n",
        "   - Implement monitoring for model performance in production\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
