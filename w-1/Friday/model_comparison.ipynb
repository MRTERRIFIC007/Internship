{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Model Comparison and Selection\n",
        "\n",
        "This notebook systematically compares different machine learning models for credit card fraud detection:\n",
        "- Detailed performance comparison using multiple metrics\n",
        "- Statistical testing for model differences\n",
        "- Ensemble methods and voting classifiers\n",
        "- ROC and PR curve analysis\n",
        "- Final model selection with justification\n",
        "\n",
        "The goal is to identify the best performing model for our specific use case while considering various performance trade-offs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                            confusion_matrix, classification_report, roc_auc_score,\n",
        "                            roc_curve, precision_recall_curve, average_precision_score)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n",
        "                             VotingClassifier, AdaBoostClassifier)\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('viridis')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Dataset Loading and Preprocessing\n",
        "\n",
        "Let's reuse our credit card fraud dataset and preprocessing pipeline from previous notebooks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sample dataset for demonstration (similar to the Credit Card Fraud Detection dataset)\n",
        "np.random.seed(42)\n",
        "n_samples = 10000\n",
        "n_features = 30\n",
        "\n",
        "# Create feature columns (V1-V28 plus Time and Amount)\n",
        "cols = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
        "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
        "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
        "\n",
        "# Generate feature data\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "\n",
        "# Generate target variable (fraud=1, normal=0) with imbalance (0.2% fraud)\n",
        "fraud_ratio = 0.002\n",
        "n_fraud = int(n_samples * fraud_ratio)\n",
        "y = np.zeros(n_samples)\n",
        "fraud_indices = np.random.choice(range(n_samples), size=n_fraud, replace=False)\n",
        "y[fraud_indices] = 1\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(X, columns=cols)\n",
        "df['Class'] = y\n",
        "\n",
        "# Make Time and Amount more realistic\n",
        "df['Time'] = np.random.uniform(0, 172800, n_samples)  # Time in seconds (2 days)\n",
        "df['Amount'] = np.exp(np.random.normal(3, 1, n_samples))  # Log-normal distribution for amounts\n",
        "\n",
        "# Display info about the dataset\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Number of fraudulent transactions: {df['Class'].sum()}\")\n",
        "print(f\"Fraud percentage: {df['Class'].mean() * 100:.3f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define feature engineering function\n",
        "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        X_copy = X.copy()\n",
        "        \n",
        "        # Create time-based features\n",
        "        X_copy['Hour'] = X_copy['Time'] // 3600  # Convert seconds to hours\n",
        "        X_copy['Hour_sin'] = np.sin(2 * np.pi * X_copy['Hour'] / 24)  # Cyclical encoding\n",
        "        X_copy['Hour_cos'] = np.cos(2 * np.pi * X_copy['Hour'] / 24)  # Cyclical encoding\n",
        "        \n",
        "        # Log transform for Amount (common for financial data)\n",
        "        X_copy['Amount_log'] = np.log1p(X_copy['Amount'])\n",
        "        \n",
        "        # Create interaction features between selected V features\n",
        "        X_copy['V1_V2'] = X_copy['V1'] * X_copy['V2']\n",
        "        X_copy['V1_V3'] = X_copy['V1'] * X_copy['V3']\n",
        "        X_copy['V2_V3'] = X_copy['V2'] * X_copy['V3']\n",
        "        \n",
        "        return X_copy\n",
        "\n",
        "# Split the data\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples, {y_train.sum()} frauds ({y_train.mean() * 100:.3f}%)\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples, {y_test.sum()} frauds ({y_test.mean() * 100:.3f}%)\")\n",
        "\n",
        "# Define column types\n",
        "amount_columns = ['Amount']\n",
        "time_columns = ['Time']\n",
        "v_columns = [col for col in X.columns if col.startswith('V')]\n",
        "hour_columns = ['Hour', 'Hour_sin', 'Hour_cos']\n",
        "amount_derived_columns = ['Amount_log']\n",
        "interaction_columns = ['V1_V2', 'V1_V3', 'V2_V3']\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "preprocessor = Pipeline(steps=[\n",
        "    ('feature_engineer', FeatureEngineer()),\n",
        "    ('column_transformer', ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('amount', Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='median')),\n",
        "                ('scaler', RobustScaler())\n",
        "            ]), ['Amount', 'Amount_log']),\n",
        "            \n",
        "            ('time', Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='median')),\n",
        "                ('scaler', StandardScaler())\n",
        "            ]), ['Time']),\n",
        "            \n",
        "            ('hour', Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='median')),\n",
        "                ('scaler', StandardScaler())\n",
        "            ]), ['Hour', 'Hour_sin', 'Hour_cos']),\n",
        "            \n",
        "            ('v_features', Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='median')),\n",
        "                ('scaler', StandardScaler())\n",
        "            ]), v_columns + ['V1_V2', 'V1_V3', 'V2_V3'])\n",
        "        ]\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Process the training and test data\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Apply SMOTE to the processed training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
        "\n",
        "print(f\"Processed training data shape: {X_train_processed.shape}\")\n",
        "print(f\"Resampled training data shape: {X_train_resampled.shape}\")\n",
        "print(f\"Processed test data shape: {X_test_processed.shape}\")\n",
        "\n",
        "# Print class distribution after resampling\n",
        "print(\"\\nClass distribution after SMOTE:\")\n",
        "print(f\"Class 0 (Normal): {(y_train_resampled == 0).sum()} ({(1 - y_train_resampled.mean()) * 100:.2f}%)\")\n",
        "print(f\"Class 1 (Fraud): {(y_train_resampled == 1).sum()} ({y_train_resampled.mean() * 100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Define Evaluation Functions\n",
        "\n",
        "Let's create functions to consistently evaluate and compare different models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to evaluate a model with various metrics\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
        "    \"\"\"\n",
        "    Evaluate a model on the test set and return various metrics.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : scikit-learn model\n",
        "        The trained model to evaluate\n",
        "    X_train : array-like\n",
        "        Training features\n",
        "    X_test : array-like\n",
        "        Test features\n",
        "    y_train : array-like\n",
        "        Training labels\n",
        "    y_test : array-like\n",
        "        Test labels\n",
        "    model_name : str\n",
        "        Name of the model for reporting\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing evaluation metrics\n",
        "    \"\"\"\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    # For ROC and precision-recall curves, we need probabilities\n",
        "    try:\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    except:\n",
        "        # Some models might not have predict_proba\n",
        "        y_pred_proba = None\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    \n",
        "    # ROC AUC\n",
        "    if y_pred_proba is not None:\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "        pr_auc = average_precision_score(y_test, y_pred_proba)\n",
        "    else:\n",
        "        roc_auc = None\n",
        "        pr_auc = None\n",
        "    \n",
        "    # Return metrics dictionary for later comparison\n",
        "    metrics_dict = {\n",
        "        'model_name': model_name,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'pr_auc': pr_auc,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba\n",
        "    }\n",
        "    \n",
        "    return metrics_dict\n",
        "\n",
        "def display_model_metrics(metrics_dict):\n",
        "    \"\"\"\n",
        "    Display metrics and visualizations for a model evaluation.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    metrics_dict : dict\n",
        "        Dictionary containing evaluation metrics from evaluate_model\n",
        "    \"\"\"\n",
        "    model_name = metrics_dict['model_name']\n",
        "    y_pred = metrics_dict['y_pred']\n",
        "    y_pred_proba = metrics_dict['y_pred_proba']\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"=== {model_name} Evaluation ===\")\n",
        "    print(f\"Accuracy: {metrics_dict['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {metrics_dict['precision']:.4f}\")\n",
        "    print(f\"Recall: {metrics_dict['recall']:.4f}\")\n",
        "    print(f\"F1 Score: {metrics_dict['f1']:.4f}\")\n",
        "    if metrics_dict['roc_auc'] is not None:\n",
        "        print(f\"ROC AUC: {metrics_dict['roc_auc']:.4f}\")\n",
        "        print(f\"PR AUC: {metrics_dict['pr_auc']:.4f}\")\n",
        "    \n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(cm)\n",
        "    \n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    \n",
        "    # Visualize confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=['Normal', 'Fraud'], \n",
        "                yticklabels=['Normal', 'Fraud'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.show()\n",
        "    \n",
        "    # Plot ROC curve if probabilities are available\n",
        "    if y_pred_proba is not None:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {metrics_dict[\"roc_auc\"]:.4f})')\n",
        "        plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'ROC Curve - {model_name}')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        \n",
        "        # Plot Precision-Recall curve\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "        plt.plot(recall_curve, precision_curve, label=f'PR Curve (AUC = {metrics_dict[\"pr_auc\"]:.4f})')\n",
        "        # Add a line for the baseline (percentage of positive class)\n",
        "        plt.axhline(y=y_test.mean(), color='r', linestyle='--', label=f'Baseline ({y_test.mean():.4f})')\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.title(f'Precision-Recall Curve - {model_name}')\n",
        "        plt.legend()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Cross-Validation for Model Selection\n",
        "\n",
        "Before we compare models, let's use cross-validation to get a more robust evaluation of their performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define models to compare\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(C=1.0, max_iter=1000, random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),\n",
        "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
        "    'SVM (Linear)': SVC(kernel='linear', C=1.0, probability=True, random_state=42),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
        "}\n",
        "\n",
        "# Create a function for cross-validation\n",
        "def cross_validate_models(models, X, y, cv=5, scoring='f1'):\n",
        "    \"\"\"\n",
        "    Perform cross-validation on multiple models and return results.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    models : dict\n",
        "        Dictionary of model name -> model\n",
        "    X : array-like\n",
        "        Features\n",
        "    y : array-like\n",
        "        Target\n",
        "    cv : int or cross-validation generator\n",
        "        Cross-validation strategy\n",
        "    scoring : str\n",
        "        Scoring metric\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary of model name -> CV scores\n",
        "    \"\"\"\n",
        "    cv_results = {}\n",
        "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        print(f\"Cross-validating {name}...\")\n",
        "        scores = cross_val_score(model, X, y, cv=skf, scoring=scoring)\n",
        "        cv_results[name] = scores\n",
        "        print(f\"  Mean {scoring}: {scores.mean():.4f} (±{scores.std():.4f})\\n\")\n",
        "    \n",
        "    return cv_results\n",
        "\n",
        "# Perform cross-validation with F1 score\n",
        "cv_results_f1 = cross_validate_models(models, X_train_resampled, y_train_resampled, cv=5, scoring='f1')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize cross-validation results\n",
        "cv_means = {name: scores.mean() for name, scores in cv_results_f1.items()}\n",
        "cv_stds = {name: scores.std() for name, scores in cv_results_f1.items()}\n",
        "\n",
        "# Sort by mean F1 score\n",
        "cv_means_sorted = {k: v for k, v in sorted(cv_means.items(), key=lambda item: item[1], reverse=True)}\n",
        "cv_stds_sorted = {k: cv_stds[k] for k in cv_means_sorted.keys()}\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(range(len(cv_means_sorted)), list(cv_means_sorted.values()), \n",
        "        yerr=list(cv_stds_sorted.values()),\n",
        "        tick_label=list(cv_means_sorted.keys()))\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Cross-Validation Results (F1 Score)')\n",
        "plt.ylabel('Mean F1 Score')\n",
        "plt.ylim(0.7, 1.0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "cv_df = pd.DataFrame({\n",
        "    'Model': list(cv_means_sorted.keys()),\n",
        "    'Mean F1': list(cv_means_sorted.values()),\n",
        "    'Std F1': list(cv_stds_sorted.values())\n",
        "})\n",
        "\n",
        "# Display table of results\n",
        "cv_df\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Statistical Testing for Model Comparison\n",
        "\n",
        "Let's perform statistical tests to determine if there are significant differences between our top models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the top 3 models based on cross-validation\n",
        "top_models = list(cv_means_sorted.keys())[:3]\n",
        "print(f\"Top 3 models: {top_models}\")\n",
        "\n",
        "# Perform paired t-tests between the top models\n",
        "print(\"\\nPaired t-tests for statistical significance:\")\n",
        "for i in range(len(top_models)):\n",
        "    for j in range(i+1, len(top_models)):\n",
        "        model_i = top_models[i]\n",
        "        model_j = top_models[j]\n",
        "        \n",
        "        # Perform paired t-test\n",
        "        t_stat, p_value = stats.ttest_rel(cv_results_f1[model_i], cv_results_f1[model_j])\n",
        "        \n",
        "        # Determine if the difference is significant (α = 0.05)\n",
        "        significant = \"significant\" if p_value < 0.05 else \"not significant\"\n",
        "        \n",
        "        print(f\"{model_i} vs {model_j}:\")\n",
        "        print(f\"  t-statistic: {t_stat:.4f}\")\n",
        "        print(f\"  p-value: {p_value:.4f}\")\n",
        "        print(f\"  Difference is {significant}\")\n",
        "        print(\"\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Detailed Evaluation of Top Models\n",
        "\n",
        "Let's train and evaluate the top models on our test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate top models\n",
        "top_metrics = []\n",
        "\n",
        "for model_name in top_models:\n",
        "    print(f\"\\nTraining and evaluating {model_name}...\")\n",
        "    model = models[model_name]\n",
        "    model.fit(X_train_resampled, y_train_resampled)\n",
        "    \n",
        "    # Evaluate model\n",
        "    metrics = evaluate_model(model, X_train_resampled, X_test_processed, y_train_resampled, y_test, model_name)\n",
        "    top_metrics.append(metrics)\n",
        "    \n",
        "    # Display metrics\n",
        "    display_model_metrics(metrics)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. ROC and PR Curve Comparison\n",
        "\n",
        "Let's compare the ROC and Precision-Recall curves of our top models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare ROC curves of top models\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for metrics in top_metrics:\n",
        "    model_name = metrics['model_name']\n",
        "    y_pred_proba = metrics['y_pred_proba']\n",
        "    \n",
        "    if y_pred_proba is not None:\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "        roc_auc = metrics['roc_auc']\n",
        "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.4f})')\n",
        "\n",
        "# Add random baseline\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve Comparison')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Compare Precision-Recall curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for metrics in top_metrics:\n",
        "    model_name = metrics['model_name']\n",
        "    y_pred_proba = metrics['y_pred_proba']\n",
        "    \n",
        "    if y_pred_proba is not None:\n",
        "        precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "        pr_auc = metrics['pr_auc']\n",
        "        plt.plot(recall_curve, precision_curve, label=f'{model_name} (AUC = {pr_auc:.4f})')\n",
        "\n",
        "# Add baseline\n",
        "plt.axhline(y=y_test.mean(), color='r', linestyle='--', label=f'Baseline ({y_test.mean():.4f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve Comparison')\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Ensemble Methods\n",
        "\n",
        "Let's try combining our top models into ensemble models to see if we can further improve performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create voting classifier with top models\n",
        "top_model_objects = [models[model_name] for model_name in top_models]\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[(model_name, model) for model_name, model in zip(top_models, top_model_objects)],\n",
        "    voting='soft'  # Use probability estimates for voting\n",
        ")\n",
        "\n",
        "# Train the voting classifier\n",
        "print(\"Training Voting Classifier...\")\n",
        "voting_clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Evaluate the voting classifier\n",
        "voting_metrics = evaluate_model(\n",
        "    voting_clf, X_train_resampled, X_test_processed, \n",
        "    y_train_resampled, y_test, \"Voting Ensemble\"\n",
        ")\n",
        "display_model_metrics(voting_metrics)\n",
        "\n",
        "# Add to metrics list for later comparison\n",
        "top_metrics.append(voting_metrics)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Final Model Comparison\n",
        "\n",
        "Let's compare all our models and decide on the best one for our fraud detection task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a final comparison DataFrame\n",
        "final_models = [metrics['model_name'] for metrics in top_metrics]\n",
        "final_df = pd.DataFrame({\n",
        "    'Model': final_models,\n",
        "    'Accuracy': [metrics['accuracy'] for metrics in top_metrics],\n",
        "    'Precision': [metrics['precision'] for metrics in top_metrics],\n",
        "    'Recall': [metrics['recall'] for metrics in top_metrics],\n",
        "    'F1 Score': [metrics['f1'] for metrics in top_metrics],\n",
        "    'ROC AUC': [metrics['roc_auc'] for metrics in top_metrics],\n",
        "    'PR AUC': [metrics['pr_auc'] for metrics in top_metrics]\n",
        "})\n",
        "\n",
        "# Sort by F1 score\n",
        "final_df = final_df.sort_values('F1 Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display the final comparison\n",
        "print(\"Final Model Comparison:\")\n",
        "final_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize final model comparison\n",
        "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC', 'PR AUC']\n",
        "models = final_df['Model'].tolist()\n",
        "\n",
        "# Create a grouped bar chart\n",
        "plt.figure(figsize=(14, 8))\n",
        "x = np.arange(len(models))\n",
        "width = 0.15\n",
        "multiplier = 0\n",
        "\n",
        "for metric in metrics_to_plot:\n",
        "    offset = width * multiplier\n",
        "    plt.bar(x + offset, final_df[metric], width, label=metric)\n",
        "    multiplier += 1\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Final Model Comparison')\n",
        "plt.xticks(x + width * (len(metrics_to_plot) - 1) / 2, models, rotation=45, ha='right')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim(0.7, 1.0)\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Identify the best model\n",
        "best_model_idx = final_df['F1 Score'].idxmax()\n",
        "best_model_name = final_df.loc[best_model_idx, 'Model']\n",
        "best_model_f1 = final_df.loc[best_model_idx, 'F1 Score']\n",
        "\n",
        "print(f\"The best model is {best_model_name} with an F1 Score of {best_model_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. Summary and Next Steps\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "1. **Model Performance Comparison**:\n",
        "   - Several models demonstrated strong performance on our credit card fraud detection task\n",
        "   - The top performing models were [top models will appear here based on execution]\n",
        "   - Statistical testing showed [significance results will appear here based on execution]\n",
        "   - The Voting Ensemble model [comparison with individual models will appear here based on execution]\n",
        "\n",
        "2. **Best Model Selection**:\n",
        "   - [Best model will appear here based on execution] achieved the highest F1 score\n",
        "   - This model offers the best balance between precision and recall\n",
        "   - It demonstrated robust performance in cross-validation and on the test set\n",
        "\n",
        "3. **Evaluation Metrics**:\n",
        "   - Precision-Recall curves provided more insight than ROC curves for this imbalanced dataset\n",
        "   - For fraud detection, we prioritized models with high recall to minimize missed frauds\n",
        "   - PR AUC was a valuable metric given the class imbalance\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **Advanced Optimization**:\n",
        "   - Apply hyperparameter tuning to further optimize the best model\n",
        "   - Explore advanced feature engineering based on domain knowledge\n",
        "   - Investigate additional techniques for handling imbalanced data\n",
        "\n",
        "2. **Model Interpretability**:\n",
        "   - Implement model interpretability tools (SHAP, LIME) to understand predictions\n",
        "   - Analyze feature importance for domain insights\n",
        "   - Create visualizations to explain model decisions\n",
        "\n",
        "3. **Deployment Preparation**:\n",
        "   - Serialize the best model for production use\n",
        "   - Create an inference pipeline that includes preprocessing steps\n",
        "   - Design monitoring systems for model performance\n",
        "   - Implement API endpoints for real-time fraud detection\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
