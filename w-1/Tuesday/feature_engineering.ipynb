{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Feature Engineering Basics\n",
        "\n",
        "This notebook demonstrates various feature engineering techniques to enhance machine learning model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Loading the Dataset\n",
        "\n",
        "We'll use the Wine dataset from sklearn for our feature engineering examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Wine dataset\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Load data\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Create a DataFrame\n",
        "feature_names = wine.feature_names\n",
        "target_names = wine.target_names\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['target'] = y\n",
        "df['target_name'] = df['target'].map({i: name for i, name in enumerate(target_names)})\n",
        "\n",
        "# Display basic information\n",
        "print(\"Wine dataset loaded successfully!\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Features: {', '.join(feature_names)}\")\n",
        "print(f\"Target classes: {', '.join(target_names)}\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Basic statistics\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(df.describe())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Feature Scaling and Normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the feature distributions before scaling\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, feature in enumerate(feature_names[:6]):  # Plot first 6 features\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    sns.histplot(df[feature], kde=True)\n",
        "    plt.title(f'Distribution of {feature}')\n",
        "    plt.tight_layout()\n",
        "plt.suptitle('Original Feature Distributions', y=1.02, fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Boxplot to show scale differences\n",
        "plt.figure(figsize=(14, 6))\n",
        "df_features = df.drop(['target', 'target_name'], axis=1)\n",
        "plt.boxplot(df_features.values, labels=df_features.columns, vert=False)\n",
        "plt.title('Original Feature Scales')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Feature')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Apply different scaling methods\n",
        "X_features = df.drop(['target', 'target_name'], axis=1)\n",
        "\n",
        "# 1. StandardScaler (Z-score normalization)\n",
        "scaler_standard = StandardScaler()\n",
        "X_standard = scaler_standard.fit_transform(X_features)\n",
        "df_standard = pd.DataFrame(X_standard, columns=X_features.columns)\n",
        "\n",
        "# 2. MinMaxScaler (Min-Max scaling)\n",
        "scaler_minmax = MinMaxScaler()\n",
        "X_minmax = scaler_minmax.fit_transform(X_features)\n",
        "df_minmax = pd.DataFrame(X_minmax, columns=X_features.columns)\n",
        "\n",
        "# 3. RobustScaler (Robust to outliers)\n",
        "scaler_robust = RobustScaler()\n",
        "X_robust = scaler_robust.fit_transform(X_features)\n",
        "df_robust = pd.DataFrame(X_robust, columns=X_features.columns)\n",
        "\n",
        "# Compare the scaling methods with boxplots\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.boxplot(X_features.values, labels=X_features.columns, vert=False)\n",
        "plt.title('Original Data')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.boxplot(df_standard.values, labels=df_standard.columns, vert=False)\n",
        "plt.title('StandardScaler (Z-score)')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.boxplot(df_minmax.values, labels=df_minmax.columns, vert=False)\n",
        "plt.title('MinMaxScaler (0-1 range)')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.boxplot(df_robust.values, labels=df_robust.columns, vert=False)\n",
        "plt.title('RobustScaler (based on quantiles)')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.suptitle('Comparison of Scaling Methods', y=0.98, fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Encoding Categorical Variables\n",
        "\n",
        "Let's create a new dataset with categorical variables to demonstrate encoding techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sample dataset with categorical variables\n",
        "np.random.seed(42)\n",
        "n_samples = 100\n",
        "\n",
        "# Create categorical features\n",
        "colors = ['red', 'blue', 'green', 'yellow']\n",
        "sizes = ['small', 'medium', 'large']\n",
        "countries = ['USA', 'Canada', 'UK', 'France', 'Germany']\n",
        "ratings = ['low', 'medium', 'high']\n",
        "\n",
        "# Generate data\n",
        "cat_data = pd.DataFrame({\n",
        "    'color': np.random.choice(colors, n_samples),\n",
        "    'size': np.random.choice(sizes, n_samples),\n",
        "    'country': np.random.choice(countries, n_samples),\n",
        "    'rating': np.random.choice(ratings, n_samples),\n",
        "    'price': np.random.normal(loc=50, scale=15, size=n_samples),\n",
        "    'weight': np.random.uniform(low=0.1, high=10.0, size=n_samples)\n",
        "})\n",
        "\n",
        "# Display the data\n",
        "print(\"Sample categorical dataset:\")\n",
        "print(cat_data.head())\n",
        "\n",
        "# Distribution of categorical variables\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "sns.countplot(y=cat_data['color'])\n",
        "plt.title('Distribution of Colors')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "sns.countplot(y=cat_data['size'])\n",
        "plt.title('Distribution of Sizes')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "sns.countplot(y=cat_data['country'])\n",
        "plt.title('Distribution of Countries')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "sns.countplot(y=cat_data['rating'])\n",
        "plt.title('Distribution of Ratings')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Apply to each categorical column\n",
        "cat_data_label = cat_data.copy()\n",
        "for col in ['color', 'size', 'country', 'rating']:\n",
        "    cat_data_label[f'{col}_encoded'] = label_encoder.fit_transform(cat_data_label[col])\n",
        "\n",
        "print(\"Label Encoding Results:\")\n",
        "print(cat_data_label.head())\n",
        "\n",
        "# Show mapping for each variable\n",
        "for col in ['color', 'size', 'country', 'rating']:\n",
        "    unique_values = cat_data[col].unique()\n",
        "    encoded_values = label_encoder.fit_transform(unique_values)\n",
        "    mapping = dict(zip(unique_values, encoded_values))\n",
        "    print(f\"\\nLabel Encoding mapping for {col}:\")\n",
        "    for original, encoded in mapping.items():\n",
        "        print(f\"  {original} -> {encoded}\")\n",
        "\n",
        "# 2. One-Hot Encoding\n",
        "# Apply one-hot encoding\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False, drop='first')  # Drop first to avoid multicollinearity\n",
        "\n",
        "# Select categorical columns\n",
        "cat_cols = ['color', 'size', 'country', 'rating']\n",
        "cat_data_encoded = cat_data.copy()\n",
        "\n",
        "# Apply one-hot encoding\n",
        "encoded_data = one_hot_encoder.fit_transform(cat_data_encoded[cat_cols])\n",
        "\n",
        "# Get feature names\n",
        "feature_names = []\n",
        "for i, col in enumerate(cat_cols):\n",
        "    categories = one_hot_encoder.categories_[i][1:]  # Skip first category (dropped)\n",
        "    feature_names.extend([f\"{col}_{category}\" for category in categories])\n",
        "\n",
        "# Create DataFrame with encoded features\n",
        "encoded_df = pd.DataFrame(encoded_data, columns=feature_names)\n",
        "\n",
        "# Combine with numerical features\n",
        "cat_data_onehot = pd.concat([cat_data_encoded[['price', 'weight']], encoded_df], axis=1)\n",
        "\n",
        "print(\"\\nOne-Hot Encoding Results (first 5 rows, first 10 columns):\")\n",
        "print(cat_data_onehot.iloc[:5, :10])\n",
        "print(f\"\\nShape after one-hot encoding: {cat_data_onehot.shape}\")\n",
        "\n",
        "# 3. Ordinal Encoding (for ordered categories)\n",
        "# Define the order for ordinal variables\n",
        "size_order = ['small', 'medium', 'large']\n",
        "rating_order = ['low', 'medium', 'high']\n",
        "\n",
        "# Create ordinal encoder\n",
        "ordinal_encoder = OrdinalEncoder(categories=[size_order, rating_order])\n",
        "\n",
        "# Apply ordinal encoding to size and rating\n",
        "cat_data_ordinal = cat_data.copy()\n",
        "cat_data_ordinal[['size_ordinal', 'rating_ordinal']] = ordinal_encoder.fit_transform(cat_data_ordinal[['size', 'rating']])\n",
        "\n",
        "print(\"\\nOrdinal Encoding Results:\")\n",
        "print(cat_data_ordinal[['size', 'size_ordinal', 'rating', 'rating_ordinal']].head())\n",
        "\n",
        "# Show the ordinal mapping\n",
        "print(\"\\nOrdinal Encoding mapping for size:\")\n",
        "for i, val in enumerate(size_order):\n",
        "    print(f\"  {val} -> {i}\")\n",
        "\n",
        "print(\"\\nOrdinal Encoding mapping for rating:\")\n",
        "for i, val in enumerate(rating_order):\n",
        "    print(f\"  {val} -> {i}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Creating New Features from Existing Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's go back to the wine dataset\n",
        "# 1. Creating polynomial features\n",
        "# Select a subset of features for demonstration\n",
        "X_subset = df[['alcohol', 'malic_acid', 'ash']].values\n",
        "\n",
        "# Create polynomial features\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly.fit_transform(X_subset)\n",
        "\n",
        "# Get feature names\n",
        "poly_features = poly.get_feature_names_out(['alcohol', 'malic_acid', 'ash'])\n",
        "\n",
        "# Create DataFrame with polynomial features\n",
        "df_poly = pd.DataFrame(X_poly, columns=poly_features)\n",
        "\n",
        "print(\"Original features:\")\n",
        "print(df[['alcohol', 'malic_acid', 'ash']].head())\n",
        "\n",
        "print(\"\\nPolynomial features (degree=2):\")\n",
        "print(df_poly.head())\n",
        "print(f\"\\nFeature names: {', '.join(poly_features)}\")\n",
        "\n",
        "# 2. Creating interaction features manually\n",
        "# Let's create some interaction features for the wine dataset\n",
        "df_interact = df.copy()\n",
        "\n",
        "# Create interaction features\n",
        "df_interact['alcohol_ash'] = df_interact['alcohol'] * df_interact['ash']\n",
        "df_interact['flavanoids_color'] = df_interact['flavanoids'] * df_interact['color_intensity']\n",
        "df_interact['proline_phenols'] = df_interact['proline'] * df_interact['total_phenols']\n",
        "\n",
        "# Create ratio features\n",
        "df_interact['alcohol_to_malic'] = df_interact['alcohol'] / df_interact['malic_acid']\n",
        "df_interact['hue_to_color'] = df_interact['hue'] / df_interact['color_intensity']\n",
        "\n",
        "# Create sum features\n",
        "df_interact['phenols_flavanoids'] = df_interact['total_phenols'] + df_interact['flavanoids']\n",
        "\n",
        "print(\"\\nCreated interaction features:\")\n",
        "print(df_interact[['alcohol_ash', 'flavanoids_color', 'proline_phenols', \n",
        "                  'alcohol_to_malic', 'hue_to_color', 'phenols_flavanoids']].head())\n",
        "\n",
        "# 3. Creating binned features\n",
        "# Bin the alcohol content\n",
        "df_bins = df.copy()\n",
        "df_bins['alcohol_bin'] = pd.cut(df_bins['alcohol'], bins=4, labels=['low', 'medium', 'high', 'very_high'])\n",
        "df_bins['proline_bin'] = pd.qcut(df_bins['proline'], q=4, labels=['low', 'medium', 'high', 'very_high'])\n",
        "\n",
        "print(\"\\nBinned features:\")\n",
        "print(df_bins[['alcohol', 'alcohol_bin', 'proline', 'proline_bin']].head(10))\n",
        "\n",
        "# Visualize the binned features\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.countplot(y=df_bins['alcohol_bin'])\n",
        "plt.title('Distribution of Alcohol Bins')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.countplot(y=df_bins['proline_bin'])\n",
        "plt.title('Distribution of Proline Bins')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Creating domain-specific features\n",
        "# For wine data, we can create some domain-specific features\n",
        "\n",
        "df_domain = df.copy()\n",
        "\n",
        "# Alcohol-to-sugar ratio (alcohol / residual sugar)\n",
        "df_domain['alcohol_sugar_ratio'] = df_domain['alcohol'] / df_domain['residual_sugar']\n",
        "\n",
        "# Total acidity (fixed acidity + volatile acidity)\n",
        "df_domain['total_acidity'] = df_domain['malic_acid'] + df_domain['ash']\n",
        "\n",
        "# Phenolic compounds ratio\n",
        "df_domain['phenolics_ratio'] = df_domain['total_phenols'] / df_domain['flavanoids']\n",
        "\n",
        "# Color-to-hue ratio\n",
        "df_domain['color_hue_ratio'] = df_domain['color_intensity'] / df_domain['hue']\n",
        "\n",
        "print(\"Domain-specific features:\")\n",
        "print(df_domain[['alcohol_sugar_ratio', 'total_acidity', 'phenolics_ratio', 'color_hue_ratio']].head())\n",
        "\n",
        "# Visualize relationships between new features and target\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "sns.boxplot(x='target_name', y='alcohol_sugar_ratio', data=df_domain)\n",
        "plt.title('Alcohol-Sugar Ratio by Wine Class')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "sns.boxplot(x='target_name', y='total_acidity', data=df_domain)\n",
        "plt.title('Total Acidity by Wine Class')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "sns.boxplot(x='target_name', y='phenolics_ratio', data=df_domain)\n",
        "plt.title('Phenolics Ratio by Wine Class')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "sns.boxplot(x='target_name', y='color_hue_ratio', data=df_domain)\n",
        "plt.title('Color-Hue Ratio by Wine Class')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Feature Selection Techniques\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's use the original wine dataset for feature selection\n",
        "X = df.drop(['target', 'target_name'], axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1. Univariate Feature Selection\n",
        "# Select top k features based on ANOVA F-value\n",
        "selector = SelectKBest(f_classif, k=5)\n",
        "X_new = selector.fit_transform(X_train, y_train)\n",
        "\n",
        "# Get selected feature names\n",
        "selected_features_mask = selector.get_support()\n",
        "selected_features = X.columns[selected_features_mask]\n",
        "\n",
        "print(\"Top 5 features selected by ANOVA F-value:\")\n",
        "for i, feature in enumerate(selected_features):\n",
        "    score = selector.scores_[selected_features_mask][i]\n",
        "    print(f\"  {feature}: F-score = {score:.2f}\")\n",
        "\n",
        "# Visualize feature scores\n",
        "plt.figure(figsize=(12, 6))\n",
        "scores = selector.scores_\n",
        "feature_scores = list(zip(X.columns, scores))\n",
        "feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "features, scores = zip(*feature_scores)\n",
        "\n",
        "plt.barh(range(len(features)), scores, align='center')\n",
        "plt.yticks(range(len(features)), features)\n",
        "plt.title('Feature Importance (ANOVA F-value)')\n",
        "plt.xlabel('F-score')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Recursive Feature Elimination (RFE)\n",
        "# Use Random Forest as the base estimator\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rfe = RFE(estimator=rf, n_features_to_select=5, step=1)\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "# Get selected feature names\n",
        "rfe_selected_features = X.columns[rfe.support_]\n",
        "\n",
        "print(\"\\nTop 5 features selected by RFE with Random Forest:\")\n",
        "for i, feature in enumerate(rfe_selected_features):\n",
        "    print(f\"  {feature}: Rank = {rfe.ranking_[X.columns.get_loc(feature)]}\")\n",
        "\n",
        "# Visualize feature ranking\n",
        "plt.figure(figsize=(12, 6))\n",
        "ranking = rfe.ranking_\n",
        "feature_ranking = list(zip(X.columns, ranking))\n",
        "feature_ranking.sort(key=lambda x: x[1])\n",
        "features, ranking = zip(*feature_ranking)\n",
        "\n",
        "plt.barh(range(len(features)), [1/r for r in ranking], align='center')  # Invert ranking for visualization\n",
        "plt.yticks(range(len(features)), features)\n",
        "plt.title('Feature Ranking (RFE with Random Forest)')\n",
        "plt.xlabel('Inverse Ranking (higher is better)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Feature Importance from Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "print(\"\\nFeature importances from Random Forest:\")\n",
        "for i in range(X.shape[1]):\n",
        "    print(f\"  {X.columns[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
        "\n",
        "# Visualize feature importances\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(range(X.shape[1]), importances[indices], align='center')\n",
        "plt.yticks(range(X.shape[1]), [X.columns[i] for i in indices])\n",
        "plt.title('Feature Importance from Random Forest')\n",
        "plt.xlabel('Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Dimensionality Reduction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Principal Component Analysis (PCA)\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "# Print explained variance\n",
        "print(\"Explained variance by each principal component:\")\n",
        "for i, var in enumerate(explained_variance):\n",
        "    print(f\"  PC{i+1}: {var:.4f} ({cumulative_variance[i]:.4f} cumulative)\")\n",
        "\n",
        "# Visualize explained variance\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7)\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance, 'ro-')\n",
        "plt.title('Explained Variance by Principal Component')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'bo-')\n",
        "plt.axhline(y=0.9, color='r', linestyle='--', label='90% Variance')\n",
        "plt.title('Cumulative Explained Variance')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Determine number of components for 90% variance\n",
        "n_components = np.argmax(cumulative_variance >= 0.9) + 1\n",
        "print(f\"\\nNumber of components needed for 90% variance: {n_components}\")\n",
        "\n",
        "# Apply PCA with selected number of components\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca_reduced = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(f\"\\nOriginal data shape: {X.shape}\")\n",
        "print(f\"Reduced data shape: {X_pca_reduced.shape}\")\n",
        "\n",
        "# Visualize first two principal components\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', s=100, alpha=0.8, edgecolors='k')\n",
        "plt.title('PCA: First Two Principal Components')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.colorbar(scatter, label='Wine Class')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Feature loadings (correlation between original features and principal components)\n",
        "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
        "\n",
        "# Visualize feature loadings for first two components\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.scatter(loadings[:, 0], loadings[:, 1], s=200)\n",
        "for i, feature in enumerate(X.columns):\n",
        "    plt.annotate(feature, (loadings[i, 0], loadings[i, 1]), fontsize=12)\n",
        "plt.title('Feature Loadings for First Two Principal Components')\n",
        "plt.xlabel(f'Principal Component 1 ({explained_variance[0]:.2%} variance)')\n",
        "plt.ylabel(f'Principal Component 2 ({explained_variance[1]:.2%} variance)')\n",
        "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Evaluating Feature Engineering Impact on Model Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's compare the performance of a Random Forest classifier with:\n",
        "# 1. Original features\n",
        "# 2. Selected features from RFE\n",
        "# 3. PCA-transformed features\n",
        "# 4. Original + engineered features\n",
        "\n",
        "# Original features\n",
        "X_train_orig, X_test_orig, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Selected features from RFE\n",
        "rfe = RFE(estimator=RandomForestClassifier(random_state=42), n_features_to_select=5, step=1)\n",
        "rfe.fit(X_train_orig, y_train)\n",
        "X_train_rfe = rfe.transform(X_train_orig)\n",
        "X_test_rfe = rfe.transform(X_test_orig)\n",
        "\n",
        "# PCA-transformed features\n",
        "pca = PCA(n_components=n_components)\n",
        "X_train_pca = pca.fit_transform(scaler.fit_transform(X_train_orig))\n",
        "X_test_pca = pca.transform(scaler.transform(X_test_orig))\n",
        "\n",
        "# Original + engineered features\n",
        "# Create engineered features\n",
        "def create_engineered_features(X):\n",
        "    X_eng = X.copy()\n",
        "    X_eng['alcohol_sugar_ratio'] = X_eng['alcohol'] / X_eng['residual_sugar']\n",
        "    X_eng['total_acidity'] = X_eng['malic_acid'] + X_eng['ash']\n",
        "    X_eng['phenolics_ratio'] = X_eng['total_phenols'] / X_eng['flavanoids']\n",
        "    X_eng['color_hue_ratio'] = X_eng['color_intensity'] / X_eng['hue']\n",
        "    return X_eng\n",
        "\n",
        "X_train_eng = create_engineered_features(X_train_orig)\n",
        "X_test_eng = create_engineered_features(X_test_orig)\n",
        "\n",
        "# Function to evaluate model performance\n",
        "def evaluate_model(X_train, X_test, y_train, y_test, name):\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(\"  Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "    return accuracy, model\n",
        "\n",
        "# Evaluate models\n",
        "print(\"Evaluating the impact of feature engineering on model performance...\")\n",
        "acc_orig, model_orig = evaluate_model(X_train_orig, X_test_orig, y_train, y_test, \"Original Features\")\n",
        "acc_rfe, model_rfe = evaluate_model(X_train_rfe, X_test_rfe, y_train, y_test, \"RFE Selected Features\")\n",
        "acc_pca, model_pca = evaluate_model(X_train_pca, X_test_pca, y_train, y_test, \"PCA Features\")\n",
        "acc_eng, model_eng = evaluate_model(X_train_eng, X_test_eng, y_train, y_test, \"Original + Engineered Features\")\n",
        "\n",
        "# Compare accuracies\n",
        "accuracies = {\n",
        "    'Original Features': acc_orig,\n",
        "    'RFE Selected Features': acc_rfe,\n",
        "    'PCA Features': acc_pca,\n",
        "    'Original + Engineered Features': acc_eng\n",
        "}\n",
        "\n",
        "# Visualize accuracies\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(accuracies.keys(), accuracies.values(), color=['blue', 'green', 'orange', 'red'])\n",
        "plt.title('Model Accuracy with Different Feature Engineering Approaches')\n",
        "plt.xlabel('Feature Set')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0.5, 1.0)\n",
        "plt.grid(True, linestyle='--', alpha=0.7, axis='y')\n",
        "\n",
        "# Add accuracy values on top of bars\n",
        "for i, (key, value) in enumerate(accuracies.items()):\n",
        "    plt.text(i, value + 0.01, f'{value:.4f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary of Feature Engineering Techniques\n",
        "\n",
        "In this notebook, we've explored various feature engineering techniques:\n",
        "\n",
        "### 1. Feature Scaling and Normalization\n",
        "- **StandardScaler (Z-score)**: Standardizes features to have mean=0 and variance=1\n",
        "- **MinMaxScaler**: Scales features to a specific range (usually [0,1])\n",
        "- **RobustScaler**: Scales features using statistics that are robust to outliers\n",
        "\n",
        "### 2. Encoding Categorical Variables\n",
        "- **Label Encoding**: Converts categories to numeric values (0, 1, 2, ...)\n",
        "- **One-Hot Encoding**: Creates binary columns for each category\n",
        "- **Ordinal Encoding**: Assigns ordered numeric values based on category order\n",
        "\n",
        "### 3. Creating New Features\n",
        "- **Polynomial Features**: Creates interaction terms and polynomial features\n",
        "- **Interaction Features**: Multiplication of two or more features\n",
        "- **Ratio Features**: Division of one feature by another\n",
        "- **Binning**: Converting continuous variables into discrete categories\n",
        "- **Domain-specific Features**: Creating features based on domain knowledge\n",
        "\n",
        "### 4. Feature Selection\n",
        "- **Univariate Selection (SelectKBest)**: Selects features based on statistical tests\n",
        "- **Recursive Feature Elimination (RFE)**: Recursively removes features based on importance\n",
        "- **Feature Importance from Models**: Uses model-specific feature importance scores\n",
        "\n",
        "### 5. Dimensionality Reduction\n",
        "- **Principal Component Analysis (PCA)**: Transforms features into uncorrelated components\n",
        "\n",
        "### 6. Impact on Model Performance\n",
        "- We compared model performance using different feature engineering approaches\n",
        "- The best approach depends on the specific dataset and problem\n",
        "\n",
        "Feature engineering is a critical step in the machine learning pipeline that can significantly improve model performance. It requires a combination of domain knowledge, statistical understanding, and experimentation to determine the most effective techniques for a given problem.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
