{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# First Machine Learning Model: Linear Regression\n",
        "\n",
        "In this notebook, we'll implement our first machine learning model - Linear Regression. We'll cover the theory, implementation from scratch, using scikit-learn, and model evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Linear Regression Theory\n",
        "\n",
        "Linear regression is one of the simplest and most widely used statistical models. It models the relationship between a dependent variable (target) and one or more independent variables (features) by fitting a linear equation to the observed data.\n",
        "\n",
        "### Simple Linear Regression\n",
        "\n",
        "$$y = \\beta_0 + \\beta_1 x + \\epsilon$$\n",
        "\n",
        "Where:\n",
        "- $y$ is the dependent variable (target)\n",
        "- $x$ is the independent variable (feature)\n",
        "- $\\beta_0$ is the y-intercept (bias)\n",
        "- $\\beta_1$ is the slope (coefficient)\n",
        "- $\\epsilon$ is the error term\n",
        "\n",
        "### Multiple Linear Regression\n",
        "\n",
        "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon$$\n",
        "\n",
        "Or in matrix notation:\n",
        "\n",
        "$$y = X\\beta + \\epsilon$$\n",
        "\n",
        "### Parameter Estimation\n",
        "\n",
        "The most common method to estimate the parameters is Ordinary Least Squares (OLS), which minimizes the sum of squared residuals:\n",
        "\n",
        "$$\\min_{\\beta} \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_{i1} - ... - \\beta_p x_{ip})^2$$\n",
        "\n",
        "The closed-form solution for OLS is:\n",
        "\n",
        "$$\\hat{\\beta} = (X^TX)^{-1}X^Ty$$\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Simple Linear Regression from Scratch\n",
        "\n",
        "Let's implement a simple linear regression model from scratch using synthetic data to better understand the underlying concepts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create synthetic data\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)  # 100 samples, 1 feature\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)  # true relationship: y = 4 + 3x + noise\n",
        "\n",
        "# Visualize the data\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, alpha=0.7)\n",
        "plt.title('Synthetic Data')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.show()\n",
        "\n",
        "# Implement linear regression from scratch using the normal equation\n",
        "class LinearRegressionFromScratch:\n",
        "    def __init__(self):\n",
        "        self.theta = None  # Model parameters\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        # Add bias term (intercept)\n",
        "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "        \n",
        "        # Normal equation: theta = (X^T X)^(-1) X^T y\n",
        "        self.theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        # Add bias term for prediction\n",
        "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "        \n",
        "        # Make predictions\n",
        "        return X_b.dot(self.theta)\n",
        "    \n",
        "    def get_params(self):\n",
        "        if self.theta is None:\n",
        "            return None\n",
        "        return {\"intercept\": self.theta[0][0], \"coefficient\": self.theta[1][0]}\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegressionFromScratch()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get model parameters\n",
        "params = model.get_params()\n",
        "print(f\"Intercept (β₀): {params['intercept']:.4f}\")\n",
        "print(f\"Coefficient (β₁): {params['coefficient']:.4f}\")\n",
        "print(f\"True parameters: Intercept = 4, Coefficient = 3\")\n",
        "\n",
        "# Make predictions\n",
        "X_new = np.array([[0], [2]])  # Two sample points for visualization\n",
        "y_pred = model.predict(X_new)\n",
        "\n",
        "# Visualize the model\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, alpha=0.7, label='Data points')\n",
        "plt.plot(X_new, y_pred, 'r-', linewidth=2, label='Linear regression model')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Linear Regression from Scratch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Calculate error metrics\n",
        "y_pred_all = model.predict(X)\n",
        "mse = np.mean((y - y_pred_all) ** 2)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = 1 - (np.sum((y - y_pred_all) ** 2) / np.sum((y - np.mean(y)) ** 2))\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"R-squared (R²): {r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Linear Regression using scikit-learn\n",
        "\n",
        "Now let's use scikit-learn's implementation of linear regression on the same synthetic dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use scikit-learn's LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X, y)\n",
        "\n",
        "# Print model parameters\n",
        "print(f\"Intercept (β₀): {lin_reg.intercept_[0]:.4f}\")\n",
        "print(f\"Coefficient (β₁): {lin_reg.coef_[0][0]:.4f}\")\n",
        "print(f\"True parameters: Intercept = 4, Coefficient = 3\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_sk = lin_reg.predict(X)\n",
        "\n",
        "# Calculate error metrics\n",
        "mse_sk = mean_squared_error(y, y_pred_sk)\n",
        "rmse_sk = np.sqrt(mse_sk)\n",
        "r2_sk = r2_score(y, y_pred_sk)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse_sk:.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_sk:.4f}\")\n",
        "print(f\"R-squared (R²): {r2_sk:.4f}\")\n",
        "\n",
        "# Compare with our implementation\n",
        "print(\"\\nComparison with our implementation:\")\n",
        "print(f\"Intercept difference: {abs(params['intercept'] - lin_reg.intercept_[0]):.10f}\")\n",
        "print(f\"Coefficient difference: {abs(params['coefficient'] - lin_reg.coef_[0][0]):.10f}\")\n",
        "print(f\"MSE difference: {abs(mse - mse_sk):.10f}\")\n",
        "\n",
        "# Visualize the scikit-learn model\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, alpha=0.7, label='Data points')\n",
        "plt.plot(X_new, lin_reg.predict(X_new), 'r-', linewidth=2, label='scikit-learn Linear Regression')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Linear Regression with scikit-learn')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Multiple Linear Regression on Real Data\n",
        "\n",
        "Now let's apply multiple linear regression to a real-world dataset - the California Housing dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "# Create a DataFrame\n",
        "X_housing = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y_housing = pd.Series(housing.target)\n",
        "\n",
        "# Display dataset information\n",
        "print(f\"Dataset shape: {X_housing.shape}\")\n",
        "print(f\"Features: {X_housing.columns.tolist()}\")\n",
        "print(f\"Target: Median house value (in $100,000)\")\n",
        "\n",
        "# Summary statistics\n",
        "X_housing.describe().round(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore the data\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Visualize feature distributions\n",
        "for i, feature in enumerate(X_housing.columns):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    sns.histplot(X_housing[feature], kde=True)\n",
        "    plt.title(f'Distribution of {feature}')\n",
        "    plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Explore target distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(y_housing, kde=True)\n",
        "plt.title('Distribution of Median House Value')\n",
        "plt.xlabel('Median House Value ($100,000)')\n",
        "plt.show()\n",
        "\n",
        "# Check correlations\n",
        "correlation_matrix = pd.concat([X_housing, y_housing], axis=1).corr()\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_housing, y_housing, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a multiple linear regression model\n",
        "lin_reg_multi = LinearRegression()\n",
        "lin_reg_multi.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get model coefficients\n",
        "coefficients = pd.DataFrame({\n",
        "    'Feature': X_housing.columns,\n",
        "    'Coefficient': lin_reg_multi.coef_\n",
        "})\n",
        "coefficients = coefficients.sort_values('Coefficient', ascending=False)\n",
        "\n",
        "print(f\"Intercept: {lin_reg_multi.intercept_:.4f}\")\n",
        "print(\"\\nFeature Coefficients:\")\n",
        "print(coefficients)\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(coefficients['Feature'], coefficients['Coefficient'])\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.title('Feature Importance (Coefficient Magnitude)')\n",
        "plt.grid(axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = lin_reg_multi.predict(X_train_scaled)\n",
        "y_test_pred = lin_reg_multi.predict(X_test_scaled)\n",
        "\n",
        "# Calculate error metrics\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "train_rmse = np.sqrt(train_mse)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "print(\"\\nTraining Set Metrics:\")\n",
        "print(f\"MSE: {train_mse:.4f}\")\n",
        "print(f\"RMSE: {train_rmse:.4f}\")\n",
        "print(f\"MAE: {train_mae:.4f}\")\n",
        "print(f\"R²: {train_r2:.4f}\")\n",
        "\n",
        "print(\"\\nTest Set Metrics:\")\n",
        "print(f\"MSE: {test_mse:.4f}\")\n",
        "print(f\"RMSE: {test_rmse:.4f}\")\n",
        "print(f\"MAE: {test_mae:.4f}\")\n",
        "print(f\"R²: {test_r2:.4f}\")\n",
        "\n",
        "# Visualize predictions vs actual values\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Training set\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(y_train, y_train_pred, alpha=0.5)\n",
        "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Training Set: Actual vs Predicted')\n",
        "\n",
        "# Test set\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(y_test, y_test_pred, alpha=0.5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Test Set: Actual vs Predicted')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize residuals\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Training set residuals\n",
        "plt.subplot(1, 2, 1)\n",
        "train_residuals = y_train - y_train_pred\n",
        "plt.scatter(y_train_pred, train_residuals, alpha=0.5)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Training Set: Residual Plot')\n",
        "\n",
        "# Test set residuals\n",
        "plt.subplot(1, 2, 2)\n",
        "test_residuals = y_test - y_test_pred\n",
        "plt.scatter(y_test_pred, test_residuals, alpha=0.5)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Test Set: Residual Plot')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Regularized Linear Regression\n",
        "\n",
        "Standard linear regression can be prone to overfitting, especially with many features. Regularization techniques help address this by penalizing large coefficients:\n",
        "\n",
        "1. **Ridge Regression (L2 regularization)**: Adds a penalty term proportional to the square of coefficient magnitudes\n",
        "   $$\\min_{\\beta} \\sum_{i=1}^{n} (y_i - \\beta X_i)^2 + \\alpha \\sum_{j=1}^{p} \\beta_j^2$$\n",
        "\n",
        "2. **Lasso Regression (L1 regularization)**: Adds a penalty term proportional to the absolute value of coefficients\n",
        "   $$\\min_{\\beta} \\sum_{i=1}^{n} (y_i - \\beta X_i)^2 + \\alpha \\sum_{j=1}^{p} |\\beta_j|$$\n",
        "\n",
        "Let's compare standard linear regression with these regularized versions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Ridge and Lasso models with different regularization strengths\n",
        "alphas = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "\n",
        "# Store results\n",
        "results = {'alpha': [], 'model_type': [], 'train_r2': [], 'test_r2': []}\n",
        "\n",
        "# Baseline Linear Regression (for comparison)\n",
        "results['alpha'].append(0)\n",
        "results['model_type'].append('Linear')\n",
        "results['train_r2'].append(train_r2)\n",
        "results['test_r2'].append(test_r2)\n",
        "\n",
        "# Ridge Regression\n",
        "for alpha in alphas:\n",
        "    ridge = Ridge(alpha=alpha)\n",
        "    ridge.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Evaluate on training and test sets\n",
        "    train_score = ridge.score(X_train_scaled, y_train)\n",
        "    test_score = ridge.score(X_test_scaled, y_test)\n",
        "    \n",
        "    # Store results\n",
        "    results['alpha'].append(alpha)\n",
        "    results['model_type'].append('Ridge')\n",
        "    results['train_r2'].append(train_score)\n",
        "    results['test_r2'].append(test_score)\n",
        "\n",
        "# Lasso Regression\n",
        "for alpha in alphas:\n",
        "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
        "    lasso.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Evaluate on training and test sets\n",
        "    train_score = lasso.score(X_train_scaled, y_train)\n",
        "    test_score = lasso.score(X_test_scaled, y_test)\n",
        "    \n",
        "    # Store results\n",
        "    results['alpha'].append(alpha)\n",
        "    results['model_type'].append('Lasso')\n",
        "    results['train_r2'].append(train_score)\n",
        "    results['test_r2'].append(test_score)\n",
        "\n",
        "# Convert to DataFrame for easier plotting\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Training R²\n",
        "plt.subplot(1, 2, 1)\n",
        "for model in ['Linear', 'Ridge', 'Lasso']:\n",
        "    model_data = results_df[results_df['model_type'] == model]\n",
        "    if model == 'Linear':\n",
        "        plt.axhline(y=model_data['train_r2'].values[0], color='blue', linestyle='-', label='Linear')\n",
        "    else:\n",
        "        plt.plot(model_data['alpha'], model_data['train_r2'], marker='o', label=model)\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Alpha (regularization strength)')\n",
        "plt.ylabel('R² Score')\n",
        "plt.title('Training Set R² vs Alpha')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Test R²\n",
        "plt.subplot(1, 2, 2)\n",
        "for model in ['Linear', 'Ridge', 'Lasso']:\n",
        "    model_data = results_df[results_df['model_type'] == model]\n",
        "    if model == 'Linear':\n",
        "        plt.axhline(y=model_data['test_r2'].values[0], color='blue', linestyle='-', label='Linear')\n",
        "    else:\n",
        "        plt.plot(model_data['alpha'], model_data['test_r2'], marker='o', label=model)\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Alpha (regularization strength)')\n",
        "plt.ylabel('R² Score')\n",
        "plt.title('Test Set R² vs Alpha')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find best models\n",
        "best_ridge = results_df[results_df['model_type'] == 'Ridge'].iloc[results_df[results_df['model_type'] == 'Ridge']['test_r2'].idxmax()]\n",
        "best_lasso = results_df[results_df['model_type'] == 'Lasso'].iloc[results_df[results_df['model_type'] == 'Lasso']['test_r2'].idxmax()]\n",
        "\n",
        "print(f\"Best Ridge model: alpha = {best_ridge['alpha']}, Test R² = {best_ridge['test_r2']:.4f}\")\n",
        "print(f\"Best Lasso model: alpha = {best_lasso['alpha']}, Test R² = {best_lasso['test_r2']:.4f}\")\n",
        "print(f\"Linear Regression: Test R² = {test_r2:.4f}\")\n",
        "\n",
        "# Train the best Ridge model to examine coefficients\n",
        "best_ridge_model = Ridge(alpha=best_ridge['alpha'])\n",
        "best_ridge_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Train the best Lasso model to examine coefficients\n",
        "best_lasso_model = Lasso(alpha=best_lasso['alpha'], max_iter=10000)\n",
        "best_lasso_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Compare coefficients\n",
        "coef_comparison = pd.DataFrame({\n",
        "    'Feature': X_housing.columns,\n",
        "    'Linear': lin_reg_multi.coef_,\n",
        "    'Ridge': best_ridge_model.coef_,\n",
        "    'Lasso': best_lasso_model.coef_\n",
        "})\n",
        "\n",
        "print(\"\\nCoefficient Comparison:\")\n",
        "print(coef_comparison)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize coefficient comparison\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Reshape data for plotting\n",
        "coef_plot_data = pd.melt(coef_comparison, id_vars=['Feature'], var_name='Model', value_name='Coefficient')\n",
        "\n",
        "# Plot\n",
        "sns.barplot(x='Feature', y='Coefficient', hue='Model', data=coef_plot_data)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Coefficient Comparison Across Models')\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare prediction performance of the best models\n",
        "linear_preds = lin_reg_multi.predict(X_test_scaled)\n",
        "ridge_preds = best_ridge_model.predict(X_test_scaled)\n",
        "lasso_preds = best_lasso_model.predict(X_test_scaled)\n",
        "\n",
        "# Create scatter plots\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(y_test, linear_preds, alpha=0.5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.title('Linear Regression')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(y_test, ridge_preds, alpha=0.5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.title(f'Ridge Regression (α={best_ridge[\"alpha\"]})')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(y_test, lasso_preds, alpha=0.5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.title(f'Lasso Regression (α={best_lasso[\"alpha\"]})')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Conclusion\n",
        "\n",
        "In this notebook, we've explored linear regression from multiple perspectives:\n",
        "\n",
        "1. **Theory**: Understanding the mathematical foundations of linear regression\n",
        "2. **Implementation from Scratch**: Building a simple linear regression model using the normal equation\n",
        "3. **scikit-learn Implementation**: Using scikit-learn's LinearRegression class\n",
        "4. **Multiple Linear Regression**: Applying linear regression to real-world data with multiple features\n",
        "5. **Regularization**: Comparing standard linear regression with Ridge and Lasso regularization\n",
        "\n",
        "Key takeaways:\n",
        "\n",
        "1. Linear regression is a powerful and interpretable model for continuous target variables\n",
        "2. The coefficients provide insights into feature importance and relationships\n",
        "3. Regularization helps prevent overfitting and can improve model generalization\n",
        "4. Ridge regression (L2) shrinks coefficients toward zero but rarely eliminates them completely\n",
        "5. Lasso regression (L1) can perform feature selection by setting some coefficients to exactly zero\n",
        "\n",
        "Linear regression serves as an excellent baseline model and foundation for understanding more complex machine learning algorithms.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
