{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Introduction to scikit-learn\n",
        "\n",
        "This notebook provides an overview of the scikit-learn library structure and its core components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Import key scikit-learn modules\n",
        "import sklearn\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Overview of scikit-learn\n",
        "\n",
        "scikit-learn is a popular machine learning library in Python that provides:\n",
        "\n",
        "1. Simple and efficient tools for data mining and data analysis\n",
        "2. Accessible to everybody, and reusable in various contexts\n",
        "3. Built on NumPy, SciPy, and matplotlib\n",
        "4. Open source, commercially usable - BSD license\n",
        "\n",
        "Let's explore the major components of the library and how they work together.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check scikit-learn version\n",
        "print(f\"scikit-learn version: {sklearn.__version__}\")\n",
        "\n",
        "# Display major modules\n",
        "major_modules = [\n",
        "    'sklearn.datasets', 'sklearn.preprocessing', 'sklearn.model_selection',\n",
        "    'sklearn.feature_extraction', 'sklearn.feature_selection', 'sklearn.decomposition',\n",
        "    'sklearn.linear_model', 'sklearn.ensemble', 'sklearn.tree', 'sklearn.neighbors',\n",
        "    'sklearn.svm', 'sklearn.cluster', 'sklearn.metrics', 'sklearn.pipeline'\n",
        "]\n",
        "\n",
        "print(\"\\nMajor scikit-learn modules:\")\n",
        "for module in major_modules:\n",
        "    print(f\"- {module}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. The scikit-learn API Design\n",
        "\n",
        "scikit-learn's API is designed to be consistent and easy to use. Most objects follow a simple set of conventions:\n",
        "\n",
        "1. **Estimators**: Objects that learn from data (e.g., `LinearRegression`, `RandomForestClassifier`)\n",
        "   - They implement a `fit(X, y)` method\n",
        "   - After fitting, they store learned parameters with a trailing underscore (e.g., `coef_`)\n",
        "\n",
        "2. **Predictors**: Estimators with a `predict(X)` method\n",
        "   - Can also have other methods like `predict_proba()` for probabilities\n",
        "   - Often also implement `score()` to evaluate predictions\n",
        "\n",
        "3. **Transformers**: Estimators that transform data (e.g., `StandardScaler`, `PCA`)\n",
        "   - Implement `transform(X)` to create a new representation of the data\n",
        "   - Also implement `fit_transform(X)` as a convenience method\n",
        "\n",
        "4. **Meta-estimators**: Estimators that take another estimator as parameter\n",
        "   - Examples include `Pipeline`, `GridSearchCV`, and ensemble methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of working with scikit-learn's API\n",
        "\n",
        "# 1. Load dataset\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Features: {iris.feature_names}\")\n",
        "print(f\"Target classes: {iris.target_names}\")\n",
        "print(f\"Number of samples per class: {np.bincount(y)}\")\n",
        "\n",
        "# 2. Split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Preprocess the data (transformer)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)  # Note: only transform, not fit_transform\n",
        "\n",
        "# 4. Train a model (estimator)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 5. Make predictions (predictor)\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_proba = model.predict_proba(X_test_scaled)\n",
        "\n",
        "# 6. Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the model's decision boundaries\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Use PCA to reduce to 2 dimensions for visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Train a new logistic regression model on the PCA-transformed data\n",
        "model_pca = LogisticRegression(max_iter=200)\n",
        "model_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "# Create a mesh grid to visualize decision boundaries\n",
        "def plot_decision_boundaries(X, y, model, ax=None):\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    \n",
        "    # Create mesh grid\n",
        "    h = 0.02  # step size in the mesh\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    \n",
        "    # Make predictions on the mesh grid\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    # Plot decision boundaries\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3)\n",
        "    \n",
        "    # Plot data points\n",
        "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', s=50, cmap='viridis')\n",
        "    \n",
        "    return ax\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Training data\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_decision_boundaries(X_train_pca, y_train, model_pca)\n",
        "plt.title('Decision Boundaries (Training Data)')\n",
        "plt.xlabel('PCA Feature 1')\n",
        "plt.ylabel('PCA Feature 2')\n",
        "\n",
        "# Test data\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_decision_boundaries(X_test_pca, y_test, model_pca)\n",
        "plt.title('Decision Boundaries (Test Data)')\n",
        "plt.xlabel('PCA Feature 1')\n",
        "plt.ylabel('PCA Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Datasets in scikit-learn\n",
        "\n",
        "scikit-learn provides various datasets for practice and experimentation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore available datasets in scikit-learn\n",
        "from sklearn import datasets\n",
        "\n",
        "# 1. Toy datasets (small, for quick testing)\n",
        "print(\"Toy datasets:\")\n",
        "print(\"- Iris dataset (classification)\")\n",
        "print(\"- Digits dataset (classification)\")\n",
        "print(\"- Wine dataset (classification)\")\n",
        "print(\"- Breast cancer dataset (classification)\")\n",
        "print(\"- Boston housing dataset (regression)\")\n",
        "print(\"- Diabetes dataset (regression)\")\n",
        "\n",
        "# 2. Sample generators (synthetic data)\n",
        "print(\"\\nSample generators:\")\n",
        "print(\"- make_classification: Generate random n-class classification problem\")\n",
        "print(\"- make_regression: Generate random regression problem\")\n",
        "print(\"- make_blobs: Generate isotropic Gaussian blobs for clustering\")\n",
        "print(\"- make_circles/make_moons: Generate 2D classification datasets\")\n",
        "\n",
        "# 3. Real-world datasets (fetch from remote)\n",
        "print(\"\\nReal-world datasets:\")\n",
        "print(\"- fetch_california_housing: California housing regression dataset\")\n",
        "print(\"- fetch_covtype: Forest cover type classification dataset\")\n",
        "print(\"- fetch_20newsgroups: 20 newsgroups text dataset\")\n",
        "print(\"- fetch_olivetti_faces: Olivetti faces dataset\")\n",
        "\n",
        "# Let's explore a few examples\n",
        "# Example 1: Digits dataset (for classification)\n",
        "digits = datasets.load_digits()\n",
        "print(f\"\\nDigits dataset: {digits.data.shape} samples with {digits.data.shape[1]} features\")\n",
        "print(f\"Target classes: {np.unique(digits.target)}\")\n",
        "\n",
        "# Example 2: Synthetic classification data\n",
        "from sklearn.datasets import make_classification\n",
        "X_synth, y_synth = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=10,\n",
        "    n_redundant=5,\n",
        "    n_classes=3,\n",
        "    random_state=42\n",
        ")\n",
        "print(f\"\\nSynthetic classification data: {X_synth.shape}\")\n",
        "print(f\"Class distribution: {np.bincount(y_synth)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the digits dataset\n",
        "plt.figure(figsize=(14, 4))\n",
        "for i in range(10):\n",
        "    plt.subplot(1, 10, i+1)\n",
        "    plt.imshow(digits.images[i], cmap='binary')\n",
        "    plt.title(f\"Digit: {digits.target[i]}\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize synthetic data using PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reduce to 2 dimensions for visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_synth_2d = pca.fit_transform(X_synth)\n",
        "\n",
        "# Plot the synthetic data\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i in range(3):  # 3 classes\n",
        "    plt.scatter(X_synth_2d[y_synth == i, 0], X_synth_2d[y_synth == i, 1], label=f'Class {i}', alpha=0.7)\n",
        "plt.title('PCA Visualization of Synthetic Classification Data')\n",
        "plt.xlabel('PCA Feature 1')\n",
        "plt.ylabel('PCA Feature 2')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Supervised Learning Models in scikit-learn\n",
        "\n",
        "scikit-learn provides a wide variety of supervised learning algorithms. Let's explore a few of the most common ones:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's compare different classification algorithms on the Iris dataset\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Set up the models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=200),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'SVM': SVC(),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Neural Network': MLPClassifier(max_iter=500)\n",
        "}\n",
        "\n",
        "# Use the original Iris dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    iris.data, iris.target, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train and evaluate each model\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    # Train the model\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results[name] = accuracy\n",
        "    \n",
        "    print(f\"{name} - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Visualize results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(results.keys(), results.values())\n",
        "plt.title('Model Comparison on Iris Dataset')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0.8, 1.05)  # Adjust as needed\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Model Selection and Evaluation in scikit-learn\n",
        "\n",
        "Model selection and evaluation are critical steps in the machine learning workflow. scikit-learn provides various tools for these purposes:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.1 Cross-Validation\n",
        "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
        "\n",
        "# Let's focus on the Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Basic cross-validation\n",
        "cv_scores = cross_val_score(rf, X_train_scaled, y_train, cv=5)\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(f\"Mean CV score: {cv_scores.mean():.4f}\")\n",
        "print(f\"Standard deviation: {cv_scores.std():.4f}\")\n",
        "\n",
        "# Using KFold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "kf_scores = cross_val_score(rf, X_train_scaled, y_train, cv=kf)\n",
        "print(\"\\nKFold cross-validation scores:\", kf_scores)\n",
        "print(f\"Mean KFold CV score: {kf_scores.mean():.4f}\")\n",
        "\n",
        "# Using StratifiedKFold (maintains class distribution in each fold)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "skf_scores = cross_val_score(rf, X_train_scaled, y_train, cv=skf)\n",
        "print(\"\\nStratifiedKFold cross-validation scores:\", skf_scores)\n",
        "print(f\"Mean StratifiedKFold CV score: {skf_scores.mean():.4f}\")\n",
        "\n",
        "# 5.2 Hyperparameter Tuning\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "# Define parameter grid for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "grid_search = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nGrid Search Results:\")\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Get test set performance with best model\n",
        "best_model = grid_search.best_estimator_\n",
        "test_accuracy = best_model.score(X_test_scaled, y_test)\n",
        "print(f\"Test accuracy with best model: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize grid search results\n",
        "# Extract results from grid search\n",
        "grid_results = pd.DataFrame(grid_search.cv_results_)\n",
        "\n",
        "# Function to plot parameter comparison\n",
        "def plot_grid_search_param(results, param_name, scoring='mean_test_score'):\n",
        "    plt.figure(figsize=(13, 5))\n",
        "    \n",
        "    # Group by the parameter and calculate mean score\n",
        "    grouped = results.groupby(f'param_{param_name}')[scoring].mean()\n",
        "    \n",
        "    # Plot bar chart\n",
        "    sns.barplot(x=grouped.index, y=grouped.values)\n",
        "    plt.title(f'Grid Search Scores for Different {param_name} Values')\n",
        "    plt.xlabel(param_name)\n",
        "    plt.ylabel('Mean Test Score')\n",
        "    plt.ylim(0.9, 1.0)  # Adjust as needed\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the effect of different parameters\n",
        "for param in param_grid.keys():\n",
        "    plot_grid_search_param(grid_results, param)\n",
        "\n",
        "# 5.3 Model Evaluation Metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "\n",
        "# Use the best model from grid search\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "y_prob = best_model.predict_proba(X_test_scaled)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=iris.target_names,\n",
        "            yticklabels=iris.target_names)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "# ROC Curve and AUC (one-vs-rest for multiclass)\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i, class_name in enumerate(iris.target_names):\n",
        "    # Calculate ROC curve and AUC for each class\n",
        "    fpr, tpr, _ = roc_curve(y_test == i, y_prob[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    \n",
        "    # Plot ROC curve\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'{class_name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves (One-vs-Rest)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Pipeline API\n",
        "\n",
        "The Pipeline API is one of scikit-learn's most powerful features, allowing you to chain multiple preprocessing steps and a model into a single object:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating a complete pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),               # Step 1: Scale the data\n",
        "    ('feature_selection', SelectKBest(f_classif, k=3)),  # Step 2: Select top k features\n",
        "    ('pca', PCA(n_components=2)),               # Step 3: Reduce dimensions with PCA\n",
        "    ('classifier', RandomForestClassifier(random_state=42))  # Step 4: Train a classifier\n",
        "])\n",
        "\n",
        "# Train the pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on test data\n",
        "pipeline_accuracy = pipeline.score(X_test, y_test)\n",
        "print(f\"Pipeline accuracy: {pipeline_accuracy:.4f}\")\n",
        "\n",
        "# Make predictions (the pipeline handles all preprocessing steps)\n",
        "y_pipeline_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Inspect the pipeline steps\n",
        "print(\"\\nPipeline steps:\")\n",
        "for step_idx, (name, transformer) in enumerate(pipeline.steps):\n",
        "    print(f\"Step {step_idx+1}: {name} - {transformer.__class__.__name__}\")\n",
        "    \n",
        "    # Print details for each step if available\n",
        "    if name == 'feature_selection':\n",
        "        selected_features = iris.feature_names[transformer.get_support()]\n",
        "        print(f\"  Selected features: {', '.join(selected_features)}\")\n",
        "    elif name == 'classifier':\n",
        "        print(f\"  Number of trees: {transformer.n_estimators}\")\n",
        "        print(f\"  Feature importances: {transformer.feature_importances_}\")\n",
        "        \n",
        "# Using ColumnTransformer with Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Let's create an example dataset with mixed data types\n",
        "X_mixed = pd.DataFrame({\n",
        "    'feature1': np.random.randn(100),  # numerical\n",
        "    'feature2': np.random.randn(100),  # numerical\n",
        "    'category': np.random.choice(['A', 'B', 'C'], 100)  # categorical\n",
        "})\n",
        "y_mixed = np.random.randint(0, 2, 100)  # binary target\n",
        "\n",
        "# Define preprocessing for numerical and categorical features\n",
        "numerical_features = ['feature1', 'feature2']\n",
        "categorical_features = ['category']\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine transformers using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create the full pipeline\n",
        "full_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Split the mixed data\n",
        "X_mixed_train, X_mixed_test, y_mixed_train, y_mixed_test = train_test_split(\n",
        "    X_mixed, y_mixed, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train and evaluate\n",
        "full_pipeline.fit(X_mixed_train, y_mixed_train)\n",
        "mixed_accuracy = full_pipeline.score(X_mixed_test, y_mixed_test)\n",
        "print(f\"\\nMixed data pipeline accuracy: {mixed_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Conclusion\n",
        "\n",
        "In this notebook, we've explored the core components of scikit-learn:\n",
        "\n",
        "1. **API Design**: Consistent estimator, predictor, and transformer interfaces\n",
        "2. **Datasets**: Built-in datasets and tools for creating synthetic data\n",
        "3. **Supervised Learning Models**: A variety of algorithms for classification and regression\n",
        "4. **Model Selection & Evaluation**: Cross-validation, hyperparameter tuning, and metrics\n",
        "5. **Pipeline API**: Combining preprocessing steps and models into a single object\n",
        "\n",
        "scikit-learn provides a consistent and user-friendly interface for machine learning tasks, making it an excellent choice for both beginners and experienced practitioners. As we move forward, we'll continue to leverage scikit-learn's capabilities to build and evaluate machine learning models efficiently.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
