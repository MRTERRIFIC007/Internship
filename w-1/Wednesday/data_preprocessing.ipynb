{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Building Data Preprocessing Pipelines\n",
        "\n",
        "In this notebook, we'll explore how to build robust data preprocessing pipelines for machine learning workflows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Loading and Exploring the Dataset\n",
        "\n",
        "Let's start by loading a dataset and understanding its structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "# Create a DataFrame\n",
        "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y = pd.Series(housing.target, name='MedHouseValue')\n",
        "\n",
        "# Display dataset information\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Features: {X.columns.tolist()}\")\n",
        "print(f\"Target variable: {y.name}\")\n",
        "print(\"\\nFeature statistics:\")\n",
        "X.describe().round(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "missing_values = X.isnull().sum()\n",
        "print(\"Missing values per column:\")\n",
        "print(missing_values)\n",
        "\n",
        "# Visualize feature distributions\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(X.columns):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    sns.histplot(X[col], kde=True)\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Introducing Artificial Issues for Preprocessing\n",
        "\n",
        "To demonstrate preprocessing techniques, let's introduce some artificial issues to our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy of the dataset to modify\n",
        "X_messy = X.copy()\n",
        "\n",
        "# 1. Introduce missing values (about 5% of data)\n",
        "for col in X_messy.columns[:4]:  # Add missing values to first 4 columns\n",
        "    mask = np.random.rand(len(X_messy)) < 0.05\n",
        "    X_messy.loc[mask, col] = np.nan\n",
        "\n",
        "# 2. Add outliers to MedInc column\n",
        "outlier_idx = np.random.choice(len(X_messy), 20, replace=False)\n",
        "X_messy.loc[outlier_idx, 'MedInc'] = X_messy['MedInc'].max() * 2.5\n",
        "\n",
        "# 3. Add a categorical column (for demonstration purposes)\n",
        "regions = ['coastal', 'inland', 'valley', 'mountain']\n",
        "X_messy['Region'] = np.random.choice(regions, size=len(X_messy))\n",
        "\n",
        "# 4. Scale AveRooms to a different range (0-100) to demonstrate scaling issues\n",
        "X_messy['AveRooms'] = X_messy['AveRooms'] * 10\n",
        "\n",
        "# Display the modified dataset\n",
        "print(f\"Modified dataset shape: {X_messy.shape}\")\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(X_messy.isnull().sum())\n",
        "print(\"\\nSample of the messy dataset:\")\n",
        "X_messy.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the effect of outliers we introduced\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(x=X['MedInc'])\n",
        "plt.title('Original MedInc Distribution')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(x=X_messy['MedInc'])\n",
        "plt.title('MedInc with Outliers')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize the scaled AveRooms feature\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(X['AveRooms'], kde=True)\n",
        "plt.title('Original AveRooms Distribution')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(X_messy['AveRooms'], kde=True)\n",
        "plt.title('Scaled AveRooms Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check categorical feature distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='Region', data=X_messy)\n",
        "plt.title('Distribution of Region Categories')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Handling Missing Values\n",
        "\n",
        "There are several strategies for handling missing values. Let's explore the most common ones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_messy, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n",
        "\n",
        "# Check missing values in the training set\n",
        "print(\"\\nMissing values in training set:\")\n",
        "print(X_train.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.1 Simple Imputation (Mean, Median, Most Frequent)\n",
        "\n",
        "# Define different imputation strategies for different columns\n",
        "# - Mean imputation for MedInc (income)\n",
        "# - Median imputation for HouseAge (to handle skewed distributions better)\n",
        "# - Most frequent value for AveRooms\n",
        "# - Constant value (0) for AveBedrms\n",
        "\n",
        "# Create and fit imputers\n",
        "mean_imputer = SimpleImputer(strategy='mean')\n",
        "median_imputer = SimpleImputer(strategy='median')\n",
        "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
        "constant_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
        "\n",
        "# Apply imputation\n",
        "X_train_mean_imputed = mean_imputer.fit_transform(X_train[['MedInc']])\n",
        "X_train_median_imputed = median_imputer.fit_transform(X_train[['HouseAge']])\n",
        "X_train_mode_imputed = mode_imputer.fit_transform(X_train[['AveRooms']])\n",
        "X_train_constant_imputed = constant_imputer.fit_transform(X_train[['AveBedrms']])\n",
        "\n",
        "# Check results (for MedInc column)\n",
        "print(f\"Original MedInc mean: {X_train['MedInc'].mean()}\")\n",
        "print(f\"After mean imputation: {np.mean(X_train_mean_imputed)}\")\n",
        "print(f\"Missing values before imputation: {X_train['MedInc'].isnull().sum()}\")\n",
        "print(f\"Missing values after imputation: {np.isnan(X_train_mean_imputed).sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.2 KNN Imputation (more advanced)\n",
        "# This method imputes values based on k nearest neighbors\n",
        "\n",
        "# Create and fit KNN imputer\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "\n",
        "# Select numerical columns for KNN imputation\n",
        "numerical_cols = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
        "X_train_knn = X_train[numerical_cols].copy()\n",
        "\n",
        "# Apply KNN imputation\n",
        "X_train_knn_imputed = knn_imputer.fit_transform(X_train_knn)\n",
        "\n",
        "# Convert back to DataFrame for comparison\n",
        "X_train_knn_imputed_df = pd.DataFrame(X_train_knn_imputed, columns=numerical_cols)\n",
        "\n",
        "# Compare original vs KNN imputed values for MedInc\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(X_train['MedInc'].dropna(), kde=True, color='blue')\n",
        "plt.title('Original MedInc (without NaNs)')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(X_train_knn_imputed_df['MedInc'], kde=True, color='green')\n",
        "plt.title('KNN Imputed MedInc')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Original MedInc mean: {X_train['MedInc'].mean()}\")\n",
        "print(f\"After KNN imputation mean: {X_train_knn_imputed_df['MedInc'].mean()}\")\n",
        "print(f\"Missing values before imputation: {X_train['MedInc'].isnull().sum()}\")\n",
        "print(f\"Missing values after imputation: {X_train_knn_imputed_df['MedInc'].isnull().sum()}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Feature Scaling\n",
        "\n",
        "Different scaling methods have different properties and use cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the imputed data from KNN imputation\n",
        "X_train_cleaned = X_train_knn_imputed_df.copy()\n",
        "\n",
        "# 4.1 StandardScaler (mean=0, std=1)\n",
        "std_scaler = StandardScaler()\n",
        "X_train_std_scaled = std_scaler.fit_transform(X_train_cleaned)\n",
        "\n",
        "# 4.2 MinMaxScaler (scales to a range, typically [0, 1])\n",
        "minmax_scaler = MinMaxScaler()\n",
        "X_train_minmax_scaled = minmax_scaler.fit_transform(X_train_cleaned)\n",
        "\n",
        "# 4.3 RobustScaler (uses quantiles, more robust to outliers)\n",
        "robust_scaler = RobustScaler()\n",
        "X_train_robust_scaled = robust_scaler.fit_transform(X_train_cleaned)\n",
        "\n",
        "# Convert to DataFrames for comparison\n",
        "X_train_std_df = pd.DataFrame(X_train_std_scaled, columns=numerical_cols)\n",
        "X_train_minmax_df = pd.DataFrame(X_train_minmax_scaled, columns=numerical_cols)\n",
        "X_train_robust_df = pd.DataFrame(X_train_robust_scaled, columns=numerical_cols)\n",
        "\n",
        "# Compare scaling methods on the MedInc column (which has outliers)\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "sns.boxplot(x=X_train_cleaned['MedInc'])\n",
        "plt.title('Original Data')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "sns.boxplot(x=X_train_std_df['MedInc'])\n",
        "plt.title('StandardScaler')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "sns.boxplot(x=X_train_minmax_df['MedInc'])\n",
        "plt.title('MinMaxScaler')\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "sns.boxplot(x=X_train_robust_df['MedInc'])\n",
        "plt.title('RobustScaler')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print scaling statistics\n",
        "print(\"Original MedInc stats:\")\n",
        "print(f\"Mean: {X_train_cleaned['MedInc'].mean():.4f}\")\n",
        "print(f\"Std: {X_train_cleaned['MedInc'].std():.4f}\")\n",
        "print(f\"Min: {X_train_cleaned['MedInc'].min():.4f}\")\n",
        "print(f\"Max: {X_train_cleaned['MedInc'].max():.4f}\")\n",
        "\n",
        "print(\"\\nStandardScaler MedInc stats:\")\n",
        "print(f\"Mean: {X_train_std_df['MedInc'].mean():.4f}\")\n",
        "print(f\"Std: {X_train_std_df['MedInc'].std():.4f}\")\n",
        "print(f\"Min: {X_train_std_df['MedInc'].min():.4f}\")\n",
        "print(f\"Max: {X_train_std_df['MedInc'].max():.4f}\")\n",
        "\n",
        "print(\"\\nMinMaxScaler MedInc stats:\")\n",
        "print(f\"Mean: {X_train_minmax_df['MedInc'].mean():.4f}\")\n",
        "print(f\"Std: {X_train_minmax_df['MedInc'].std():.4f}\")\n",
        "print(f\"Min: {X_train_minmax_df['MedInc'].min():.4f}\")\n",
        "print(f\"Max: {X_train_minmax_df['MedInc'].max():.4f}\")\n",
        "\n",
        "print(\"\\nRobustScaler MedInc stats:\")\n",
        "print(f\"Mean: {X_train_robust_df['MedInc'].mean():.4f}\")\n",
        "print(f\"Std: {X_train_robust_df['MedInc'].std():.4f}\")\n",
        "print(f\"Min: {X_train_robust_df['MedInc'].min():.4f}\")\n",
        "print(f\"Max: {X_train_robust_df['MedInc'].max():.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Handling Categorical Features\n",
        "\n",
        "Categorical features need special treatment before they can be used in ML models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.1 Label Encoding (for ordinal categories)\n",
        "# This assigns a numeric value to each category\n",
        "\n",
        "# Create a sample ordinal feature (for demonstration)\n",
        "X_train_cat = X_train.copy()\n",
        "quality_map = {'low': 0, 'medium': 1, 'high': 2}\n",
        "X_train_cat['HousingQuality'] = np.random.choice(['low', 'medium', 'high'], size=len(X_train_cat))\n",
        "\n",
        "# Apply label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "X_train_cat['HousingQuality_encoded'] = label_encoder.fit_transform(X_train_cat['HousingQuality'])\n",
        "\n",
        "# Display the encoding\n",
        "print(\"Label Encoding Results:\")\n",
        "print(pd.crosstab(X_train_cat['HousingQuality'], X_train_cat['HousingQuality_encoded']))\n",
        "\n",
        "# 5.2 One-Hot Encoding (for nominal categories)\n",
        "# This creates binary columns for each category\n",
        "\n",
        "# Apply one-hot encoding to the 'Region' column\n",
        "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
        "region_encoded = onehot_encoder.fit_transform(X_train_cat[['Region']])\n",
        "\n",
        "# Create a DataFrame with the encoded values\n",
        "region_encoded_df = pd.DataFrame(\n",
        "    region_encoded, \n",
        "    columns=onehot_encoder.get_feature_names_out(['Region'])\n",
        ")\n",
        "\n",
        "# Display the first few rows of the one-hot encoded data\n",
        "print(\"\\nOne-Hot Encoding Results (first 5 rows):\")\n",
        "print(region_encoded_df.head())\n",
        "\n",
        "# Check how original categories map to one-hot encoded columns\n",
        "print(\"\\nCategory mapping:\")\n",
        "for i, category in enumerate(onehot_encoder.categories_[0]):\n",
        "    print(f\"Category '{category}' → Column 'Region_{category}'\")\n",
        "\n",
        "# Combine with original data\n",
        "X_train_with_onehot = pd.concat([X_train_cat, region_encoded_df], axis=1)\n",
        "print(\"\\nDataFrame with one-hot encoded 'Region' (first 5 rows, selected columns):\")\n",
        "print(X_train_with_onehot[['Region', 'Region_coastal', 'Region_inland', 'Region_mountain', 'Region_valley']].head())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Feature Selection\n",
        "\n",
        "Not all features are equally important for our model. Feature selection helps identify the most relevant ones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the clean numerical data\n",
        "X_select = X_train_cleaned.copy()\n",
        "\n",
        "# Use SelectKBest with f_regression (for regression tasks)\n",
        "selector = SelectKBest(score_func=f_regression, k=4)  # Select top 4 features\n",
        "X_selected = selector.fit_transform(X_select, y_train)\n",
        "\n",
        "# Get feature scores and p-values\n",
        "scores = selector.scores_\n",
        "p_values = selector.pvalues_\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "feature_scores = pd.DataFrame({\n",
        "    'Feature': X_select.columns,\n",
        "    'Score': scores,\n",
        "    'p-value': p_values,\n",
        "    'Selected': selector.get_support()\n",
        "})\n",
        "\n",
        "# Sort by score\n",
        "feature_scores = feature_scores.sort_values('Score', ascending=False)\n",
        "\n",
        "print(\"Feature Selection Results:\")\n",
        "print(feature_scores)\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Score', y='Feature', data=feature_scores)\n",
        "plt.title('Feature Importance Scores')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Get the selected feature names\n",
        "selected_features = X_select.columns[selector.get_support()]\n",
        "print(f\"\\nSelected features: {', '.join(selected_features)}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Building a Complete Preprocessing Pipeline\n",
        "\n",
        "Now let's combine all these preprocessing steps into a single pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the original messy data again\n",
        "X_train_original = X_train.copy()\n",
        "X_test_original = X_test.copy()\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
        "categorical_features = ['Region']\n",
        "\n",
        "# Create preprocessing steps for numeric features\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', KNNImputer(n_neighbors=5)),\n",
        "    ('scaler', RobustScaler())\n",
        "])\n",
        "\n",
        "# Create preprocessing steps for categorical features\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create the final pipeline\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('selector', SelectKBest(f_regression, k=5))\n",
        "])\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_preprocessed = pipeline.fit_transform(X_train_original, y_train)\n",
        "\n",
        "# Transform the test data (using the transformations learned from the training data)\n",
        "X_test_preprocessed = pipeline.transform(X_test_original)\n",
        "\n",
        "print(f\"Original training data shape: {X_train_original.shape}\")\n",
        "print(f\"Preprocessed training data shape: {X_train_preprocessed.shape}\")\n",
        "print(f\"Original test data shape: {X_test_original.shape}\")\n",
        "print(f\"Preprocessed test data shape: {X_test_preprocessed.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extending the pipeline with a model (Linear Regression)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Create a full pipeline including the model\n",
        "full_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('selector', SelectKBest(f_regression, k=5)),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "\n",
        "# Fit the model on training data\n",
        "full_pipeline.fit(X_train_original, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = full_pipeline.predict(X_train_original)\n",
        "y_test_pred = full_pipeline.predict(X_test_original)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"Model Performance:\")\n",
        "print(f\"Training MSE: {train_mse:.4f}\")\n",
        "print(f\"Test MSE: {test_mse:.4f}\")\n",
        "print(f\"Training R²: {train_r2:.4f}\")\n",
        "print(f\"Test R²: {test_r2:.4f}\")\n",
        "\n",
        "# Visualize predictions vs actual values\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(y_train, y_train_pred, alpha=0.5)\n",
        "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title('Training Set: Actual vs Predicted')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(y_test, y_test_pred, alpha=0.5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title('Test Set: Actual vs Predicted')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Handling Imbalanced Datasets\n",
        "\n",
        "Imbalanced datasets are common in classification problems. Let's explore techniques to address this issue.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an imbalanced classification dataset for demonstration\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "# Generate imbalanced binary classification data\n",
        "X_imb, y_imb = make_classification(\n",
        "    n_samples=1000, n_features=10, n_informative=5, n_redundant=3,\n",
        "    n_classes=2, weights=[0.9, 0.1], random_state=42\n",
        ")\n",
        "\n",
        "# Split the data\n",
        "X_imb_train, X_imb_test, y_imb_train, y_imb_test = train_test_split(\n",
        "    X_imb, y_imb, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Check class distribution\n",
        "train_class_counts = Counter(y_imb_train)\n",
        "print(\"Class distribution in training set:\")\n",
        "print(f\"Class 0 (majority): {train_class_counts[0]} samples ({train_class_counts[0]/len(y_imb_train):.1%})\")\n",
        "print(f\"Class 1 (minority): {train_class_counts[1]} samples ({train_class_counts[1]/len(y_imb_train):.1%})\")\n",
        "\n",
        "# Visualize class imbalance\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(['Class 0 (Majority)', 'Class 1 (Minority)'], [train_class_counts[0], train_class_counts[1]])\n",
        "plt.title('Class Distribution in Training Set')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply different resampling techniques\n",
        "\n",
        "# 1. Random Oversampling\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_ros, y_ros = ros.fit_resample(X_imb_train, y_imb_train)\n",
        "ros_class_counts = Counter(y_ros)\n",
        "\n",
        "# 2. SMOTE (Synthetic Minority Over-sampling Technique)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_smote, y_smote = smote.fit_resample(X_imb_train, y_imb_train)\n",
        "smote_class_counts = Counter(y_smote)\n",
        "\n",
        "# 3. Random Undersampling\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_rus, y_rus = rus.fit_resample(X_imb_train, y_imb_train)\n",
        "rus_class_counts = Counter(y_rus)\n",
        "\n",
        "# Visualize class distributions after resampling\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.bar(['Class 0', 'Class 1'], [train_class_counts[0], train_class_counts[1]])\n",
        "plt.title('Original Data')\n",
        "plt.ylabel('Number of Samples')\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.bar(['Class 0', 'Class 1'], [ros_class_counts[0], ros_class_counts[1]])\n",
        "plt.title('Random Oversampling')\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.bar(['Class 0', 'Class 1'], [smote_class_counts[0], smote_class_counts[1]])\n",
        "plt.title('SMOTE')\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.bar(['Class 0', 'Class 1'], [rus_class_counts[0], rus_class_counts[1]])\n",
        "plt.title('Random Undersampling')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print sample counts\n",
        "print(\"Sample counts after resampling:\")\n",
        "print(f\"Original - Class 0: {train_class_counts[0]}, Class 1: {train_class_counts[1]}, Total: {len(y_imb_train)}\")\n",
        "print(f\"Random Oversampling - Class 0: {ros_class_counts[0]}, Class 1: {ros_class_counts[1]}, Total: {len(y_ros)}\")\n",
        "print(f\"SMOTE - Class 0: {smote_class_counts[0]}, Class 1: {smote_class_counts[1]}, Total: {len(y_smote)}\")\n",
        "print(f\"Random Undersampling - Class 0: {rus_class_counts[0]}, Class 1: {rus_class_counts[1]}, Total: {len(y_rus)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's visualize the effect of SMOTE in feature space\n",
        "# We'll focus on just 2 features for visualization\n",
        "\n",
        "# Use PCA to reduce dimensions for visualization\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_imb_train_2d = pca.fit_transform(X_imb_train)\n",
        "X_smote_2d = pca.transform(X_smote)\n",
        "\n",
        "# Plot original vs SMOTE data\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_imb_train_2d[y_imb_train==0, 0], X_imb_train_2d[y_imb_train==0, 1], \n",
        "            label='Class 0 (Majority)', alpha=0.5, s=10)\n",
        "plt.scatter(X_imb_train_2d[y_imb_train==1, 0], X_imb_train_2d[y_imb_train==1, 1], \n",
        "            label='Class 1 (Minority)', alpha=0.8, s=30)\n",
        "plt.title('Original Imbalanced Data')\n",
        "plt.xlabel('PCA Feature 1')\n",
        "plt.ylabel('PCA Feature 2')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_smote_2d[y_smote==0, 0], X_smote_2d[y_smote==0, 1], \n",
        "            label='Class 0', alpha=0.5, s=10)\n",
        "plt.scatter(X_smote_2d[y_smote==1, 0], X_smote_2d[y_smote==1, 1], \n",
        "            label='Class 1 (with synthetic samples)', alpha=0.8, s=30)\n",
        "plt.title('After SMOTE Oversampling')\n",
        "plt.xlabel('PCA Feature 1')\n",
        "plt.ylabel('PCA Feature 2')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. Cross-Validation with Pipelines\n",
        "\n",
        "Robust evaluation of our preprocessing and model using cross-validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up cross-validation with our pipeline\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "# Return to our housing dataset\n",
        "X_full = X_messy.copy()\n",
        "\n",
        "# Create the pipeline\n",
        "preprocessing_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('selector', SelectKBest(f_regression, k=5)),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "\n",
        "# Set up cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(\n",
        "    preprocessing_pipeline, X_full, y, \n",
        "    cv=kf, scoring='r2'\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(\"Cross-Validation R² Scores:\")\n",
        "for i, score in enumerate(cv_scores):\n",
        "    print(f\"Fold {i+1}: {score:.4f}\")\n",
        "print(f\"\\nMean R²: {np.mean(cv_scores):.4f}\")\n",
        "print(f\"Standard Deviation: {np.std(cv_scores):.4f}\")\n",
        "\n",
        "# Visualize CV scores\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(range(1, len(cv_scores)+1), cv_scores)\n",
        "plt.axhline(y=np.mean(cv_scores), color='r', linestyle='--', label=f'Mean R² = {np.mean(cv_scores):.4f}')\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('R² Score')\n",
        "plt.title('Cross-Validation Results')\n",
        "plt.xticks(range(1, len(cv_scores)+1))\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 10. Conclusion\n",
        "\n",
        "In this notebook, we've explored comprehensive data preprocessing techniques:\n",
        "\n",
        "1. **Handling Missing Values**\n",
        "   - Simple imputation (mean, median, mode)\n",
        "   - KNN imputation\n",
        "\n",
        "2. **Feature Scaling**\n",
        "   - StandardScaler\n",
        "   - MinMaxScaler\n",
        "   - RobustScaler\n",
        "\n",
        "3. **Categorical Feature Encoding**\n",
        "   - Label encoding\n",
        "   - One-hot encoding\n",
        "\n",
        "4. **Feature Selection**\n",
        "   - SelectKBest with f_regression\n",
        "\n",
        "5. **Building Preprocessing Pipelines**\n",
        "   - ColumnTransformer for different column types\n",
        "   - Combining preprocessing steps with models\n",
        "\n",
        "6. **Handling Imbalanced Data**\n",
        "   - Random oversampling\n",
        "   - SMOTE\n",
        "   - Random undersampling\n",
        "\n",
        "7. **Cross-Validation with Pipelines**\n",
        "   - Evaluating preprocessing and model performance together\n",
        "\n",
        "These preprocessing techniques are essential for preparing data for machine learning models and ensuring reliable results.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
