{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Model Evaluation Metrics and Techniques\n",
        "\n",
        "This notebook covers model evaluation metrics and techniques for classification:\n",
        "- Accuracy, Precision, Recall, F1-Score\n",
        "- Confusion Matrix\n",
        "- ROC Curves and AUC\n",
        "- Cross-validation strategies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                            confusion_matrix, precision_recall_curve, roc_curve, auc,\n",
        "                            classification_report, roc_auc_score)\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('viridis')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Data Preparation\n",
        "\n",
        "Let's load and prepare a dataset to demonstrate various model evaluation techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name='target')\n",
        "\n",
        "# Display dataset information\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Number of classes: {len(np.unique(y))}\")\n",
        "print(f\"Class distribution:\\n{y.value_counts()}\")\n",
        "print(f\"Class names: {data.target_names}\")\n",
        "\n",
        "# Display first few rows\n",
        "X.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "# Scale features for models that benefit from scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Train Different Models\n",
        "\n",
        "Let's train different classification models that we'll use to demonstrate evaluation metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define and train different models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
        "    'SVM': SVC(kernel='rbf', C=1.0, probability=True, random_state=42)\n",
        "}\n",
        "\n",
        "# Dictionary to store predictions\n",
        "y_preds = {}\n",
        "y_pred_probs = {}\n",
        "\n",
        "# Train models and make predictions\n",
        "for name, model in models.items():\n",
        "    if name in ['Logistic Regression', 'SVM']:\n",
        "        # Models that benefit from scaling\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_preds[name] = model.predict(X_test_scaled)\n",
        "        y_pred_probs[name] = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    else:\n",
        "        # Models that don't require scaling\n",
        "        model.fit(X_train, y_train)\n",
        "        y_preds[name] = model.predict(X_test)\n",
        "        y_pred_probs[name] = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    print(f\"{name} model trained.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Basic Classification Metrics\n",
        "\n",
        "Let's calculate basic classification metrics for each model:\n",
        "- **Accuracy**: The proportion of correct predictions among the total number of predictions\n",
        "- **Precision**: The proportion of true positive predictions among all positive predictions (measures exactness)\n",
        "- **Recall**: The proportion of true positive predictions among all actual positives (measures completeness)\n",
        "- **F1-Score**: The harmonic mean of precision and recall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate metrics for each model\n",
        "metrics = pd.DataFrame(index=models.keys(), columns=['Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
        "\n",
        "for name, y_pred in y_preds.items():\n",
        "    metrics.loc[name, 'Accuracy'] = accuracy_score(y_test, y_pred)\n",
        "    metrics.loc[name, 'Precision'] = precision_score(y_test, y_pred)\n",
        "    metrics.loc[name, 'Recall'] = recall_score(y_test, y_pred)\n",
        "    metrics.loc[name, 'F1-Score'] = f1_score(y_test, y_pred)\n",
        "\n",
        "# Sort by F1-Score\n",
        "metrics = metrics.sort_values('F1-Score', ascending=False)\n",
        "\n",
        "# Display metrics\n",
        "metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the metrics\n",
        "metrics_melted = pd.melt(metrics.reset_index(), id_vars='index', \n",
        "                          value_vars=['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
        "                          var_name='Metric', value_name='Score')\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='index', y='Score', hue='Metric', data=metrics_melted)\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0.8, 1.0)  # Adjust y-axis for better visualization\n",
        "plt.legend(title='Metric')\n",
        "plt.xticks(rotation=30)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Confusion Matrix\n",
        "\n",
        "The confusion matrix provides a detailed breakdown of predictions:\n",
        "- **True Positives (TP)**: Correctly predicted positive cases\n",
        "- **True Negatives (TN)**: Correctly predicted negative cases\n",
        "- **False Positives (FP)**: Negative cases incorrectly predicted as positive (Type I error)\n",
        "- **False Negatives (FN)**: Positive cases incorrectly predicted as negative (Type II error)\n",
        "\n",
        "Let's visualize confusion matrices for each model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to plot confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, title, ax=None):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    \n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, \n",
        "               yticklabels=data.target_names, ax=ax)\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_ylabel('Actual')\n",
        "    ax.set_title(title)\n",
        "    \n",
        "# Plot confusion matrices for all models\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, (name, y_pred) in enumerate(y_preds.items()):\n",
        "    plot_confusion_matrix(y_test, y_pred, f'Confusion Matrix - {name}', axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. ROC Curves and AUC\n",
        "\n",
        "Receiver Operating Characteristic (ROC) curves plot the true positive rate (sensitivity) against the false positive rate (1-specificity) at various threshold settings. The Area Under the Curve (AUC) is a single value summary of the ROC curve that ranges from 0 to 1:\n",
        "\n",
        "- AUC = 0.5: Model has no discriminative ability (equivalent to random guessing)\n",
        "- AUC = 1.0: Perfect discrimination\n",
        "- AUC < 0.5: Model is worse than random guessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate ROC curves and AUC for each model\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for name, y_pred_prob in y_pred_probs.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "# Add the random guessing line\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random guessing')\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves for Different Models')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display AUC values\n",
        "auc_values = {}\n",
        "for name, y_pred_prob in y_pred_probs.items():\n",
        "    auc_values[name] = roc_auc_score(y_test, y_pred_prob)\n",
        "\n",
        "auc_df = pd.DataFrame.from_dict(auc_values, orient='index', columns=['AUC'])\n",
        "auc_df = auc_df.sort_values('AUC', ascending=False)\n",
        "auc_df\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Precision-Recall Curves\n",
        "\n",
        "Precision-Recall curves are particularly useful for imbalanced datasets, as they focus on the performance of the positive class rather than the negative class. The curves plot precision against recall at different threshold settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate precision-recall curves for each model\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for name, y_pred_prob in y_pred_probs.items():\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
        "    avg_precision = np.mean(precision)\n",
        "    plt.plot(recall, precision, label=f'{name} (Avg. Precision = {avg_precision:.3f})')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curves for Different Models')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Classification Reports\n",
        "\n",
        "Classification reports provide a comprehensive view of all the main classification metrics for each class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate classification reports for each model\n",
        "for name, y_pred in y_preds.items():\n",
        "    print(f\"Classification Report for {name}:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Cross-Validation\n",
        "\n",
        "Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The most common type is k-fold cross-validation, where the data is divided into k subsets, and the model is trained and evaluated k times, each time using a different subset as the test set and the remaining data as the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform 5-fold cross-validation on each model\n",
        "cv_results = {}\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for name, model in models.items():\n",
        "    if name in ['Logistic Regression', 'SVM']:\n",
        "        # Models that benefit from scaling\n",
        "        scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')\n",
        "    else:\n",
        "        # Models that don't require scaling\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
        "    \n",
        "    cv_results[name] = scores\n",
        "    \n",
        "    print(f\"{name} - Cross-validation scores: {scores}\")\n",
        "    print(f\"{name} - Mean CV score: {scores.mean():.4f} (Â±{scores.std():.4f})\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize cross-validation results\n",
        "cv_means = [scores.mean() for scores in cv_results.values()]\n",
        "cv_stds = [scores.std() for scores in cv_results.values()]\n",
        "model_names = list(cv_results.keys())\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(range(len(model_names)), cv_means, yerr=cv_stds, alpha=0.7, capsize=10)\n",
        "plt.xticks(range(len(model_names)), model_names, rotation=30)\n",
        "plt.ylabel('Mean Accuracy')\n",
        "plt.title('Cross-Validation Results (5-fold)')\n",
        "plt.ylim(0.9, 1.0)  # Adjust y-axis for better visualization\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. Summary and Key Takeaways\n",
        "\n",
        "### Classification Metrics\n",
        "- **Accuracy**: Simple but can be misleading for imbalanced data\n",
        "- **Precision/Recall**: Better for imbalanced data; focus on positive class performance\n",
        "- **F1-Score**: Harmonic mean of precision and recall\n",
        "- **Confusion Matrix**: Provides detailed breakdown of predictions\n",
        "- **ROC/AUC**: Evaluates model's ability to discriminate between classes\n",
        "  \n",
        "### Cross-Validation\n",
        "- Provides more robust evaluation by testing on multiple data splits\n",
        "- Reduces the risk of overfitting to a single train/test split\n",
        "- Helps identify model stability (low variance in scores is better)\n",
        "\n",
        "### Best Practices\n",
        "- Use multiple metrics to evaluate models, not just accuracy\n",
        "- Consider the specific problem context (e.g., cost of false positives vs. false negatives)\n",
        "- Cross-validate to ensure model robustness\n",
        "- For imbalanced data, use precision-recall curves and class-specific metrics\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Model Evaluation Techniques\n",
        "\n",
        "In this notebook, we'll explore various techniques for evaluating the performance of machine learning models, focusing on both classification and regression metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer, fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, \n",
        "    confusion_matrix, classification_report, roc_curve, auc,\n",
        "    precision_recall_curve, average_precision_score,\n",
        "    mean_squared_error, mean_absolute_error, r2_score\n",
        ")\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
