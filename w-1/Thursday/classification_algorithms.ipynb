{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Classification Algorithms\n",
        "\n",
        "This notebook covers implementation of various classification algorithms including:\n",
        "- Logistic Regression\n",
        "- Decision Trees\n",
        "- Random Forest\n",
        "- Support Vector Machines (SVMs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('viridis')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Loading and Preparing Dataset\n",
        "\n",
        "We'll use a common classification dataset from scikit-learn to demonstrate different algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a sample dataset (Breast Cancer Wisconsin dataset)\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name='target')\n",
        "\n",
        "# Display dataset information\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Number of classes: {len(np.unique(y))}\")\n",
        "print(f\"Class distribution:\\n{y.value_counts()}\")\n",
        "\n",
        "# Display first few rows of features\n",
        "X.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Logistic Regression\n",
        "\n",
        "Logistic Regression is a linear model for classification that predicts the probability of an instance belonging to a particular class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train a logistic regression model\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_log = log_reg.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Logistic Regression Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_log):.4f}\\n\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_log))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test, y_pred_log)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Logistic Regression')\n",
        "plt.show()\n",
        "\n",
        "# Feature importance (coefficients)\n",
        "coef_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': log_reg.coef_[0]\n",
        "}).sort_values('Coefficient', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Coefficient', y='Feature', data=coef_df.head(10))\n",
        "plt.title('Top 10 Features by Importance (Logistic Regression)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Decision Tree Classifier\n",
        "\n",
        "Decision Trees are non-parametric supervised learning methods that create a model that predicts the target by learning simple decision rules from the data features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train a decision tree classifier\n",
        "dt_clf = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_dt = dt_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Decision Tree Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\\n\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test, y_pred_dt)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Decision Tree')\n",
        "plt.show()\n",
        "\n",
        "# Visualize the decision tree\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(dt_clf, filled=True, feature_names=X.columns, class_names=data.target_names, rounded=True)\n",
        "plt.title('Decision Tree Visualization')\n",
        "plt.show()\n",
        "\n",
        "# Feature importance\n",
        "feat_imp_dt = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': dt_clf.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Importance', y='Feature', data=feat_imp_dt.head(10))\n",
        "plt.title('Top 10 Features by Importance (Decision Tree)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Random Forest Classifier\n",
        "\n",
        "Random Forests are ensemble learning methods that construct multiple decision trees during training and output the class that is the mode of the classes of the individual trees.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train a random forest classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Random Forest Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\\n\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test, y_pred_rf)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Random Forest')\n",
        "plt.show()\n",
        "\n",
        "# Feature importance\n",
        "feat_imp_rf = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf_clf.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Importance', y='Feature', data=feat_imp_rf.head(10))\n",
        "plt.title('Top 10 Features by Importance (Random Forest)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Support Vector Machine (SVM)\n",
        "\n",
        "SVMs are powerful classifiers that find the hyperplane that best separates the classes with the maximum margin.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train an SVM classifier\n",
        "svm_clf = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)\n",
        "svm_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_svm = svm_clf.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"SVM Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}\\n\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test, y_pred_svm)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - SVM')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Comparing Model Performance\n",
        "\n",
        "Let's compare the performance of all the classifiers we've implemented.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dataframe with model performance metrics\n",
        "models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'SVM']\n",
        "y_preds = [y_pred_log, y_pred_dt, y_pred_rf, y_pred_svm]\n",
        "\n",
        "# Calculate accuracy for each model\n",
        "accuracies = [accuracy_score(y_test, y_pred) for y_pred in y_preds]\n",
        "\n",
        "# Calculate precision, recall, and f1-score for class 1 (malignant)\n",
        "precisions, recalls, f1_scores = [], [], []\n",
        "\n",
        "for y_pred in y_preds:\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "    precisions.append(report['1']['precision'])\n",
        "    recalls.append(report['1']['recall'])\n",
        "    f1_scores.append(report['1']['f1-score'])\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': models,\n",
        "    'Accuracy': accuracies,\n",
        "    'Precision': precisions,\n",
        "    'Recall': recalls,\n",
        "    'F1-Score': f1_scores\n",
        "})\n",
        "\n",
        "# Display comparison table\n",
        "comparison_df.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model comparison\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "comparison_df_melted = pd.melt(comparison_df, id_vars='Model', value_vars=metrics, var_name='Metric', value_name='Score')\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.barplot(x='Model', y='Score', hue='Metric', data=comparison_df_melted)\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.ylim(0.8, 1.0)  # Adjust y-axis for better visualization\n",
        "plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Summary and Key Takeaways\n",
        "\n",
        "- **Logistic Regression**: A simple, interpretable linear model that performs well on linearly separable data.\n",
        "- **Decision Trees**: Intuitive, easy to interpret, but prone to overfitting without proper pruning.\n",
        "- **Random Forests**: Generally more robust than individual decision trees due to ensemble learning, but less interpretable.\n",
        "- **Support Vector Machines**: Powerful for high-dimensional spaces, but can be computationally expensive and less interpretable.\n",
        "\n",
        "Each algorithm has its strengths and weaknesses, and the choice of which to use depends on the specific problem, data characteristics, and requirements for interpretability and performance.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
