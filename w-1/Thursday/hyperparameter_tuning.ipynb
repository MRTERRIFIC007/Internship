{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "This notebook covers techniques for optimizing model performance through hyperparameter tuning:\n",
        "- Grid Search and Random Search\n",
        "- Validation and Learning Curves\n",
        "- Identifying and addressing Overfitting/Underfitting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from time import time\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.model_selection import learning_curve, validation_curve, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('viridis')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Data Preparation\n",
        "\n",
        "Let's load and prepare a dataset for our hyperparameter tuning examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name='target')\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Number of classes: {len(np.unique(y))}\")\n",
        "print(f\"Class distribution:\\n{y.value_counts()}\")\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Overfitting and Underfitting\n",
        "\n",
        "Overfitting and underfitting are common problems in machine learning:\n",
        "\n",
        "- **Underfitting**: Model is too simple, captures too little information, and performs poorly on both training and testing data\n",
        "- **Overfitting**: Model is too complex, captures noise, performs well on training data but poorly on testing data\n",
        "- **Just right**: Model has the right complexity, generalizes well to unseen data\n",
        "\n",
        "Let's explore learning curves to diagnose these issues.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=5,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \"\"\"\n",
        "    Generate 3 plots: the test and training learning curve, the training\n",
        "    samples vs fit times curve, the fit times vs score curve.\n",
        "    \"\"\"\n",
        "    if axes is None:\n",
        "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "    axes[0].set_title(title)\n",
        "    if ylim is not None:\n",
        "        axes[0].set_ylim(*ylim)\n",
        "    axes[0].set_xlabel(\"Training examples\")\n",
        "    axes[0].set_ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
        "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
        "                       train_sizes=train_sizes,\n",
        "                       return_times=True)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    fit_times_mean = np.mean(fit_times, axis=1)\n",
        "    fit_times_std = np.std(fit_times, axis=1)\n",
        "\n",
        "    # Plot learning curve\n",
        "    axes[0].grid()\n",
        "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                         color=\"r\")\n",
        "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                         color=\"g\")\n",
        "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "                 label=\"Training score\")\n",
        "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "                 label=\"Cross-validation score\")\n",
        "    axes[0].legend(loc=\"best\")\n",
        "\n",
        "    # Plot n_samples vs fit_times\n",
        "    axes[1].grid()\n",
        "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
        "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
        "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
        "    axes[1].set_xlabel(\"Training examples\")\n",
        "    axes[1].set_ylabel(\"fit_times\")\n",
        "    axes[1].set_title(\"Scalability of the model\")\n",
        "\n",
        "    # Plot fit_time vs score\n",
        "    axes[2].grid()\n",
        "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
        "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
        "    axes[2].set_xlabel(\"fit_times\")\n",
        "    axes[2].set_ylabel(\"Score\")\n",
        "    axes[2].set_title(\"Performance of the model\")\n",
        "\n",
        "    return plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create examples of underfitting, overfitting, and well-balanced models\n",
        "underfit_model = LogisticRegression(C=0.001, max_iter=1000)  # High regularization = underfitting\n",
        "overfit_model = DecisionTreeClassifier(max_depth=None)  # No depth limit = potential overfitting\n",
        "balanced_model = RandomForestClassifier(n_estimators=100, max_depth=3)  # Balanced complexity\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
        "\n",
        "# Plot learning curves for underfit model\n",
        "plot_learning_curve(underfit_model, \"Learning Curves (Logistic Regression with high regularization - Underfitting)\",\n",
        "                    X_train_scaled, y_train, axes=axes[0], ylim=(0.7, 1.01))\n",
        "\n",
        "# Plot learning curves for overfit model\n",
        "plot_learning_curve(overfit_model, \"Learning Curves (Decision Tree with no max_depth - Overfitting)\",\n",
        "                   X_train, y_train, axes=axes[1], ylim=(0.7, 1.01))\n",
        "\n",
        "# Plot learning curves for balanced model\n",
        "plot_learning_curve(balanced_model, \"Learning Curves (Random Forest with max_depth=3 - Balanced)\",\n",
        "                   X_train, y_train, axes=axes[2], ylim=(0.7, 1.01))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Grid Search for Hyperparameter Tuning\n",
        "\n",
        "Grid Search is an exhaustive search that tries all combinations of hyperparameters specified in a grid.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define hyperparameter grid for logistic regression\n",
        "param_grid_log = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Create and fit the grid search\n",
        "grid_search_log = GridSearchCV(\n",
        "    LogisticRegression(max_iter=1000, random_state=42),\n",
        "    param_grid_log,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    return_train_score=True,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Time the grid search\n",
        "start_time = time()\n",
        "grid_search_log.fit(X_train_scaled, y_train)\n",
        "end_time = time()\n",
        "\n",
        "print(f\"GridSearchCV took {end_time - start_time:.2f} seconds for {len(grid_search_log.cv_results_['params'])} candidates parameter settings.\")\n",
        "\n",
        "# Get the best parameters and score\n",
        "best_params_log = grid_search_log.best_params_\n",
        "best_score_log = grid_search_log.best_score_\n",
        "\n",
        "print(f\"Best parameters: {best_params_log}\")\n",
        "print(f\"Best cross-validation score: {best_score_log:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model_log = grid_search_log.best_estimator_\n",
        "y_pred_log = best_model_log.predict(X_test_scaled)\n",
        "test_accuracy_log = accuracy_score(y_test, y_pred_log)\n",
        "\n",
        "print(f\"Test accuracy with best model: {test_accuracy_log:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_log))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize grid search results\n",
        "results = pd.DataFrame(grid_search_log.cv_results_)\n",
        "\n",
        "# Plot mean test scores for different values of C, grouped by solver and penalty\n",
        "plt.figure(figsize=(12, 8))\n",
        "for solver in ['liblinear', 'saga']:\n",
        "    for penalty in ['l1', 'l2']:\n",
        "        mask = (results['param_solver'] == solver) & (results['param_penalty'] == penalty)\n",
        "        plt.plot(results.loc[mask, 'param_C'], results.loc[mask, 'mean_test_score'], \n",
        "                marker='o', label=f\"{solver}, {penalty}\")\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.xlabel('C (regularization parameter)')\n",
        "plt.ylabel('Mean test accuracy')\n",
        "plt.title('Grid Search Results: Logistic Regression')\n",
        "plt.legend(title='Solver, Penalty')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Random Search for Hyperparameter Tuning\n",
        "\n",
        "Random Search samples random combinations from the hyperparameter space, which can be more efficient than Grid Search when dealing with many hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define hyperparameter space for Random Forest\n",
        "param_dist_rf = {\n",
        "    'n_estimators': [10, 50, 100, 200],\n",
        "    'max_depth': [None, 5, 10, 15, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Create and fit the random search\n",
        "random_search_rf = RandomizedSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    param_distributions=param_dist_rf,\n",
        "    n_iter=20,  # Number of parameter settings sampled\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    return_train_score=True,\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Time the random search\n",
        "start_time = time()\n",
        "random_search_rf.fit(X_train, y_train)\n",
        "end_time = time()\n",
        "\n",
        "print(f\"RandomizedSearchCV took {end_time - start_time:.2f} seconds for {random_search_rf.n_iter} candidates parameter settings.\")\n",
        "\n",
        "# Get the best parameters and score\n",
        "best_params_rf = random_search_rf.best_params_\n",
        "best_score_rf = random_search_rf.best_score_\n",
        "\n",
        "print(f\"Best parameters: {best_params_rf}\")\n",
        "print(f\"Best cross-validation score: {best_score_rf:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model_rf = random_search_rf.best_estimator_\n",
        "y_pred_rf = best_model_rf.predict(X_test)\n",
        "test_accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"Test accuracy with best model: {test_accuracy_rf:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize random search results\n",
        "results_rf = pd.DataFrame(random_search_rf.cv_results_)\n",
        "\n",
        "# Sort by test score\n",
        "results_rf = results_rf.sort_values('mean_test_score', ascending=False)\n",
        "\n",
        "# Plot top 10 results\n",
        "plt.figure(figsize=(15, 8))\n",
        "top_results = results_rf.head(10).reset_index(drop=True)\n",
        "\n",
        "# Create a list to store parameter values as text\n",
        "param_text = []\n",
        "for i, params in enumerate(top_results['params']):\n",
        "    text = \", \".join([f\"{k}={v}\" for k, v in params.items() \n",
        "                     if k in ['n_estimators', 'max_depth']])\n",
        "    param_text.append(text)\n",
        "\n",
        "plt.barh(range(len(top_results)), top_results['mean_test_score'], xerr=top_results['std_test_score'])\n",
        "plt.yticks(range(len(top_results)), param_text)\n",
        "plt.xlabel('Mean Test Accuracy')\n",
        "plt.title('Top 10 Parameter Combinations (Random Search)')\n",
        "plt.xlim(0.9, 1.0)\n",
        "plt.gca().invert_yaxis()  # Highest score at the top\n",
        "plt.grid(axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Validation Curves\n",
        "\n",
        "Validation curves help us visualize how model performance changes with a specific hyperparameter, which is useful for identifying optimal parameter values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create validation curve for SVM's C parameter\n",
        "param_range = np.logspace(-3, 3, 7)\n",
        "train_scores, test_scores = validation_curve(\n",
        "    SVC(kernel='rbf', gamma='scale', random_state=42),\n",
        "    X_train_scaled, y_train,\n",
        "    param_name=\"C\",\n",
        "    param_range=param_range,\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Calculate mean and std of training and test scores\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "# Plot validation curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Validation Curve with SVM (RBF kernel)\")\n",
        "plt.xlabel(\"C (regularization parameter)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xscale(\"log\")\n",
        "plt.grid()\n",
        "\n",
        "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
        "                 train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
        "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
        "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "plt.plot(param_range, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
        "plt.plot(param_range, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "\n",
        "# Find the optimal C value\n",
        "best_c_index = np.argmax(test_scores_mean)\n",
        "best_c = param_range[best_c_index]\n",
        "print(f\"Optimal C parameter: {best_c}\")\n",
        "print(f\"Corresponding mean test accuracy: {test_scores_mean[best_c_index]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Putting It All Together: Building an Optimal Model\n",
        "\n",
        "Let's combine our findings to build and evaluate the best SVM model with the optimal parameters we've discovered.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the optimal SVM model with the best C parameter\n",
        "optimal_svm = SVC(kernel='rbf', C=best_c, gamma='scale', probability=True, random_state=42)\n",
        "optimal_svm.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred_svm = optimal_svm.predict(X_test_scaled)\n",
        "test_accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "\n",
        "print(f\"Test accuracy with optimal SVM model: {test_accuracy_svm:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test, y_pred_svm)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Optimal SVM Model')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Compare All Optimized Models\n",
        "\n",
        "Let's compare the performance of all the models we've optimized.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dataframe with performance metrics for all optimized models\n",
        "models = ['Logistic Regression (Grid Search)', 'Random Forest (Random Search)', 'SVM (Validation Curve)']\n",
        "y_preds = [y_pred_log, y_pred_rf, y_pred_svm]\n",
        "\n",
        "# Calculate performance metrics\n",
        "accuracies = [accuracy_score(y_test, y_pred) for y_pred in y_preds]\n",
        "\n",
        "# Calculate precision, recall, and f1-score for class 1 (malignant)\n",
        "precisions, recalls, f1_scores = [], [], []\n",
        "\n",
        "for y_pred in y_preds:\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "    precisions.append(report['1']['precision'])\n",
        "    recalls.append(report['1']['recall'])\n",
        "    f1_scores.append(report['1']['f1-score'])\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': models,\n",
        "    'Accuracy': accuracies,\n",
        "    'Precision': precisions,\n",
        "    'Recall': recalls,\n",
        "    'F1-Score': f1_scores\n",
        "})\n",
        "\n",
        "# Display comparison table\n",
        "comparison_df.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Visualize model comparison\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "comparison_df_melted = pd.melt(comparison_df, id_vars='Model', value_vars=metrics, var_name='Metric', value_name='Score')\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.barplot(x='Model', y='Score', hue='Metric', data=comparison_df_melted)\n",
        "plt.title('Performance Comparison of Optimized Models')\n",
        "plt.ylim(0.9, 1.0)  # Adjust y-axis for better visualization\n",
        "plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Summary and Key Takeaways\n",
        "\n",
        "- **Overfitting and Underfitting**: Learning curves help diagnose these issues. In underfitting, both training and validation scores are low. In overfitting, training score is high but validation score is low.\n",
        "\n",
        "- **Grid Search**: Systematically explores all combinations of hyperparameter values, which is thorough but can be computationally expensive.\n",
        "\n",
        "- **Random Search**: Samples random combinations from the hyperparameter space, which is more efficient for high-dimensional parameter spaces.\n",
        "\n",
        "- **Validation Curves**: Visualize model performance across different values of a specific hyperparameter, helping identify optimal values.\n",
        "\n",
        "- **Cross-Validation**: Essential for robust model evaluation and hyperparameter tuning to avoid overfitting to the test set.\n",
        "\n",
        "- **Best Practices**:\n",
        "  - Start with a broad search and then refine\n",
        "  - Use random search for high-dimensional parameter spaces\n",
        "  - Consider the computational cost vs. benefit\n",
        "  - Always evaluate final model on a separate test set\n",
        "  - Monitor both underfitting and overfitting\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
