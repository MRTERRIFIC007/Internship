{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Project Setup: Dataset Selection, Problem Definition, and Initial Analysis\n",
        "\n",
        "This notebook outlines the setup of our classification project:\n",
        "1. Dataset selection and exploration\n",
        "2. Problem definition and objectives\n",
        "3. Project structure\n",
        "4. Initial data analysis and insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('viridis')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Dataset Selection\n",
        "\n",
        "For this project, we'll use the Credit Card Fraud Detection dataset from Kaggle. This dataset presents transactions made by credit cards, with the goal of identifying fraudulent transactions.\n",
        "\n",
        "The dataset is highly imbalanced, with frauds accounting for a very small percentage of all transactions, making it a realistic and challenging classification problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normally, you would download the dataset from Kaggle or another source\n",
        "# For this example, we'll simulate loading it (using a smaller sample for demonstration)\n",
        "# In a real project, you would use:\n",
        "# df = pd.read_csv('creditcard.csv')\n",
        "\n",
        "# Create a sample dataset for demonstration\n",
        "np.random.seed(42)\n",
        "n_samples = 10000\n",
        "n_features = 30\n",
        "\n",
        "# Create feature columns (V1-V28 plus Time and Amount)\n",
        "cols = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
        "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
        "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
        "\n",
        "# Generate feature data\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "\n",
        "# Generate target variable (fraud=1, normal=0) with imbalance (0.2% fraud)\n",
        "fraud_ratio = 0.002\n",
        "n_fraud = int(n_samples * fraud_ratio)\n",
        "y = np.zeros(n_samples)\n",
        "fraud_indices = np.random.choice(range(n_samples), size=n_fraud, replace=False)\n",
        "y[fraud_indices] = 1\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(X, columns=cols)\n",
        "df['Class'] = y\n",
        "\n",
        "# Make Time and Amount more realistic\n",
        "df['Time'] = np.random.uniform(0, 172800, n_samples)  # Time in seconds (2 days)\n",
        "df['Amount'] = np.exp(np.random.normal(3, 1, n_samples))  # Log-normal distribution for amounts\n",
        "\n",
        "# Display info about the dataset\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Number of fraudulent transactions: {df['Class'].sum()}\")\n",
        "print(f\"Fraud percentage: {df['Class'].mean() * 100:.3f}%\")\n",
        "\n",
        "# Display first few rows\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Problem Definition\n",
        "\n",
        "### Business Context\n",
        "Credit card fraud costs consumers and financial companies billions of dollars annually. Building effective fraud detection systems is crucial for:\n",
        "- Reducing financial losses\n",
        "- Protecting customers\n",
        "- Maintaining trust in the payment system\n",
        "- Reducing operational costs of manual reviews\n",
        "\n",
        "### Objective\n",
        "Build a machine learning model that can accurately identify fraudulent transactions while minimizing false positives (legitimate transactions flagged as fraud).\n",
        "\n",
        "### Success Metrics\n",
        "1. **Primary Metrics**:\n",
        "   - High recall (sensitivity) for fraud detection (minimize missed frauds)\n",
        "   - Acceptable precision (minimize false alarms)\n",
        "   - Good ROC-AUC and PR-AUC scores\n",
        "\n",
        "2. **Business Constraints**:\n",
        "   - Model must be explainable to some degree\n",
        "   - Fast inference time for real-time detection\n",
        "   - Ability to handle class imbalance\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Project Structure\n",
        "\n",
        "Let's define the directory structure and components for our project:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define project structure\n",
        "project_structure = {\n",
        "    'data': ['raw', 'processed', 'interim'],\n",
        "    'notebooks': ['01_exploratory_analysis.ipynb', \n",
        "                 '02_preprocessing_pipeline.ipynb',\n",
        "                 '03_baseline_models.ipynb',\n",
        "                 '04_model_comparison.ipynb',\n",
        "                 '05_advanced_optimization.ipynb',\n",
        "                 '06_deployment_pipeline.ipynb'],\n",
        "    'src': ['data', 'features', 'models', 'visualization', 'utils'],\n",
        "    'models': ['saved_models'],\n",
        "    'reports': ['figures', 'final_report.md'],\n",
        "    'app': ['api.py', 'templates', 'static']\n",
        "}\n",
        "\n",
        "# Print project structure\n",
        "print(\"Credit Card Fraud Detection Project Structure:\")\n",
        "print(\"=\" * 50)\n",
        "for directory, contents in project_structure.items():\n",
        "    print(f\"/{directory}\")\n",
        "    for item in contents:\n",
        "        print(f\"  ├── {item}\")\n",
        "    print()\n",
        "\n",
        "# In a real project, you would create these directories\n",
        "# But we'll skip that for this notebook\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Initial Data Analysis\n",
        "\n",
        "Let's perform some initial exploratory analysis to better understand our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"Dataset summary statistics:\")\n",
        "print(df.describe().T)\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(missing_values[missing_values > 0] if missing_values.sum() > 0 else \"No missing values found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize class distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='Class', data=df)\n",
        "plt.title('Class Distribution (0: Normal, 1: Fraud)')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Add count labels on the bars\n",
        "for p in plt.gca().patches:\n",
        "    plt.gca().annotate(f'{p.get_height()}', \n",
        "                      (p.get_x() + p.get_width() / 2., p.get_height()), \n",
        "                      ha='center', va='center', \n",
        "                      xytext=(0, 10), \n",
        "                      textcoords='offset points')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Show percentage\n",
        "print(f\"Normal transactions: {(df['Class'] == 0).sum()} ({(1 - df['Class'].mean()) * 100:.3f}%)\")\n",
        "print(f\"Fraudulent transactions: {(df['Class'] == 1).sum()} ({df['Class'].mean() * 100:.3f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze transaction amounts\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Normal transactions\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df[df['Class'] == 0]['Amount'], bins=50, kde=True)\n",
        "plt.title('Amount Distribution - Normal Transactions')\n",
        "plt.xlabel('Transaction Amount')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlim(0, 500)  # Focus on the main distribution\n",
        "\n",
        "# Fraudulent transactions\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df[df['Class'] == 1]['Amount'], bins=50, kde=True, color='red')\n",
        "plt.title('Amount Distribution - Fraudulent Transactions')\n",
        "plt.xlabel('Transaction Amount')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlim(0, 500)  # Focus on the main distribution\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare statistics of normal vs fraud transactions\n",
        "print(\"Normal transactions amount statistics:\")\n",
        "print(df[df['Class'] == 0]['Amount'].describe())\n",
        "\n",
        "print(\"\\nFraudulent transactions amount statistics:\")\n",
        "print(df[df['Class'] == 1]['Amount'].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze time distribution\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Convert time to hours for better interpretation\n",
        "df['TimeHour'] = df['Time'] / 3600  # Convert seconds to hours\n",
        "\n",
        "# Plot time distribution by class\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df[df['Class'] == 0]['TimeHour'], bins=48, kde=True, \n",
        "            label='Normal', alpha=0.7)\n",
        "plt.title('Time Distribution - Normal Transactions')\n",
        "plt.xlabel('Time (hours)')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df[df['Class'] == 1]['TimeHour'], bins=48, kde=True, \n",
        "            color='red', label='Fraud', alpha=0.7)\n",
        "plt.title('Time Distribution - Fraudulent Transactions')\n",
        "plt.xlabel('Time (hours)')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Check correlation between Time and Fraud\n",
        "time_fraud_corr = df[['TimeHour', 'Class']].corr().iloc[0, 1]\n",
        "print(f\"Correlation between Time and Fraud: {time_fraud_corr:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature correlation with target\n",
        "# Select only a subset of features for clarity\n",
        "selected_features = ['V1', 'V2', 'V3', 'V4', 'V5', 'Amount', 'Class']\n",
        "correlation = df[selected_features].corr()\n",
        "\n",
        "# Plot correlation heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "plt.title('Correlation Matrix for Selected Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find top features correlated with fraud\n",
        "feature_corrs = df.drop(['TimeHour'], axis=1).corr()['Class'].sort_values(ascending=False)\n",
        "print(\"Top positively correlated features with fraud:\")\n",
        "print(feature_corrs.head())\n",
        "print(\"\\nTop negatively correlated features with fraud:\")\n",
        "print(feature_corrs.tail())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Initial Train-Test Split\n",
        "\n",
        "Let's create an initial train-test split to prepare for model development.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features and target\n",
        "X = df.drop(['Class', 'TimeHour'], axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "# Print split information\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"  - Normal transactions: {(y_train == 0).sum()} ({(y_train == 0).sum() / len(y_train) * 100:.2f}%)\")\n",
        "print(f\"  - Fraudulent transactions: {(y_train == 1).sum()} ({(y_train == 1).sum() / len(y_train) * 100:.2f}%)\")\n",
        "print(f\"\\nTest set: {X_test.shape[0]} samples\")\n",
        "print(f\"  - Normal transactions: {(y_test == 0).sum()} ({(y_test == 0).sum() / len(y_test) * 100:.2f}%)\")\n",
        "print(f\"  - Fraudulent transactions: {(y_test == 1).sum()} ({(y_test == 1).sum() / len(y_test) * 100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Key Insights and Next Steps\n",
        "\n",
        "### Key Insights from Initial Analysis:\n",
        "1. **Class Imbalance**: The dataset is highly imbalanced, with fraud transactions making up only about 0.2% of all transactions. This will require specialized techniques such as:\n",
        "   - Resampling methods (oversampling, undersampling)\n",
        "   - Cost-sensitive learning\n",
        "   - Anomaly detection approaches\n",
        "\n",
        "2. **Amount Distribution**: Fraudulent transactions may have different amount patterns compared to normal ones. This feature will likely be important.\n",
        "\n",
        "3. **Time Patterns**: There appears to be some temporal pattern in fraudulent transactions. Time-based features might be useful.\n",
        "\n",
        "4. **Feature Correlations**: Several features show strong correlations with the target class, suggesting good predictive potential.\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **Preprocessing Pipeline**:\n",
        "   - Feature scaling (standardization/normalization)\n",
        "   - Handling class imbalance \n",
        "   - Feature engineering (time-based features, amount transformations)\n",
        "\n",
        "2. **Baseline Models**:\n",
        "   - Logistic Regression\n",
        "   - Decision Trees\n",
        "   - Random Forest\n",
        "   - Support Vector Machines\n",
        "\n",
        "3. **Model Evaluation Strategy**:\n",
        "   - Use appropriate metrics (precision, recall, F1, AUC)\n",
        "   - Implement cross-validation\n",
        "   - Set up a proper validation scheme\n",
        "\n",
        "4. **Advanced Techniques**:\n",
        "   - Ensemble methods\n",
        "   - Cost-sensitive learning\n",
        "   - Feature selection\n",
        "   - Hyperparameter tuning\n",
        "\n",
        "5. **Deployment Considerations**:\n",
        "   - Model serialization\n",
        "   - API development\n",
        "   - Monitoring system\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
