{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Text Generator Using RNNs - Part 2: Model Building and Training\n",
        "\n",
        "## Learning Objectives\n",
        "In this notebook, we'll build upon Part 1 to:\n",
        "- Design RNN architectures for text generation\n",
        "- Compile and configure models for training\n",
        "- Implement training strategies for sequence models\n",
        "- Explore temperature-based sampling techniques\n",
        "- Generate text using trained models\n",
        "\n",
        "## Recap from Part 1\n",
        "We've prepared our data with:\n",
        "- Text preprocessing and character vocabulary\n",
        "- Sequence generation with sliding windows\n",
        "- One-hot encoding for categorical data\n",
        "\n",
        "Now we'll build and train the actual RNN model!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries and reproduce data from Part 1\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import seaborn as sns\n",
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreate data from Part 1 (compact version)\n",
        "sample_text = \"\"\"\n",
        "To be or not to be, that is the question:\n",
        "Whether 'tis nobler in the mind to suffer\n",
        "The slings and arrows of outrageous fortune,\n",
        "Or to take arms against a sea of troubles\n",
        "And by opposing end them. To die—to sleep,\n",
        "No more; and by a sleep to say we end\n",
        "The heart-ache and the thousand natural shocks\n",
        "That flesh is heir to: 'tis a consummation\n",
        "Devoutly to be wish'd. To die, to sleep;\n",
        "To sleep, perchance to dream—ay, there's the rub:\n",
        "For in that sleep of death what dreams may come,\n",
        "When we have shuffled off this mortal coil,\n",
        "Must give us pause. There's the respect\n",
        "That makes calamity of so long life.\n",
        "\n",
        "The quick brown fox jumps over the lazy dog.\n",
        "Machine learning is revolutionizing the way we process data.\n",
        "Deep neural networks can learn complex patterns from large datasets.\n",
        "Artificial intelligence will transform various industries in the coming years.\n",
        "Natural language processing enables computers to understand human language.\n",
        "The future of technology lies in the intersection of AI and human creativity.\n",
        "\"\"\"\n",
        "\n",
        "# Preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s\\.\\,\\;\\:\\!\\?\\-\\'\\\"]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "processed_text = preprocess_text(sample_text)\n",
        "\n",
        "# Create vocabulary\n",
        "chars = sorted(list(set(processed_text)))\n",
        "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(f\"Text length: {len(processed_text)} characters\")\n",
        "print(f\"Vocabulary size: {vocab_size} unique characters\")\n",
        "print(f\"Characters: {chars}\")\n",
        "\n",
        "# Create sequences\n",
        "sequence_length = 40\n",
        "step = 3\n",
        "\n",
        "# Encode text\n",
        "encoded_text = [char_to_idx[char] for char in processed_text]\n",
        "\n",
        "# Create training sequences\n",
        "sequences = []\n",
        "targets = []\n",
        "\n",
        "for i in range(0, len(encoded_text) - sequence_length, step):\n",
        "    sequences.append(encoded_text[i:i + sequence_length])\n",
        "    targets.append(encoded_text[i + sequence_length])\n",
        "\n",
        "X = np.array(sequences)\n",
        "y = np.array(targets)\n",
        "\n",
        "print(f\"Training sequences shape: {X.shape}\")\n",
        "print(f\"Training targets shape: {y.shape}\")\n",
        "print(f\"Number of training examples: {len(X)}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. RNN Architecture Design for Text Generation\n",
        "\n",
        "For text generation, we have several architectural choices:\n",
        "1. **Simple RNN**: Basic recurrent layer (prone to vanishing gradients)\n",
        "2. **LSTM**: Better for longer sequences and complex patterns\n",
        "3. **GRU**: Simpler than LSTM but often comparable performance\n",
        "4. **Stacked RNNs**: Multiple layers for increased capacity\n",
        "\n",
        "We'll implement and compare different architectures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text Generation Model Architectures\n",
        "def create_simple_rnn_generator(vocab_size, sequence_length, embedding_dim=256, rnn_units=512):\n",
        "    \"\"\"\n",
        "    Create a simple RNN-based text generator\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Embedding(vocab_size, embedding_dim, input_length=sequence_length),\n",
        "        keras.layers.SimpleRNN(rnn_units, dropout=0.2, recurrent_dropout=0.2),\n",
        "        keras.layers.Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def create_lstm_generator(vocab_size, sequence_length, embedding_dim=256, lstm_units=512):\n",
        "    \"\"\"\n",
        "    Create an LSTM-based text generator\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Embedding(vocab_size, embedding_dim, input_length=sequence_length),\n",
        "        keras.layers.LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2),\n",
        "        keras.layers.Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def create_gru_generator(vocab_size, sequence_length, embedding_dim=256, gru_units=512):\n",
        "    \"\"\"\n",
        "    Create a GRU-based text generator\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Embedding(vocab_size, embedding_dim, input_length=sequence_length),\n",
        "        keras.layers.GRU(gru_units, dropout=0.2, recurrent_dropout=0.2),\n",
        "        keras.layers.Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def create_stacked_lstm_generator(vocab_size, sequence_length, embedding_dim=256, lstm_units=512):\n",
        "    \"\"\"\n",
        "    Create a stacked LSTM text generator\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Embedding(vocab_size, embedding_dim, input_length=sequence_length),\n",
        "        keras.layers.LSTM(lstm_units, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
        "        keras.layers.LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2),\n",
        "        keras.layers.Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create different model architectures\n",
        "print(\"Creating different RNN architectures...\")\n",
        "\n",
        "models = {\n",
        "    'Simple RNN': create_simple_rnn_generator(vocab_size, sequence_length, embedding_dim=128, rnn_units=256),\n",
        "    'LSTM': create_lstm_generator(vocab_size, sequence_length, embedding_dim=128, lstm_units=256),\n",
        "    'GRU': create_gru_generator(vocab_size, sequence_length, embedding_dim=128, gru_units=256),\n",
        "    'Stacked LSTM': create_stacked_lstm_generator(vocab_size, sequence_length, embedding_dim=128, lstm_units=256)\n",
        "}\n",
        "\n",
        "# Display model architectures\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{name} Architecture:\")\n",
        "    print(f\"Parameters: {model.count_params():,}\")\n",
        "    model.summary()\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Compare model complexities\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Parameter comparison\n",
        "plt.subplot(2, 2, 1)\n",
        "param_counts = [model.count_params() for model in models.values()]\n",
        "model_names = list(models.keys())\n",
        "colors = ['skyblue', 'lightcoral', 'lightgreen', 'orange']\n",
        "\n",
        "bars = plt.bar(model_names, param_counts, color=colors)\n",
        "plt.title('Model Complexity (Parameters)')\n",
        "plt.ylabel('Number of Parameters')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, count in zip(bars, param_counts):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(param_counts)*0.01,\n",
        "             f'{count:,}', ha='center', va='bottom')\n",
        "\n",
        "# Memory usage estimation\n",
        "plt.subplot(2, 2, 2)\n",
        "# Rough estimation: 4 bytes per parameter (float32)\n",
        "memory_mb = [params * 4 / (1024 * 1024) for params in param_counts]\n",
        "plt.bar(model_names, memory_mb, color=colors)\n",
        "plt.title('Estimated Memory Usage')\n",
        "plt.ylabel('Memory (MB)')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Training speed comparison (theoretical)\n",
        "plt.subplot(2, 2, 3)\n",
        "# Relative complexity (normalized)\n",
        "complexity_scores = [1.0, 1.5, 1.2, 2.0]  # Relative to Simple RNN\n",
        "plt.bar(model_names, complexity_scores, color=colors)\n",
        "plt.title('Relative Training Complexity')\n",
        "plt.ylabel('Complexity Score')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Architecture visualization\n",
        "plt.subplot(2, 2, 4)\n",
        "layer_counts = [\n",
        "    len([l for l in model.layers if 'rnn' in l.name.lower() or 'lstm' in l.name.lower() or 'gru' in l.name.lower()])\n",
        "    for model in models.values()\n",
        "]\n",
        "plt.bar(model_names, layer_counts, color=colors)\n",
        "plt.title('Number of Recurrent Layers')\n",
        "plt.ylabel('Layer Count')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Choose LSTM for detailed training (good balance of performance and stability)\n",
        "selected_model = models['LSTM']\n",
        "print(f\"\\nSelected model: LSTM\")\n",
        "print(f\"Total parameters: {selected_model.count_params():,}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Model Compilation and Training\n",
        "\n",
        "Now we'll compile our model with appropriate loss function and optimizer, then train it on our prepared sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model compilation and training\n",
        "print(\"Compiling model...\")\n",
        "selected_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Split data for training and validation\n",
        "split_idx = int(0.8 * len(X))\n",
        "X_train, X_val = X[:split_idx], X[split_idx:]\n",
        "y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
        "\n",
        "# Training configuration\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "\n",
        "# Custom callback to generate text during training\n",
        "class TextGenerationCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, seed_text=\"to be or not to be\", generate_length=100):\n",
        "        self.seed_text = seed_text\n",
        "        self.generate_length = generate_length\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch % 10 == 0:  # Generate text every 10 epochs\n",
        "            generated = self.generate_text()\n",
        "            print(f\"\\nEpoch {epoch} - Generated text:\")\n",
        "            print(f\"'{generated}'\")\n",
        "            print(\"-\" * 50)\n",
        "    \n",
        "    def generate_text(self):\n",
        "        # Simple greedy generation\n",
        "        current_sequence = [char_to_idx.get(c, 0) for c in self.seed_text[-sequence_length:]]\n",
        "        if len(current_sequence) < sequence_length:\n",
        "            current_sequence = [0] * (sequence_length - len(current_sequence)) + current_sequence\n",
        "        \n",
        "        generated = self.seed_text\n",
        "        \n",
        "        for _ in range(self.generate_length):\n",
        "            # Predict next character\n",
        "            x_pred = np.array([current_sequence])\n",
        "            pred = self.model.predict(x_pred, verbose=0)\n",
        "            next_char_idx = np.argmax(pred[0])\n",
        "            next_char = idx_to_char.get(next_char_idx, '')\n",
        "            \n",
        "            generated += next_char\n",
        "            \n",
        "            # Update sequence\n",
        "            current_sequence = current_sequence[1:] + [next_char_idx]\n",
        "        \n",
        "        return generated\n",
        "\n",
        "# Create callbacks\n",
        "text_callback = TextGenerationCallback(seed_text=\"to be or not\", generate_length=50)\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5, min_lr=1e-6)\n",
        "\n",
        "print(\"Starting training...\")\n",
        "history = selected_model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[text_callback, early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Temperature-Based Sampling and Text Generation\n",
        "\n",
        "Temperature sampling allows us to control the randomness in text generation:\n",
        "- **Low temperature (< 1.0)**: More conservative, repetitive text\n",
        "- **High temperature (> 1.0)**: More creative, potentially incoherent text\n",
        "- **Temperature = 1.0**: Standard sampling from the probability distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced text generation with temperature sampling\n",
        "def generate_text_with_temperature(model, seed_text, length=200, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generate text using temperature sampling\n",
        "    \n",
        "    Parameters:\n",
        "    model: trained RNN model\n",
        "    seed_text: starting text\n",
        "    length: number of characters to generate\n",
        "    temperature: sampling temperature (higher = more random)\n",
        "    \"\"\"\n",
        "    # Prepare seed sequence\n",
        "    current_sequence = [char_to_idx.get(c, 0) for c in seed_text[-sequence_length:]]\n",
        "    if len(current_sequence) < sequence_length:\n",
        "        current_sequence = [0] * (sequence_length - len(current_sequence)) + current_sequence\n",
        "    \n",
        "    generated = seed_text\n",
        "    \n",
        "    for _ in range(length):\n",
        "        # Get prediction\n",
        "        x_pred = np.array([current_sequence])\n",
        "        predictions = model.predict(x_pred, verbose=0)[0]\n",
        "        \n",
        "        # Apply temperature\n",
        "        if temperature == 0:\n",
        "            # Greedy sampling\n",
        "            next_char_idx = np.argmax(predictions)\n",
        "        else:\n",
        "            # Temperature sampling\n",
        "            predictions = predictions / temperature\n",
        "            predictions = np.exp(predictions) / np.sum(np.exp(predictions))\n",
        "            next_char_idx = np.random.choice(len(predictions), p=predictions)\n",
        "        \n",
        "        next_char = idx_to_char.get(next_char_idx, '')\n",
        "        generated += next_char\n",
        "        \n",
        "        # Update sequence\n",
        "        current_sequence = current_sequence[1:] + [next_char_idx]\n",
        "    \n",
        "    return generated\n",
        "\n",
        "# Visualize training progress\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Training and validation loss\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Training and validation accuracy\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Learning rate (if available)\n",
        "plt.subplot(2, 3, 3)\n",
        "if 'lr' in history.history:\n",
        "    plt.plot(history.history['lr'])\n",
        "    plt.title('Learning Rate Schedule')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.yscale('log')\n",
        "else:\n",
        "    plt.text(0.5, 0.5, 'Learning Rate\\nSchedule\\n(Not recorded)', \n",
        "             ha='center', va='center', transform=plt.gca().transAxes)\n",
        "    plt.title('Learning Rate Schedule')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Generate text with different temperatures\n",
        "temperatures = [0.2, 0.5, 1.0, 1.2]\n",
        "seed = \"to be or not to be\"\n",
        "\n",
        "plt.subplot(2, 3, 4)\n",
        "temp_samples = []\n",
        "for temp in temperatures:\n",
        "    sample = generate_text_with_temperature(selected_model, seed, length=50, temperature=temp)\n",
        "    temp_samples.append(len(set(sample)))  # Unique characters as diversity measure\n",
        "\n",
        "plt.bar(range(len(temperatures)), temp_samples, color='lightcoral')\n",
        "plt.xlabel('Temperature')\n",
        "plt.ylabel('Unique Characters')\n",
        "plt.title('Text Diversity vs Temperature')\n",
        "plt.xticks(range(len(temperatures)), temperatures)\n",
        "\n",
        "# Character frequency in generated text\n",
        "plt.subplot(2, 3, 5)\n",
        "sample_text = generate_text_with_temperature(selected_model, seed, length=200, temperature=1.0)\n",
        "char_freq = Counter(sample_text)\n",
        "common_chars = char_freq.most_common(10)\n",
        "chars, freqs = zip(*common_chars)\n",
        "plt.bar(range(len(chars)), freqs)\n",
        "plt.xlabel('Characters')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Character Frequency in Generated Text')\n",
        "plt.xticks(range(len(chars)), chars)\n",
        "\n",
        "# Model performance summary\n",
        "plt.subplot(2, 3, 6)\n",
        "final_train_acc = history.history['accuracy'][-1]\n",
        "final_val_acc = history.history['val_accuracy'][-1]\n",
        "final_train_loss = history.history['loss'][-1]\n",
        "final_val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "metrics = ['Train Acc', 'Val Acc', 'Train Loss', 'Val Loss']\n",
        "values = [final_train_acc, final_val_acc, final_train_loss, final_val_loss]\n",
        "colors = ['green', 'lightgreen', 'red', 'lightcoral']\n",
        "\n",
        "plt.bar(metrics, values, color=colors)\n",
        "plt.title('Final Model Performance')\n",
        "plt.ylabel('Value')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Generate and display text samples with different temperatures\n",
        "print(\"Text Generation Examples with Different Temperatures:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for temp in [0.2, 0.7, 1.0, 1.5]:\n",
        "    print(f\"\\nTemperature: {temp}\")\n",
        "    print(\"-\" * 40)\n",
        "    generated = generate_text_with_temperature(selected_model, \"the future of\", length=150, temperature=temp)\n",
        "    print(f\"'{generated}'\")\n",
        "\n",
        "# Analyze model performance\n",
        "print(f\"\\nModel Performance Summary:\")\n",
        "print(f\"Final Training Accuracy: {final_train_acc:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n",
        "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
        "print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n",
        "print(f\"Epochs Trained: {len(history.history['loss'])}\")\n",
        "\n",
        "# Generation quality analysis\n",
        "print(f\"\\nGeneration Quality Analysis:\")\n",
        "sample_gen = generate_text_with_temperature(selected_model, \"machine learning\", length=200, temperature=1.0)\n",
        "words = sample_gen.split()\n",
        "avg_word_length = np.mean([len(word) for word in words])\n",
        "unique_words = len(set(words))\n",
        "total_words = len(words)\n",
        "\n",
        "print(f\"Generated text length: {len(sample_gen)} characters\")\n",
        "print(f\"Average word length: {avg_word_length:.2f}\")\n",
        "print(f\"Vocabulary diversity: {unique_words}/{total_words} = {unique_words/total_words:.2f}\")\n",
        "print(f\"Sample: '{sample_gen[:100]}...'\")\n",
        "\n",
        "print(\"\\nText generation model training and evaluation complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
