{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Text Generator Using RNNs - Part 1: Data Preparation\n",
        "\n",
        "## Learning Objectives\n",
        "In this notebook, we'll build the foundation for text generation by:\n",
        "- Loading and preprocessing text data\n",
        "- Creating character sets and mappings\n",
        "- Building training sequences with proper labels\n",
        "- Implementing one-hot encoding for categorical data\n",
        "- Understanding sequence-to-sequence data preparation\n",
        "\n",
        "## Introduction to Text Generation\n",
        "\n",
        "Text generation is one of the most exciting applications of RNNs. The basic idea is:\n",
        "1. Train a model to predict the next character/word given previous context\n",
        "2. Use the trained model to generate new text by repeatedly predicting next tokens\n",
        "3. Control generation quality through sampling strategies\n",
        "\n",
        "Today we'll focus on **character-level generation** where the model learns to predict the next character in a sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "import os\n",
        "import requests\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Libraries imported successfully!\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Loading and Exploring Text Data\n",
        "\n",
        "For this demonstration, we'll use a sample text that includes various literary styles and patterns. In real applications, you might use:\n",
        "- Classic literature (Shakespeare, Alice in Wonderland)\n",
        "- Modern text (news articles, social media)\n",
        "- Code repositories\n",
        "- Domain-specific text (legal, medical, technical)\n",
        "\n",
        "Let's start by creating and exploring our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample text data for demonstration\n",
        "sample_text = \"\"\"\n",
        "To be or not to be, that is the question:\n",
        "Whether 'tis nobler in the mind to suffer\n",
        "The slings and arrows of outrageous fortune,\n",
        "Or to take arms against a sea of troubles\n",
        "And by opposing end them. To die—to sleep,\n",
        "No more; and by a sleep to say we end\n",
        "The heart-ache and the thousand natural shocks\n",
        "That flesh is heir to: 'tis a consummation\n",
        "Devoutly to be wish'd. To die, to sleep;\n",
        "To sleep, perchance to dream—ay, there's the rub:\n",
        "For in that sleep of death what dreams may come,\n",
        "When we have shuffled off this mortal coil,\n",
        "Must give us pause. There's the respect\n",
        "That makes calamity of so long life.\n",
        "\n",
        "The quick brown fox jumps over the lazy dog.\n",
        "Machine learning is revolutionizing the way we process data.\n",
        "Deep neural networks can learn complex patterns from large datasets.\n",
        "Artificial intelligence will transform various industries in the coming years.\n",
        "Natural language processing enables computers to understand human language.\n",
        "The future of technology lies in the intersection of AI and human creativity.\n",
        "\"\"\"\n",
        "\n",
        "# Clean and prepare the text\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Basic text preprocessing for character-level generation\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove extra whitespace but keep single spaces and newlines\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    \n",
        "    # Remove special characters except basic punctuation\n",
        "    text = re.sub(r'[^\\w\\s\\.\\,\\;\\:\\!\\?\\-\\'\\\"]', '', text)\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "# Preprocess the text\n",
        "processed_text = preprocess_text(sample_text)\n",
        "\n",
        "print(\"Original text length:\", len(sample_text))\n",
        "print(\"Processed text length:\", len(processed_text))\n",
        "print(\"\\nFirst 200 characters of processed text:\")\n",
        "print(repr(processed_text[:200]))\n",
        "\n",
        "# Text statistics\n",
        "print(f\"\\nText Statistics:\")\n",
        "print(f\"Total characters: {len(processed_text)}\")\n",
        "print(f\"Total words: {len(processed_text.split())}\")\n",
        "print(f\"Average word length: {np.mean([len(word) for word in processed_text.split()]):.2f}\")\n",
        "\n",
        "# Visualize character frequency\n",
        "char_counts = Counter(processed_text)\n",
        "print(f\"\\nUnique characters: {len(char_counts)}\")\n",
        "print(f\"Most common characters: {char_counts.most_common(10)}\")\n",
        "\n",
        "# Plot character frequency\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "chars, counts = zip(*char_counts.most_common(20))\n",
        "plt.bar(range(len(chars)), counts)\n",
        "plt.xticks(range(len(chars)), chars, rotation=45)\n",
        "plt.title('Top 20 Character Frequencies')\n",
        "plt.xlabel('Characters')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# Character frequency distribution\n",
        "frequencies = list(char_counts.values())\n",
        "plt.hist(frequencies, bins=20, alpha=0.7, edgecolor='black')\n",
        "plt.title('Distribution of Character Frequencies')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Number of Characters')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Creating Character Sets and Mappings\n",
        "\n",
        "For character-level text generation, we need to:\n",
        "1. Create a vocabulary of all unique characters\n",
        "2. Create mappings between characters and numerical indices\n",
        "3. Handle special tokens (start, end, unknown characters)\n",
        "\n",
        "This step is crucial as it defines how our model will represent and process text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create character vocabulary and mappings\n",
        "class CharacterVocabulary:\n",
        "    \"\"\"\n",
        "    A class to handle character-level vocabulary for text generation\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, text, add_special_tokens=True):\n",
        "        \"\"\"\n",
        "        Initialize vocabulary from text\n",
        "        \n",
        "        Parameters:\n",
        "        text: input text string\n",
        "        add_special_tokens: whether to add special tokens\n",
        "        \"\"\"\n",
        "        # Get unique characters and sort them for consistency\n",
        "        self.chars = sorted(list(set(text)))\n",
        "        \n",
        "        # Add special tokens if requested\n",
        "        if add_special_tokens:\n",
        "            special_tokens = ['<START>', '<END>', '<UNK>']\n",
        "            self.chars = special_tokens + self.chars\n",
        "        \n",
        "        # Create mappings\n",
        "        self.char_to_idx = {char: idx for idx, char in enumerate(self.chars)}\n",
        "        self.idx_to_char = {idx: char for idx, char in enumerate(self.chars)}\n",
        "        \n",
        "        # Store vocabulary size\n",
        "        self.vocab_size = len(self.chars)\n",
        "        \n",
        "        print(f\"Vocabulary created with {self.vocab_size} characters\")\n",
        "        print(f\"Characters: {self.chars}\")\n",
        "    \n",
        "    def encode(self, text):\n",
        "        \"\"\"Convert text to sequence of indices\"\"\"\n",
        "        return [self.char_to_idx.get(char, self.char_to_idx.get('<UNK>', 0)) for char in text]\n",
        "    \n",
        "    def decode(self, indices):\n",
        "        \"\"\"Convert sequence of indices back to text\"\"\"\n",
        "        return ''.join([self.idx_to_char.get(idx, '<UNK>') for idx in indices])\n",
        "    \n",
        "    def get_char_info(self):\n",
        "        \"\"\"Get detailed character information\"\"\"\n",
        "        info = []\n",
        "        for char in self.chars:\n",
        "            idx = self.char_to_idx[char]\n",
        "            char_repr = repr(char) if char.isprintable() else f\"'{char}'\"\n",
        "            info.append((idx, char_repr, char))\n",
        "        return info\n",
        "\n",
        "# Create vocabulary from our processed text\n",
        "vocab = CharacterVocabulary(processed_text, add_special_tokens=True)\n",
        "\n",
        "# Display character mappings\n",
        "print(\"\\nCharacter to Index Mappings:\")\n",
        "char_info = vocab.get_char_info()\n",
        "for idx, char_repr, char in char_info[:20]:  # Show first 20\n",
        "    print(f\"Index {idx:2d}: {char_repr:8s} -> '{char}'\")\n",
        "\n",
        "if len(char_info) > 20:\n",
        "    print(f\"... and {len(char_info) - 20} more characters\")\n",
        "\n",
        "# Test encoding and decoding\n",
        "test_string = \"hello world\"\n",
        "encoded = vocab.encode(test_string)\n",
        "decoded = vocab.decode(encoded)\n",
        "\n",
        "print(f\"\\nEncoding/Decoding Test:\")\n",
        "print(f\"Original: '{test_string}'\")\n",
        "print(f\"Encoded:  {encoded}\")\n",
        "print(f\"Decoded:  '{decoded}'\")\n",
        "\n",
        "# Visualize the vocabulary\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Character index mapping\n",
        "plt.subplot(2, 2, 1)\n",
        "indices = list(range(min(20, vocab.vocab_size)))\n",
        "chars_subset = [vocab.idx_to_char[i] for i in indices]\n",
        "plt.bar(indices, [1] * len(indices))\n",
        "plt.xticks(indices, [repr(c) for c in chars_subset], rotation=45)\n",
        "plt.title('Character Index Mapping (First 20)')\n",
        "plt.xlabel('Character Index')\n",
        "plt.ylabel('Presence')\n",
        "\n",
        "# Vocabulary size comparison\n",
        "plt.subplot(2, 2, 2)\n",
        "categories = ['Letters', 'Digits', 'Punctuation', 'Whitespace', 'Special']\n",
        "counts = [\n",
        "    sum(1 for c in vocab.chars if c.isalpha()),\n",
        "    sum(1 for c in vocab.chars if c.isdigit()),\n",
        "    sum(1 for c in vocab.chars if c in string.punctuation),\n",
        "    sum(1 for c in vocab.chars if c.isspace()),\n",
        "    sum(1 for c in vocab.chars if c.startswith('<'))\n",
        "]\n",
        "plt.pie(counts, labels=categories, autopct='%1.1f%%')\n",
        "plt.title('Character Type Distribution')\n",
        "\n",
        "# Encoding example\n",
        "plt.subplot(2, 2, 3)\n",
        "sample_chars = processed_text[:20]\n",
        "sample_encoded = vocab.encode(sample_chars)\n",
        "plt.plot(sample_encoded, 'o-', linewidth=2, markersize=8)\n",
        "plt.title('Sample Text Encoding')\n",
        "plt.xlabel('Character Position')\n",
        "plt.ylabel('Character Index')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Vocabulary coverage\n",
        "plt.subplot(2, 2, 4)\n",
        "text_chars = set(processed_text)\n",
        "vocab_chars = set(vocab.chars)\n",
        "coverage = len(text_chars.intersection(vocab_chars)) / len(text_chars) * 100\n",
        "plt.bar(['Coverage'], [coverage], color='green', alpha=0.7)\n",
        "plt.ylim(0, 100)\n",
        "plt.title(f'Vocabulary Coverage: {coverage:.1f}%')\n",
        "plt.ylabel('Percentage')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Creating Training Sequences and Labels\n",
        "\n",
        "For text generation, we need to create training pairs where:\n",
        "- **Input**: A sequence of characters (context)\n",
        "- **Output**: The next character in the sequence (target)\n",
        "\n",
        "This is often called \"sliding window\" approach where we move a window across the text to create many training examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create training sequences and labels\n",
        "class SequenceGenerator:\n",
        "    \"\"\"\n",
        "    A class to generate training sequences for character-level text generation\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, text, vocab, sequence_length=40, step=1):\n",
        "        \"\"\"\n",
        "        Initialize sequence generator\n",
        "        \n",
        "        Parameters:\n",
        "        text: input text string\n",
        "        vocab: CharacterVocabulary object\n",
        "        sequence_length: length of input sequences\n",
        "        step: step size for sliding window\n",
        "        \"\"\"\n",
        "        self.text = text\n",
        "        self.vocab = vocab\n",
        "        self.sequence_length = sequence_length\n",
        "        self.step = step\n",
        "        \n",
        "        # Encode the entire text\n",
        "        self.encoded_text = vocab.encode(text)\n",
        "        \n",
        "        print(f\"Text length: {len(text)} characters\")\n",
        "        print(f\"Encoded length: {len(self.encoded_text)} indices\")\n",
        "        print(f\"Sequence length: {sequence_length}\")\n",
        "        print(f\"Step size: {step}\")\n",
        "    \n",
        "    def create_sequences(self):\n",
        "        \"\"\"\n",
        "        Create input sequences and corresponding targets\n",
        "        \"\"\"\n",
        "        sequences = []\n",
        "        targets = []\n",
        "        \n",
        "        # Slide window across the text\n",
        "        for i in range(0, len(self.encoded_text) - self.sequence_length, self.step):\n",
        "            # Input sequence (context)\n",
        "            seq = self.encoded_text[i:i + self.sequence_length]\n",
        "            # Target (next character)\n",
        "            target = self.encoded_text[i + self.sequence_length]\n",
        "            \n",
        "            sequences.append(seq)\n",
        "            targets.append(target)\n",
        "        \n",
        "        print(f\"Created {len(sequences)} training sequences\")\n",
        "        return np.array(sequences), np.array(targets)\n",
        "    \n",
        "    def create_sequence_to_sequence_data(self):\n",
        "        \"\"\"\n",
        "        Create sequence-to-sequence data where input and output are sequences\n",
        "        \"\"\"\n",
        "        input_sequences = []\n",
        "        target_sequences = []\n",
        "        \n",
        "        for i in range(0, len(self.encoded_text) - self.sequence_length, self.step):\n",
        "            # Input sequence\n",
        "            input_seq = self.encoded_text[i:i + self.sequence_length]\n",
        "            # Target sequence (shifted by one)\n",
        "            target_seq = self.encoded_text[i + 1:i + self.sequence_length + 1]\n",
        "            \n",
        "            input_sequences.append(input_seq)\n",
        "            target_sequences.append(target_seq)\n",
        "        \n",
        "        print(f\"Created {len(input_sequences)} sequence-to-sequence pairs\")\n",
        "        return np.array(input_sequences), np.array(target_sequences)\n",
        "    \n",
        "    def visualize_sequences(self, sequences, targets, num_examples=3):\n",
        "        \"\"\"\n",
        "        Visualize some example sequences\n",
        "        \"\"\"\n",
        "        print(\"Example Training Sequences:\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        for i in range(min(num_examples, len(sequences))):\n",
        "            input_text = self.vocab.decode(sequences[i])\n",
        "            target_char = self.vocab.idx_to_char[targets[i]]\n",
        "            \n",
        "            print(f\"Example {i+1}:\")\n",
        "            print(f\"Input:  '{input_text}'\")\n",
        "            print(f\"Target: '{target_char}'\")\n",
        "            print(f\"Indices: {sequences[i][:10]}... -> {targets[i]}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "# Create sequence generator\n",
        "sequence_length = 25  # Length of context to use for prediction\n",
        "step_size = 3         # Step size for sliding window (smaller = more overlap)\n",
        "\n",
        "seq_gen = SequenceGenerator(processed_text, vocab, sequence_length, step_size)\n",
        "\n",
        "# Create training sequences\n",
        "X_sequences, y_targets = seq_gen.create_sequences()\n",
        "\n",
        "print(f\"\\nTraining data shapes:\")\n",
        "print(f\"Input sequences: {X_sequences.shape}\")\n",
        "print(f\"Target characters: {y_targets.shape}\")\n",
        "\n",
        "# Visualize some examples\n",
        "seq_gen.visualize_sequences(X_sequences, y_targets, num_examples=5)\n",
        "\n",
        "# Create sequence-to-sequence data as well (for comparison)\n",
        "X_seq2seq, y_seq2seq = seq_gen.create_sequence_to_sequence_data()\n",
        "\n",
        "print(f\"\\nSequence-to-sequence data shapes:\")\n",
        "print(f\"Input sequences: {X_seq2seq.shape}\")\n",
        "print(f\"Target sequences: {y_seq2seq.shape}\")\n",
        "\n",
        "# Analyze sequence statistics\n",
        "print(f\"\\nSequence Statistics:\")\n",
        "print(f\"Total training examples: {len(X_sequences)}\")\n",
        "print(f\"Sequence length: {sequence_length}\")\n",
        "print(f\"Vocabulary size: {vocab.vocab_size}\")\n",
        "print(f\"Coverage: {len(X_sequences) / len(processed_text) * 100:.1f}% of original text\")\n",
        "\n",
        "# Visualize sequence creation process\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Distribution of target characters\n",
        "plt.subplot(2, 3, 1)\n",
        "target_counts = Counter(y_targets)\n",
        "chars = [vocab.idx_to_char[idx] for idx in target_counts.keys()]\n",
        "counts = list(target_counts.values())\n",
        "plt.bar(range(len(counts)), counts)\n",
        "plt.title('Target Character Distribution')\n",
        "plt.xlabel('Character Index')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(range(0, len(counts), max(1, len(counts)//10)))\n",
        "\n",
        "# Sequence length distribution\n",
        "plt.subplot(2, 3, 2)\n",
        "seq_lengths = [len(seq) for seq in X_sequences]\n",
        "plt.hist(seq_lengths, bins=20, alpha=0.7, edgecolor='black')\n",
        "plt.title('Sequence Length Distribution')\n",
        "plt.xlabel('Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.axvline(sequence_length, color='red', linestyle='--', label=f'Target: {sequence_length}')\n",
        "plt.legend()\n",
        "\n",
        "# First few sequences visualization\n",
        "plt.subplot(2, 3, 3)\n",
        "first_seqs = X_sequences[:10]\n",
        "for i, seq in enumerate(first_seqs):\n",
        "    plt.plot(seq, alpha=0.7, label=f'Seq {i+1}' if i < 3 else \"\")\n",
        "plt.title('First 10 Sequences')\n",
        "plt.xlabel('Position in Sequence')\n",
        "plt.ylabel('Character Index')\n",
        "if len(first_seqs) <= 3:\n",
        "    plt.legend()\n",
        "\n",
        "# Character transition matrix (for first 10 most common chars)\n",
        "plt.subplot(2, 3, 4)\n",
        "common_chars = [idx for idx, _ in Counter(y_targets).most_common(10)]\n",
        "transition_matrix = np.zeros((len(common_chars), len(common_chars)))\n",
        "\n",
        "for seq, target in zip(X_sequences, y_targets):\n",
        "    if len(seq) > 0 and seq[-1] in common_chars and target in common_chars:\n",
        "        prev_idx = common_chars.index(seq[-1])\n",
        "        target_idx = common_chars.index(target)\n",
        "        transition_matrix[prev_idx, target_idx] += 1\n",
        "\n",
        "plt.imshow(transition_matrix, cmap='Blues')\n",
        "plt.title('Character Transition Heatmap\\n(Top 10 chars)')\n",
        "plt.xlabel('Target Character')\n",
        "plt.ylabel('Previous Character')\n",
        "plt.colorbar()\n",
        "\n",
        "# Sequence overlap visualization\n",
        "plt.subplot(2, 3, 5)\n",
        "overlaps = []\n",
        "for i in range(len(X_sequences) - 1):\n",
        "    overlap = sum(1 for a, b in zip(X_sequences[i], X_sequences[i+1]) if a == b)\n",
        "    overlaps.append(overlap)\n",
        "\n",
        "plt.hist(overlaps, bins=20, alpha=0.7, edgecolor='black')\n",
        "plt.title('Sequence Overlap Distribution')\n",
        "plt.xlabel('Number of Overlapping Characters')\n",
        "plt.ylabel('Frequency')\n",
        "plt.axvline(sequence_length - step_size, color='red', linestyle='--', \n",
        "           label=f'Expected: {sequence_length - step_size}')\n",
        "plt.legend()\n",
        "\n",
        "# Data creation efficiency\n",
        "plt.subplot(2, 3, 6)\n",
        "total_chars = len(processed_text)\n",
        "used_chars = len(X_sequences) * sequence_length\n",
        "efficiency = used_chars / total_chars * 100\n",
        "\n",
        "plt.bar(['Efficiency'], [efficiency], color='green', alpha=0.7)\n",
        "plt.ylim(0, 500)  # Allow for > 100% due to overlap\n",
        "plt.title(f'Data Utilization: {efficiency:.1f}%')\n",
        "plt.ylabel('Percentage')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. One-Hot Encoding for Categorical Data\n",
        "\n",
        "For neural networks to process categorical data (like character indices), we need to convert them to numerical vectors. One-hot encoding creates a binary vector where only one element is \"hot\" (1) and all others are \"cold\" (0).\n",
        "\n",
        "### Example:\n",
        "If we have vocabulary ['a', 'b', 'c'] and want to encode 'b':\n",
        "- 'a' → [1, 0, 0]\n",
        "- 'b' → [0, 1, 0]  \n",
        "- 'c' → [0, 0, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-Hot Encoding Implementation\n",
        "class OneHotEncoder:\n",
        "    \"\"\"\n",
        "    A class to handle one-hot encoding for character sequences\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"\n",
        "        Initialize one-hot encoder\n",
        "        \n",
        "        Parameters:\n",
        "        vocab_size: size of the vocabulary\n",
        "        \"\"\"\n",
        "        self.vocab_size = vocab_size\n",
        "        print(f\"One-hot encoder initialized for vocabulary size: {vocab_size}\")\n",
        "    \n",
        "    def encode_sequences(self, sequences):\n",
        "        \"\"\"\n",
        "        Convert sequences of indices to one-hot encoded arrays\n",
        "        \n",
        "        Parameters:\n",
        "        sequences: array of shape (num_sequences, sequence_length)\n",
        "        \n",
        "        Returns:\n",
        "        one_hot: array of shape (num_sequences, sequence_length, vocab_size)\n",
        "        \"\"\"\n",
        "        num_sequences, sequence_length = sequences.shape\n",
        "        \n",
        "        # Initialize one-hot array\n",
        "        one_hot = np.zeros((num_sequences, sequence_length, self.vocab_size))\n",
        "        \n",
        "        # Fill in the one-hot encoding\n",
        "        for i in range(num_sequences):\n",
        "            for j in range(sequence_length):\n",
        "                char_idx = sequences[i, j]\n",
        "                one_hot[i, j, char_idx] = 1\n",
        "        \n",
        "        return one_hot\n",
        "    \n",
        "    def encode_targets(self, targets):\n",
        "        \"\"\"\n",
        "        Convert target indices to one-hot encoded arrays\n",
        "        \n",
        "        Parameters:\n",
        "        targets: array of shape (num_targets,)\n",
        "        \n",
        "        Returns:\n",
        "        one_hot: array of shape (num_targets, vocab_size)\n",
        "        \"\"\"\n",
        "        num_targets = len(targets)\n",
        "        one_hot = np.zeros((num_targets, self.vocab_size))\n",
        "        \n",
        "        for i, target in enumerate(targets):\n",
        "            one_hot[i, target] = 1\n",
        "        \n",
        "        return one_hot\n",
        "    \n",
        "    def decode_predictions(self, predictions):\n",
        "        \"\"\"\n",
        "        Convert one-hot predictions back to character indices\n",
        "        \n",
        "        Parameters:\n",
        "        predictions: array of shape (num_predictions, vocab_size)\n",
        "        \n",
        "        Returns:\n",
        "        indices: array of predicted character indices\n",
        "        \"\"\"\n",
        "        return np.argmax(predictions, axis=-1)\n",
        "\n",
        "# Create one-hot encoder\n",
        "encoder = OneHotEncoder(vocab.vocab_size)\n",
        "\n",
        "# Demonstrate one-hot encoding on a small sample\n",
        "sample_size = 5\n",
        "sample_sequences = X_sequences[:sample_size]\n",
        "sample_targets = y_targets[:sample_size]\n",
        "\n",
        "print(\"Original sequences (indices):\")\n",
        "print(sample_sequences)\n",
        "print(f\"Shape: {sample_sequences.shape}\")\n",
        "\n",
        "# Encode sequences\n",
        "encoded_sequences = encoder.encode_sequences(sample_sequences)\n",
        "encoded_targets = encoder.encode_targets(sample_targets)\n",
        "\n",
        "print(f\"\\nOne-hot encoded sequences shape: {encoded_sequences.shape}\")\n",
        "print(f\"One-hot encoded targets shape: {encoded_targets.shape}\")\n",
        "\n",
        "# Show example of one-hot encoding\n",
        "print(f\"\\nExample: Character index {sample_targets[0]} becomes:\")\n",
        "print(encoded_targets[0])\n",
        "print(f\"Character: '{vocab.idx_to_char[sample_targets[0]]}'\")\n",
        "\n",
        "# Visualize one-hot encoding\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "# Visualize first sequence's one-hot encoding\n",
        "plt.subplot(3, 2, 1)\n",
        "first_seq_onehot = encoded_sequences[0]\n",
        "plt.imshow(first_seq_onehot.T, cmap='Blues', aspect='auto')\n",
        "plt.title('One-Hot Encoding of First Sequence')\n",
        "plt.xlabel('Position in Sequence')\n",
        "plt.ylabel('Character Index')\n",
        "plt.colorbar()\n",
        "\n",
        "# Show target one-hot encodings\n",
        "plt.subplot(3, 2, 2)\n",
        "plt.imshow(encoded_targets[:sample_size].T, cmap='Reds', aspect='auto')\n",
        "plt.title('One-Hot Encoding of Targets')\n",
        "plt.xlabel('Example Number')\n",
        "plt.ylabel('Character Index')\n",
        "plt.colorbar()\n",
        "\n",
        "# Memory usage comparison\n",
        "plt.subplot(3, 2, 3)\n",
        "original_size = X_sequences.nbytes / 1024  # KB\n",
        "onehot_size = encoded_sequences.nbytes / 1024 if encoded_sequences.size > 0 else 0\n",
        "compression_ratio = onehot_size / original_size if original_size > 0 else 0\n",
        "\n",
        "sizes = [original_size, onehot_size]\n",
        "labels = ['Original\\n(indices)', f'One-Hot\\n(ratio: {compression_ratio:.1f}x)']\n",
        "plt.bar(labels, sizes, color=['lightblue', 'lightcoral'])\n",
        "plt.title('Memory Usage Comparison')\n",
        "plt.ylabel('Size (KB)')\n",
        "\n",
        "# Sparsity analysis\n",
        "plt.subplot(3, 2, 4)\n",
        "sparsity = 1 - np.count_nonzero(encoded_sequences) / encoded_sequences.size\n",
        "plt.bar(['Sparsity'], [sparsity * 100], color='green', alpha=0.7)\n",
        "plt.title(f'One-Hot Sparsity: {sparsity*100:.1f}%')\n",
        "plt.ylabel('Percentage of Zeros')\n",
        "plt.ylim(0, 100)\n",
        "\n",
        "# Character distribution in one-hot format\n",
        "plt.subplot(3, 2, 5)\n",
        "char_sums = np.sum(encoded_sequences, axis=(0, 1))\n",
        "plt.bar(range(len(char_sums)), char_sums)\n",
        "plt.title('Character Frequency in One-Hot Data')\n",
        "plt.xlabel('Character Index')\n",
        "plt.ylabel('Total Occurrences')\n",
        "\n",
        "# Encoding/decoding demonstration\n",
        "plt.subplot(3, 2, 6)\n",
        "# Create some dummy predictions to demonstrate decoding\n",
        "dummy_predictions = np.random.rand(5, vocab.vocab_size)\n",
        "dummy_predictions = dummy_predictions / dummy_predictions.sum(axis=1, keepdims=True)  # Normalize\n",
        "decoded_indices = encoder.decode_predictions(dummy_predictions)\n",
        "\n",
        "x_pos = range(len(decoded_indices))\n",
        "plt.bar(x_pos, decoded_indices, alpha=0.7)\n",
        "plt.title('Decoded Predictions Example')\n",
        "plt.xlabel('Example Number')\n",
        "plt.ylabel('Predicted Character Index')\n",
        "\n",
        "for i, idx in enumerate(decoded_indices):\n",
        "    char = vocab.idx_to_char.get(idx, '?')\n",
        "    plt.text(i, idx + 0.5, f\"'{char}'\", ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Alternative: Using TensorFlow's built-in one-hot encoding\n",
        "print(\"\\nUsing TensorFlow's one-hot encoding:\")\n",
        "tf_onehot_sequences = tf.one_hot(sample_sequences, vocab.vocab_size)\n",
        "tf_onehot_targets = tf.one_hot(sample_targets, vocab.vocab_size)\n",
        "\n",
        "print(f\"TensorFlow one-hot sequences shape: {tf_onehot_sequences.shape}\")\n",
        "print(f\"TensorFlow one-hot targets shape: {tf_onehot_targets.shape}\")\n",
        "\n",
        "# Verify they're the same\n",
        "sequences_match = np.allclose(encoded_sequences, tf_onehot_sequences.numpy())\n",
        "targets_match = np.allclose(encoded_targets, tf_onehot_targets.numpy())\n",
        "\n",
        "print(f\"Manual vs TensorFlow encoding match - Sequences: {sequences_match}, Targets: {targets_match}\")\n",
        "\n",
        "# Memory efficiency analysis\n",
        "print(f\"\\nMemory Efficiency Analysis:\")\n",
        "print(f\"Original data size: {X_sequences.nbytes / 1024:.2f} KB\")\n",
        "print(f\"One-hot data size: {encoded_sequences.nbytes / 1024:.2f} KB\")\n",
        "print(f\"Expansion factor: {encoded_sequences.nbytes / X_sequences.nbytes:.1f}x\")\n",
        "print(f\"Efficiency: Each index expanded to {vocab.vocab_size} floats\")\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"\\nSummary Statistics:\")\n",
        "print(f\"- Vocabulary size: {vocab.vocab_size}\")\n",
        "print(f\"- Sequence length: {sequence_length}\")\n",
        "print(f\"- Number of training examples: {len(X_sequences)}\")\n",
        "print(f\"- Input shape after one-hot: {encoded_sequences.shape}\")\n",
        "print(f\"- Target shape after one-hot: {encoded_targets.shape}\")\n",
        "print(f\"- Data sparsity: {sparsity*100:.1f}% zeros\")\n",
        "\n",
        "print(\"\\nData preparation complete! Ready for model training.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
