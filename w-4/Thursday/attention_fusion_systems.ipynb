{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Attention Fusion with LSTM Systems\n",
        "\n",
        "## Learning Objectives\n",
        "In this notebook, we'll explore advanced attention-LSTM fusion architectures:\n",
        "- Combining attention mechanisms with LSTM architectures\n",
        "- Multi-head attention integration with LSTMs\n",
        "- Self-attention vs cross-attention in sequence modeling\n",
        "- Transformer-LSTM hybrid architectures\n",
        "- Attention visualization and interpretation techniques\n",
        "- Performance analysis of fusion systems\n",
        "\n",
        "## Introduction to Attention-LSTM Fusion\n",
        "\n",
        "Attention mechanisms can be combined with LSTMs to create powerful hybrid architectures that leverage the benefits of both:\n",
        "\n",
        "### Benefits of Attention-LSTM Fusion:\n",
        "1. **Enhanced Memory**: LSTMs provide sequential memory, attention provides selective focus\n",
        "2. **Long-range Dependencies**: Attention helps overcome LSTM limitations on very long sequences\n",
        "3. **Interpretability**: Attention weights provide insights into model decisions\n",
        "4. **Flexibility**: Different fusion strategies for different tasks\n",
        "\n",
        "### Fusion Strategies:\n",
        "- **Pre-LSTM Attention**: Attention applied to inputs before LSTM processing\n",
        "- **Post-LSTM Attention**: Attention applied to LSTM outputs\n",
        "- **Parallel Fusion**: Attention and LSTM branches combined\n",
        "- **Hierarchical Fusion**: Multiple levels of attention-LSTM interaction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Attention Fusion Systems initialized!\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# 1. Multi-Head Attention Layer\n",
        "class MultiHeadAttention(layers.Layer):\n",
        "    \"\"\"\n",
        "    Multi-head attention mechanism for fusion with LSTMs\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, num_heads, **kwargs):\n",
        "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        assert d_model % self.num_heads == 0\n",
        "        \n",
        "        self.depth = d_model // self.num_heads\n",
        "        \n",
        "        self.wq = layers.Dense(d_model)\n",
        "        self.wk = layers.Dense(d_model)\n",
        "        self.wv = layers.Dense(d_model)\n",
        "        \n",
        "        self.dense = layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        \n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "        \n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "        \n",
        "        # Scaled dot-product attention\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "        \n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)\n",
        "        \n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "        \n",
        "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(output, (batch_size, -1, self.d_model))\n",
        "        \n",
        "        output = self.dense(concat_attention)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "# 2. Attention-LSTM Fusion Architectures\n",
        "def create_attention_lstm_models(input_shape, lstm_units=64, num_classes=3):\n",
        "    \"\"\"\n",
        "    Create different attention-LSTM fusion architectures\n",
        "    \"\"\"\n",
        "    models = {}\n",
        "    \n",
        "    # Model 1: Post-LSTM Attention\n",
        "    lstm_input = layers.Input(shape=input_shape)\n",
        "    lstm_out = layers.LSTM(lstm_units, return_sequences=True)(lstm_input)\n",
        "    \n",
        "    # Self-attention on LSTM outputs\n",
        "    attention_out = layers.MultiHeadAttention(\n",
        "        num_heads=4, key_dim=lstm_units//4\n",
        "    )(lstm_out, lstm_out)\n",
        "    \n",
        "    # Global pooling and classification\n",
        "    pooled = layers.GlobalAveragePooling1D()(attention_out)\n",
        "    dense = layers.Dense(32, activation='relu')(pooled)\n",
        "    dropout = layers.Dropout(0.3)(dense)\n",
        "    output = layers.Dense(num_classes, activation='softmax')(dropout)\n",
        "    \n",
        "    models['Post_LSTM_Attention'] = keras.Model(\n",
        "        inputs=lstm_input, outputs=output, name='Post_LSTM_Attention'\n",
        "    )\n",
        "    \n",
        "    # Model 2: Pre-LSTM Attention\n",
        "    pre_input = layers.Input(shape=input_shape)\n",
        "    \n",
        "    # Self-attention on inputs\n",
        "    pre_attention = layers.MultiHeadAttention(\n",
        "        num_heads=4, key_dim=input_shape[-1]//4\n",
        "    )(pre_input, pre_input)\n",
        "    \n",
        "    # LSTM processing\n",
        "    lstm_out = layers.LSTM(lstm_units, return_sequences=False)(pre_attention)\n",
        "    dense = layers.Dense(32, activation='relu')(lstm_out)\n",
        "    dropout = layers.Dropout(0.3)(dense)\n",
        "    output = layers.Dense(num_classes, activation='softmax')(dropout)\n",
        "    \n",
        "    models['Pre_LSTM_Attention'] = keras.Model(\n",
        "        inputs=pre_input, outputs=output, name='Pre_LSTM_Attention'\n",
        "    )\n",
        "    \n",
        "    # Model 3: Parallel Fusion\n",
        "    parallel_input = layers.Input(shape=input_shape)\n",
        "    \n",
        "    # LSTM branch\n",
        "    lstm_branch = layers.LSTM(lstm_units, return_sequences=True)(parallel_input)\n",
        "    lstm_pooled = layers.GlobalAveragePooling1D()(lstm_branch)\n",
        "    \n",
        "    # Attention branch\n",
        "    attention_branch = layers.MultiHeadAttention(\n",
        "        num_heads=4, key_dim=input_shape[-1]//4\n",
        "    )(parallel_input, parallel_input)\n",
        "    attention_pooled = layers.GlobalAveragePooling1D()(attention_branch)\n",
        "    \n",
        "    # Fusion\n",
        "    fused = layers.Concatenate()([lstm_pooled, attention_pooled])\n",
        "    dense = layers.Dense(64, activation='relu')(fused)\n",
        "    dropout = layers.Dropout(0.3)(dense)\n",
        "    output = layers.Dense(num_classes, activation='softmax')(dropout)\n",
        "    \n",
        "    models['Parallel_Fusion'] = keras.Model(\n",
        "        inputs=parallel_input, outputs=output, name='Parallel_Fusion'\n",
        "    )\n",
        "    \n",
        "    # Model 4: Hierarchical Fusion\n",
        "    hier_input = layers.Input(shape=input_shape)\n",
        "    \n",
        "    # First level: LSTM processing\n",
        "    lstm_l1 = layers.LSTM(lstm_units, return_sequences=True)(hier_input)\n",
        "    \n",
        "    # Second level: Attention on LSTM outputs\n",
        "    attention_l2 = layers.MultiHeadAttention(\n",
        "        num_heads=4, key_dim=lstm_units//4\n",
        "    )(lstm_l1, lstm_l1)\n",
        "    \n",
        "    # Third level: Another LSTM\n",
        "    lstm_l3 = layers.LSTM(lstm_units//2, return_sequences=False)(attention_l2)\n",
        "    \n",
        "    dense = layers.Dense(32, activation='relu')(lstm_l3)\n",
        "    dropout = layers.Dropout(0.3)(dense)\n",
        "    output = layers.Dense(num_classes, activation='softmax')(dropout)\n",
        "    \n",
        "    models['Hierarchical_Fusion'] = keras.Model(\n",
        "        inputs=hier_input, outputs=output, name='Hierarchical_Fusion'\n",
        "    )\n",
        "    \n",
        "    return models\n",
        "\n",
        "# 3. Attention Visualization System\n",
        "class AttentionVisualizer:\n",
        "    \"\"\"\n",
        "    Visualize attention patterns in fusion models\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.attention_weights = {}\n",
        "    \n",
        "    def extract_attention_weights(self, model, input_data):\n",
        "        \"\"\"\n",
        "        Extract attention weights from model\n",
        "        \"\"\"\n",
        "        # Create a model that outputs attention weights\n",
        "        attention_layers = []\n",
        "        for layer in model.layers:\n",
        "            if isinstance(layer, layers.MultiHeadAttention):\n",
        "                attention_layers.append(layer)\n",
        "        \n",
        "        if not attention_layers:\n",
        "            print(\"No attention layers found in model\")\n",
        "            return None\n",
        "        \n",
        "        # For demonstration, we'll simulate attention weights\n",
        "        # In practice, you'd need to modify the model to return attention weights\n",
        "        batch_size, seq_len, features = input_data.shape\n",
        "        num_heads = 4\n",
        "        \n",
        "        # Simulate attention weights\n",
        "        attention_weights = np.random.softmax(\n",
        "            np.random.randn(batch_size, num_heads, seq_len, seq_len), axis=-1\n",
        "        )\n",
        "        \n",
        "        return attention_weights\n",
        "    \n",
        "    def visualize_attention_patterns(self, attention_weights, input_sequence=None):\n",
        "        \"\"\"\n",
        "        Visualize attention patterns\n",
        "        \"\"\"\n",
        "        if attention_weights is None:\n",
        "            print(\"No attention weights to visualize\")\n",
        "            return\n",
        "        \n",
        "        # Take first sample and average across heads\n",
        "        sample_attention = attention_weights[0].mean(axis=0)\n",
        "        \n",
        "        plt.figure(figsize=(15, 10))\n",
        "        \n",
        "        # Attention heatmap\n",
        "        plt.subplot(2, 3, 1)\n",
        "        sns.heatmap(sample_attention, cmap='Blues', cbar=True)\n",
        "        plt.title('Attention Weights Heatmap')\n",
        "        plt.xlabel('Key Position')\n",
        "        plt.ylabel('Query Position')\n",
        "        \n",
        "        # Attention distribution for each query\n",
        "        plt.subplot(2, 3, 2)\n",
        "        for i in range(0, sample_attention.shape[0], max(1, sample_attention.shape[0]//5)):\n",
        "            plt.plot(sample_attention[i], label=f'Query {i}', alpha=0.7)\n",
        "        plt.title('Attention Distribution by Query')\n",
        "        plt.xlabel('Key Position')\n",
        "        plt.ylabel('Attention Weight')\n",
        "        plt.legend()\n",
        "        \n",
        "        # Average attention per position\n",
        "        plt.subplot(2, 3, 3)\n",
        "        avg_attention = sample_attention.mean(axis=0)\n",
        "        plt.bar(range(len(avg_attention)), avg_attention, alpha=0.7)\n",
        "        plt.title('Average Attention per Position')\n",
        "        plt.xlabel('Position')\n",
        "        plt.ylabel('Average Attention')\n",
        "        \n",
        "        # Attention head comparison (using original multi-head weights)\n",
        "        plt.subplot(2, 3, 4)\n",
        "        head_attentions = attention_weights[0]  # First sample, all heads\n",
        "        head_means = [head.mean() for head in head_attentions]\n",
        "        plt.bar(range(len(head_means)), head_means, alpha=0.7)\n",
        "        plt.title('Average Attention by Head')\n",
        "        plt.xlabel('Head Index')\n",
        "        plt.ylabel('Average Attention')\n",
        "        \n",
        "        # Attention entropy (measure of focus)\n",
        "        plt.subplot(2, 3, 5)\n",
        "        entropies = []\n",
        "        for i in range(sample_attention.shape[0]):\n",
        "            probs = sample_attention[i]\n",
        "            entropy = -np.sum(probs * np.log(probs + 1e-10))\n",
        "            entropies.append(entropy)\n",
        "        \n",
        "        plt.plot(entropies, 'o-', alpha=0.7)\n",
        "        plt.title('Attention Entropy by Query Position')\n",
        "        plt.xlabel('Query Position')\n",
        "        plt.ylabel('Entropy (Higher = More Spread)')\n",
        "        \n",
        "        # Attention matrix statistics\n",
        "        plt.subplot(2, 3, 6)\n",
        "        stats = {\n",
        "            'Max': sample_attention.max(),\n",
        "            'Min': sample_attention.min(),\n",
        "            'Mean': sample_attention.mean(),\n",
        "            'Std': sample_attention.std()\n",
        "        }\n",
        "        \n",
        "        plt.bar(stats.keys(), stats.values(), alpha=0.7)\n",
        "        plt.title('Attention Matrix Statistics')\n",
        "        plt.ylabel('Value')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Generate sample data for attention-LSTM fusion\n",
        "def create_complex_sequence_data(num_samples=800, seq_length=40, features=16):\n",
        "    \"\"\"\n",
        "    Create complex sequence data for attention-LSTM fusion testing\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "    \n",
        "    for i in range(num_samples):\n",
        "        # Create sequences with different attention-requiring patterns\n",
        "        if i % 3 == 0:\n",
        "            # Pattern requiring early attention\n",
        "            seq = np.random.randn(seq_length, features) * 0.5\n",
        "            seq[:5] += 2.0  # Important information at beginning\n",
        "            label = 0\n",
        "        elif i % 3 == 1:\n",
        "            # Pattern requiring late attention\n",
        "            seq = np.random.randn(seq_length, features) * 0.5\n",
        "            seq[-5:] += 2.0  # Important information at end\n",
        "            label = 1\n",
        "        else:\n",
        "            # Pattern requiring distributed attention\n",
        "            important_positions = np.random.choice(seq_length, 3, replace=False)\n",
        "            seq = np.random.randn(seq_length, features) * 0.5\n",
        "            seq[important_positions] += 1.5\n",
        "            label = 2\n",
        "        \n",
        "        X.append(seq)\n",
        "        y.append(label)\n",
        "    \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Main execution\n",
        "print(\"Creating complex sequence data for attention-LSTM fusion...\")\n",
        "X, y = create_complex_sequence_data(num_samples=600, seq_length=30, features=12)\n",
        "\n",
        "# Split data\n",
        "split_idx = int(0.8 * len(X))\n",
        "X_train, X_val = X[:split_idx], X[split_idx:]\n",
        "y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Validation data shape: {X_val.shape}\")\n",
        "\n",
        "# Create attention-LSTM fusion models\n",
        "input_shape = X_train.shape[1:]\n",
        "fusion_models = create_attention_lstm_models(input_shape, lstm_units=64, num_classes=3)\n",
        "\n",
        "print(f\"\\nCreated {len(fusion_models)} attention-LSTM fusion models:\")\n",
        "for name, model in fusion_models.items():\n",
        "    print(f\"- {name}: {model.count_params():,} parameters\")\n",
        "\n",
        "# Train and evaluate models (simplified for demonstration)\n",
        "print(f\"\\nTraining attention-LSTM fusion models...\")\n",
        "results = {}\n",
        "\n",
        "for name, model in fusion_models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=10,\n",
        "        batch_size=32,\n",
        "        verbose=0\n",
        "    )\n",
        "    \n",
        "    results[name] = {\n",
        "        'model': model,\n",
        "        'history': history.history,\n",
        "        'final_val_acc': history.history['val_accuracy'][-1]\n",
        "    }\n",
        "    \n",
        "    print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
        "\n",
        "# Visualize attention patterns\n",
        "print(f\"\\nGenerating attention visualizations...\")\n",
        "visualizer = AttentionVisualizer()\n",
        "\n",
        "# Use one of the models for attention visualization\n",
        "sample_model = fusion_models['Post_LSTM_Attention']\n",
        "sample_data = X_val[:5]  # First 5 validation samples\n",
        "\n",
        "attention_weights = visualizer.extract_attention_weights(sample_model, sample_data)\n",
        "visualizer.visualize_attention_patterns(attention_weights)\n",
        "\n",
        "# Performance comparison\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Training curves\n",
        "plt.subplot(2, 3, 1)\n",
        "for name, result in results.items():\n",
        "    plt.plot(result['history']['val_accuracy'], label=name, alpha=0.7)\n",
        "plt.title('Validation Accuracy Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Final performance comparison\n",
        "plt.subplot(2, 3, 2)\n",
        "names = list(results.keys())\n",
        "final_accs = [results[name]['final_val_acc'] for name in names]\n",
        "bars = plt.bar(range(len(names)), final_accs, alpha=0.7)\n",
        "plt.title('Final Model Performance')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.xticks(range(len(names)), names, rotation=45, ha='right')\n",
        "\n",
        "for bar, acc in zip(bars, final_accs):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
        "             f'{acc:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# Parameter efficiency\n",
        "plt.subplot(2, 3, 3)\n",
        "param_counts = [fusion_models[name].count_params() for name in names]\n",
        "efficiency = [acc / (params / 1000) for acc, params in zip(final_accs, param_counts)]\n",
        "plt.bar(range(len(names)), efficiency, alpha=0.7, color='green')\n",
        "plt.title('Parameter Efficiency')\n",
        "plt.ylabel('Accuracy per 1K Parameters')\n",
        "plt.xticks(range(len(names)), names, rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nAttention-LSTM Fusion Analysis Summary:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "best_model = max(results.keys(), key=lambda x: results[x]['final_val_acc'])\n",
        "best_accuracy = results[best_model]['final_val_acc']\n",
        "\n",
        "for name, result in results.items():\n",
        "    acc = result['final_val_acc']\n",
        "    params = fusion_models[name].count_params()\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Validation Accuracy: {acc:.4f}\")\n",
        "    print(f\"  Parameters: {params:,}\")\n",
        "\n",
        "print(f\"\\nBest performing fusion model: {best_model} ({best_accuracy:.4f} accuracy)\")\n",
        "\n",
        "print(f\"\\nKey Insights from Attention-LSTM Fusion:\")\n",
        "print(\"- Attention mechanisms enhance LSTM capabilities for complex patterns\")\n",
        "print(\"- Different fusion strategies work better for different task types\")\n",
        "print(\"- Hierarchical fusion can capture multi-level dependencies\")\n",
        "print(\"- Attention provides interpretability into model decisions\")\n",
        "\n",
        "print(f\"\\nAttention Fusion Systems Complete!\")\n",
        "print(f\"Advanced LSTM architectures with attention integration mastered!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
