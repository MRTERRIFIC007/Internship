{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Short Term Memory Analysis in Neural Networks\n",
        "\n",
        "## Learning Objectives\n",
        "In this notebook, we'll explore memory mechanisms in neural networks:\n",
        "- Understanding short-term vs long-term memory concepts\n",
        "- Analyzing memory limitations in SimpleRNNs\n",
        "- Deep dive into the vanishing gradient problem\n",
        "- Measuring memory capacity and information flow\n",
        "- Exploring information bottleneck concepts\n",
        "- Comparing neural and human memory systems\n",
        "\n",
        "## Introduction to Memory in Neural Networks\n",
        "\n",
        "Memory is crucial for sequence modeling tasks. Unlike humans who have sophisticated memory systems, neural networks must learn to selectively store and retrieve information through their architecture and parameters.\n",
        "\n",
        "### Types of Memory in Neural Networks:\n",
        "1. **Short-term Memory**: Immediate information storage (hidden states)\n",
        "2. **Long-term Memory**: Persistent information storage (learned parameters)\n",
        "3. **Working Memory**: Active manipulation of information (attention mechanisms)\n",
        "4. **Episodic Memory**: Specific sequence or event memory (external memory systems)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Short-term memory analysis initialized!\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Memory Capacity Testing Framework\n",
        "class MemoryCapacityTester:\n",
        "    \"\"\"\n",
        "    A framework for testing memory capacity of RNN architectures\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, sequence_length=50, vocab_size=10):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "    def generate_memory_task(self, task_type=\"copy\", delay=10, num_samples=1000):\n",
        "        \"\"\"\n",
        "        Generate different memory tasks to test RNN capabilities\n",
        "        \"\"\"\n",
        "        X = []\n",
        "        y = []\n",
        "        \n",
        "        if task_type == \"copy\":\n",
        "            # Copy task: remember and reproduce a sequence after delay\n",
        "            for _ in range(num_samples):\n",
        "                # Generate random sequence\n",
        "                sequence = np.random.randint(1, self.vocab_size, size=self.sequence_length//2)\n",
        "                \n",
        "                # Add delimiter and padding\n",
        "                full_sequence = np.concatenate([\n",
        "                    sequence,\n",
        "                    [0] * delay,  # Delimiter/padding\n",
        "                    [0] * (self.sequence_length//2)  # Space for output\n",
        "                ])\n",
        "                \n",
        "                # Target is the original sequence\n",
        "                target = np.concatenate([\n",
        "                    [0] * (self.sequence_length//2 + delay),  # No output during input\n",
        "                    sequence  # Reproduce the sequence\n",
        "                ])\n",
        "                \n",
        "                X.append(full_sequence[:self.sequence_length])\n",
        "                y.append(target[:self.sequence_length])\n",
        "                \n",
        "        elif task_type == \"reverse\":\n",
        "            # Reverse task: remember and reproduce sequence in reverse order\n",
        "            for _ in range(num_samples):\n",
        "                sequence = np.random.randint(1, self.vocab_size, size=self.sequence_length//2)\n",
        "                \n",
        "                full_sequence = np.concatenate([\n",
        "                    sequence,\n",
        "                    [0] * delay,\n",
        "                    [0] * (self.sequence_length//2)\n",
        "                ])\n",
        "                \n",
        "                target = np.concatenate([\n",
        "                    [0] * (self.sequence_length//2 + delay),\n",
        "                    sequence[::-1]  # Reversed sequence\n",
        "                ])\n",
        "                \n",
        "                X.append(full_sequence[:self.sequence_length])\n",
        "                y.append(target[:self.sequence_length])\n",
        "                \n",
        "        elif task_type == \"selective\":\n",
        "            # Selective memory: remember only specific elements\n",
        "            for _ in range(num_samples):\n",
        "                sequence = np.random.randint(1, self.vocab_size, size=self.sequence_length//2)\n",
        "                # Mark important elements with high values\n",
        "                important_indices = np.random.choice(len(sequence), size=3, replace=False)\n",
        "                sequence[important_indices] += self.vocab_size\n",
        "                \n",
        "                full_sequence = np.concatenate([\n",
        "                    sequence,\n",
        "                    [0] * delay,\n",
        "                    [0] * (self.sequence_length//2)\n",
        "                ])\n",
        "                \n",
        "                # Target contains only the important elements\n",
        "                target_seq = np.zeros(self.sequence_length//2)\n",
        "                target_seq[important_indices] = sequence[important_indices] - self.vocab_size\n",
        "                \n",
        "                target = np.concatenate([\n",
        "                    [0] * (self.sequence_length//2 + delay),\n",
        "                    target_seq\n",
        "                ])\n",
        "                \n",
        "                X.append(full_sequence[:self.sequence_length])\n",
        "                y.append(target[:self.sequence_length])\n",
        "        \n",
        "        return np.array(X), np.array(y)\n",
        "    \n",
        "    def create_long_dependency_task(self, dependency_length=20):\n",
        "        \"\"\"\n",
        "        Create task requiring long-term dependencies\n",
        "        \"\"\"\n",
        "        X = []\n",
        "        y = []\n",
        "        \n",
        "        for _ in range(1000):\n",
        "            # Create sequence with important information at the beginning\n",
        "            important_info = np.random.randint(1, self.vocab_size)\n",
        "            \n",
        "            sequence = np.random.randint(1, self.vocab_size, size=self.sequence_length)\n",
        "            sequence[0] = important_info  # Important info at start\n",
        "            \n",
        "            # Target depends on the first element\n",
        "            target = np.zeros(self.sequence_length)\n",
        "            target[-1] = important_info  # Must remember from beginning to end\n",
        "            \n",
        "            X.append(sequence)\n",
        "            y.append(target)\n",
        "        \n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "# 2. Gradient Flow Analysis\n",
        "class GradientFlowAnalyzer:\n",
        "    \"\"\"\n",
        "    Analyze gradient flow through RNN architectures\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.gradients = []\n",
        "        \n",
        "    def compute_gradients(self, X, y):\n",
        "        \"\"\"\n",
        "        Compute gradients for a batch of data\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.model(X, training=True)\n",
        "            loss = tf.keras.losses.sparse_categorical_crossentropy(y, predictions)\n",
        "            loss = tf.reduce_mean(loss)\n",
        "        \n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "        return gradients, loss\n",
        "    \n",
        "    def analyze_gradient_norms(self, X, y, num_batches=10):\n",
        "        \"\"\"\n",
        "        Analyze gradient norms across different layers\n",
        "        \"\"\"\n",
        "        gradient_norms = []\n",
        "        \n",
        "        for i in range(num_batches):\n",
        "            batch_X = X[i*32:(i+1)*32]\n",
        "            batch_y = y[i*32:(i+1)*32]\n",
        "            \n",
        "            gradients, loss = self.compute_gradients(batch_X, batch_y)\n",
        "            \n",
        "            batch_norms = []\n",
        "            for grad in gradients:\n",
        "                if grad is not None:\n",
        "                    norm = tf.norm(grad).numpy()\n",
        "                    batch_norms.append(norm)\n",
        "                else:\n",
        "                    batch_norms.append(0.0)\n",
        "            \n",
        "            gradient_norms.append(batch_norms)\n",
        "        \n",
        "        return np.array(gradient_norms)\n",
        "\n",
        "# Initialize memory tester\n",
        "memory_tester = MemoryCapacityTester(sequence_length=40, vocab_size=8)\n",
        "\n",
        "# Generate different memory tasks\n",
        "print(\"Generating memory tasks...\")\n",
        "copy_X, copy_y = memory_tester.generate_memory_task(\"copy\", delay=10, num_samples=500)\n",
        "reverse_X, reverse_y = memory_tester.generate_memory_task(\"reverse\", delay=5, num_samples=500)\n",
        "selective_X, selective_y = memory_tester.generate_memory_task(\"selective\", delay=8, num_samples=500)\n",
        "long_dep_X, long_dep_y = memory_tester.create_long_dependency_task(dependency_length=30)\n",
        "\n",
        "print(f\"Copy task shapes: X={copy_X.shape}, y={copy_y.shape}\")\n",
        "print(f\"Reverse task shapes: X={reverse_X.shape}, y={reverse_y.shape}\")\n",
        "print(f\"Selective task shapes: X={selective_X.shape}, y={selective_y.shape}\")\n",
        "print(f\"Long dependency task shapes: X={long_dep_X.shape}, y={long_dep_y.shape}\")\n",
        "\n",
        "# Visualize memory tasks\n",
        "plt.figure(figsize=(16, 10))\n",
        "\n",
        "# Copy task visualization\n",
        "plt.subplot(2, 4, 1)\n",
        "sample_idx = 0\n",
        "plt.plot(copy_X[sample_idx], 'b-', label='Input', alpha=0.7)\n",
        "plt.plot(copy_y[sample_idx], 'r--', label='Target', alpha=0.7)\n",
        "plt.title('Copy Task Example')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "\n",
        "# Reverse task visualization  \n",
        "plt.subplot(2, 4, 2)\n",
        "plt.plot(reverse_X[sample_idx], 'b-', label='Input', alpha=0.7)\n",
        "plt.plot(reverse_y[sample_idx], 'r--', label='Target', alpha=0.7)\n",
        "plt.title('Reverse Task Example')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "\n",
        "# Selective memory task\n",
        "plt.subplot(2, 4, 3)\n",
        "plt.plot(selective_X[sample_idx], 'b-', label='Input', alpha=0.7)\n",
        "plt.plot(selective_y[sample_idx], 'r--', label='Target', alpha=0.7)\n",
        "plt.title('Selective Memory Task')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "\n",
        "# Long dependency task\n",
        "plt.subplot(2, 4, 4)\n",
        "plt.plot(long_dep_X[sample_idx], 'b-', label='Input', alpha=0.7)\n",
        "plt.plot(long_dep_y[sample_idx], 'r--', label='Target', alpha=0.7)\n",
        "plt.title('Long Dependency Task')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "\n",
        "# Task complexity analysis\n",
        "plt.subplot(2, 4, 5)\n",
        "tasks = ['Copy', 'Reverse', 'Selective', 'Long Dep']\n",
        "input_complexity = [\n",
        "    np.std(copy_X.flatten()),\n",
        "    np.std(reverse_X.flatten()),\n",
        "    np.std(selective_X.flatten()),\n",
        "    np.std(long_dep_X.flatten())\n",
        "]\n",
        "plt.bar(tasks, input_complexity, color='skyblue', alpha=0.7)\n",
        "plt.title('Input Complexity (Std Dev)')\n",
        "plt.ylabel('Standard Deviation')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Memory requirement analysis\n",
        "plt.subplot(2, 4, 6)\n",
        "memory_requirements = [10, 15, 12, 35]  # Estimated memory steps needed\n",
        "plt.bar(tasks, memory_requirements, color='lightcoral', alpha=0.7)\n",
        "plt.title('Estimated Memory Requirements')\n",
        "plt.ylabel('Memory Steps')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Information flow analysis\n",
        "plt.subplot(2, 4, 7)\n",
        "# Calculate information flow (entropy-based measure)\n",
        "def calculate_entropy(data):\n",
        "    unique, counts = np.unique(data.flatten(), return_counts=True)\n",
        "    probs = counts / counts.sum()\n",
        "    return -np.sum(probs * np.log2(probs + 1e-10))\n",
        "\n",
        "entropies = [\n",
        "    calculate_entropy(copy_X),\n",
        "    calculate_entropy(reverse_X),\n",
        "    calculate_entropy(selective_X),\n",
        "    calculate_entropy(long_dep_X)\n",
        "]\n",
        "\n",
        "plt.bar(tasks, entropies, color='lightgreen', alpha=0.7)\n",
        "plt.title('Information Content (Entropy)')\n",
        "plt.ylabel('Entropy (bits)')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Task difficulty ranking\n",
        "plt.subplot(2, 4, 8)\n",
        "difficulty_scores = [2, 3, 4, 5]  # Subjective difficulty ranking\n",
        "colors = ['green', 'yellow', 'orange', 'red']\n",
        "bars = plt.bar(tasks, difficulty_scores, color=colors, alpha=0.7)\n",
        "plt.title('Estimated Task Difficulty')\n",
        "plt.ylabel('Difficulty Score')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Add difficulty labels\n",
        "for bar, score in zip(bars, difficulty_scores):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "             f'{score}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nMemory Task Analysis Complete!\")\n",
        "print(f\"Generated {len(copy_X)} samples for each task type\")\n",
        "print(f\"Tasks range from simple copy to complex long-term dependencies\")\n",
        "print(f\"Ready to test RNN memory capabilities!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
