{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# LSTM Variations and Improvements\n",
        "\n",
        "## Learning Objectives\n",
        "In this notebook, we'll explore advanced LSTM variants and architectural improvements:\n",
        "- Peephole connections in LSTMs and their benefits\n",
        "- Coupled forget and input gates analysis\n",
        "- Detailed GRU vs LSTM comparison\n",
        "- Bidirectional LSTM architectures\n",
        "- Stacked and hierarchical LSTM designs\n",
        "- Performance analysis of different variants\n",
        "\n",
        "## Introduction to LSTM Variants\n",
        "\n",
        "While the standard LSTM is powerful, several variants have been proposed to address specific limitations or improve performance:\n",
        "\n",
        "### Key LSTM Variants:\n",
        "1. **Peephole LSTM**: Cell state directly influences gates\n",
        "2. **Coupled Gates LSTM**: Forget and input gates are coupled\n",
        "3. **GRU (Gated Recurrent Unit)**: Simplified architecture with reset and update gates\n",
        "4. **Bidirectional LSTM**: Processes sequences in both directions\n",
        "5. **Stacked LSTM**: Multiple LSTM layers for hierarchical learning\n",
        "\n",
        "Each variant offers specific advantages for different types of sequence modeling tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"LSTM Variations Comparison initialized!\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# 1. LSTM Variant Architectures\n",
        "def create_lstm_variants(input_shape, units=64, num_classes=3):\n",
        "    \"\"\"\n",
        "    Create different LSTM variant architectures\n",
        "    \"\"\"\n",
        "    models = {}\n",
        "    \n",
        "    # Standard LSTM\n",
        "    models['Standard_LSTM'] = keras.Sequential([\n",
        "        layers.LSTM(units, return_sequences=False),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='Standard_LSTM')\n",
        "    \n",
        "    # Bidirectional LSTM\n",
        "    models['Bidirectional_LSTM'] = keras.Sequential([\n",
        "        layers.Bidirectional(layers.LSTM(units//2, return_sequences=False)),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='Bidirectional_LSTM')\n",
        "    \n",
        "    # Stacked LSTM\n",
        "    models['Stacked_LSTM'] = keras.Sequential([\n",
        "        layers.LSTM(units, return_sequences=True),\n",
        "        layers.LSTM(units//2, return_sequences=False),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='Stacked_LSTM')\n",
        "    \n",
        "    # GRU for comparison\n",
        "    models['GRU'] = keras.Sequential([\n",
        "        layers.GRU(units, return_sequences=False),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='GRU')\n",
        "    \n",
        "    # Bidirectional GRU\n",
        "    models['Bidirectional_GRU'] = keras.Sequential([\n",
        "        layers.Bidirectional(layers.GRU(units//2, return_sequences=False)),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='Bidirectional_GRU')\n",
        "    \n",
        "    return models\n",
        "\n",
        "# 2. Performance Analysis Framework\n",
        "class LSTMVariantAnalyzer:\n",
        "    \"\"\"\n",
        "    Analyze and compare different LSTM variants\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.results = {}\n",
        "        \n",
        "    def compare_architectures(self, models, X_train, y_train, X_val, y_val, epochs=10):\n",
        "        \"\"\"\n",
        "        Train and compare different LSTM variants\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        \n",
        "        for name, model in models.items():\n",
        "            print(f\"\\nTraining {name}...\")\n",
        "            \n",
        "            # Compile model\n",
        "            model.compile(\n",
        "                optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "            \n",
        "            # Train model\n",
        "            history = model.fit(\n",
        "                X_train, y_train,\n",
        "                validation_data=(X_val, y_val),\n",
        "                epochs=epochs,\n",
        "                batch_size=32,\n",
        "                verbose=0\n",
        "            )\n",
        "            \n",
        "            # Store results\n",
        "            results[name] = {\n",
        "                'model': model,\n",
        "                'history': history.history,\n",
        "                'params': model.count_params(),\n",
        "                'final_val_acc': history.history['val_accuracy'][-1],\n",
        "                'final_val_loss': history.history['val_loss'][-1]\n",
        "            }\n",
        "            \n",
        "            print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
        "        \n",
        "        self.results = results\n",
        "        return results\n",
        "    \n",
        "    def visualize_comparison(self):\n",
        "        \"\"\"\n",
        "        Create comprehensive comparison visualization\n",
        "        \"\"\"\n",
        "        if not self.results:\n",
        "            print(\"No results to visualize. Run comparison first.\")\n",
        "            return\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        \n",
        "        # Training accuracy curves\n",
        "        axes[0, 0].set_title('Training Accuracy')\n",
        "        for name, result in self.results.items():\n",
        "            axes[0, 0].plot(result['history']['accuracy'], label=name, alpha=0.7)\n",
        "        axes[0, 0].set_xlabel('Epoch')\n",
        "        axes[0, 0].set_ylabel('Accuracy')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Validation accuracy curves\n",
        "        axes[0, 1].set_title('Validation Accuracy')\n",
        "        for name, result in self.results.items():\n",
        "            axes[0, 1].plot(result['history']['val_accuracy'], label=name, alpha=0.7)\n",
        "        axes[0, 1].set_xlabel('Epoch')\n",
        "        axes[0, 1].set_ylabel('Accuracy')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Final performance comparison\n",
        "        names = list(self.results.keys())\n",
        "        final_accs = [self.results[name]['final_val_acc'] for name in names]\n",
        "        \n",
        "        bars = axes[0, 2].bar(range(len(names)), final_accs, alpha=0.7)\n",
        "        axes[0, 2].set_title('Final Validation Accuracy')\n",
        "        axes[0, 2].set_ylabel('Accuracy')\n",
        "        axes[0, 2].set_xticks(range(len(names)))\n",
        "        axes[0, 2].set_xticklabels(names, rotation=45, ha='right')\n",
        "        \n",
        "        # Add value labels\n",
        "        for bar, acc in zip(bars, final_accs):\n",
        "            axes[0, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
        "                           f'{acc:.3f}', ha='center', va='bottom')\n",
        "        \n",
        "        # Parameter count comparison\n",
        "        param_counts = [self.results[name]['params'] for name in names]\n",
        "        axes[1, 0].bar(range(len(names)), param_counts, alpha=0.7, color='orange')\n",
        "        axes[1, 0].set_title('Parameter Count')\n",
        "        axes[1, 0].set_ylabel('Number of Parameters')\n",
        "        axes[1, 0].set_xticks(range(len(names)))\n",
        "        axes[1, 0].set_xticklabels(names, rotation=45, ha='right')\n",
        "        \n",
        "        # Efficiency analysis (accuracy per parameter)\n",
        "        efficiency = [acc / (params / 1000) for acc, params in zip(final_accs, param_counts)]\n",
        "        axes[1, 1].bar(range(len(names)), efficiency, alpha=0.7, color='green')\n",
        "        axes[1, 1].set_title('Efficiency (Accuracy per 1K Parameters)')\n",
        "        axes[1, 1].set_ylabel('Efficiency Score')\n",
        "        axes[1, 1].set_xticks(range(len(names)))\n",
        "        axes[1, 1].set_xticklabels(names, rotation=45, ha='right')\n",
        "        \n",
        "        # Training loss comparison\n",
        "        axes[1, 2].set_title('Training Loss')\n",
        "        for name, result in self.results.items():\n",
        "            axes[1, 2].plot(result['history']['loss'], label=name, alpha=0.7)\n",
        "        axes[1, 2].set_xlabel('Epoch')\n",
        "        axes[1, 2].set_ylabel('Loss')\n",
        "        axes[1, 2].legend()\n",
        "        axes[1, 2].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Create sample data for comparison\n",
        "def create_sample_sequence_data(num_samples=1000, seq_length=50, features=10):\n",
        "    \"\"\"\n",
        "    Create sample sequence data for LSTM variant comparison\n",
        "    \"\"\"\n",
        "    # Generate sequences with different patterns\n",
        "    X = []\n",
        "    y = []\n",
        "    \n",
        "    for i in range(num_samples):\n",
        "        # Create different sequence patterns\n",
        "        if i % 3 == 0:\n",
        "            # Pattern 1: Increasing trend\n",
        "            seq = np.random.randn(seq_length, features) + np.linspace(0, 2, seq_length).reshape(-1, 1)\n",
        "            label = 0\n",
        "        elif i % 3 == 1:\n",
        "            # Pattern 2: Decreasing trend\n",
        "            seq = np.random.randn(seq_length, features) + np.linspace(2, 0, seq_length).reshape(-1, 1)\n",
        "            label = 1\n",
        "        else:\n",
        "            # Pattern 3: Oscillating pattern\n",
        "            seq = np.random.randn(seq_length, features) + np.sin(np.linspace(0, 4*np.pi, seq_length)).reshape(-1, 1)\n",
        "            label = 2\n",
        "        \n",
        "        X.append(seq)\n",
        "        y.append(label)\n",
        "    \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Generate sample data\n",
        "print(\"Generating sample sequence data...\")\n",
        "X, y = create_sample_sequence_data(num_samples=800, seq_length=30, features=8)\n",
        "\n",
        "# Split data\n",
        "split_idx = int(0.8 * len(X))\n",
        "X_train, X_val = X[:split_idx], X[split_idx:]\n",
        "y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Validation data shape: {X_val.shape}\")\n",
        "\n",
        "# Create LSTM variants\n",
        "input_shape = X_train.shape[1:]\n",
        "lstm_variants = create_lstm_variants(input_shape, units=64, num_classes=3)\n",
        "\n",
        "print(f\"\\nCreated {len(lstm_variants)} LSTM variants:\")\n",
        "for name, model in lstm_variants.items():\n",
        "    print(f\"- {name}: {model.count_params():,} parameters\")\n",
        "\n",
        "# Initialize analyzer and run comparison\n",
        "analyzer = LSTMVariantAnalyzer()\n",
        "print(f\"\\nRunning LSTM variant comparison...\")\n",
        "\n",
        "# Compare architectures\n",
        "results = analyzer.compare_architectures(\n",
        "    lstm_variants, X_train, y_train, X_val, y_val, epochs=15\n",
        ")\n",
        "\n",
        "# Visualize results\n",
        "print(f\"\\nGenerating comparison visualization...\")\n",
        "analyzer.visualize_comparison()\n",
        "\n",
        "# Print detailed analysis\n",
        "print(f\"\\nLSTM Variant Analysis Summary:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "best_accuracy = 0\n",
        "best_model = \"\"\n",
        "best_efficiency = 0\n",
        "most_efficient = \"\"\n",
        "\n",
        "for name, result in results.items():\n",
        "    acc = result['final_val_acc']\n",
        "    params = result['params']\n",
        "    efficiency = acc / (params / 1000)\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Final Accuracy: {acc:.4f}\")\n",
        "    print(f\"  Parameters: {params:,}\")\n",
        "    print(f\"  Efficiency: {efficiency:.4f}\")\n",
        "    \n",
        "    if acc > best_accuracy:\n",
        "        best_accuracy = acc\n",
        "        best_model = name\n",
        "    \n",
        "    if efficiency > best_efficiency:\n",
        "        best_efficiency = efficiency\n",
        "        most_efficient = name\n",
        "\n",
        "print(f\"\\nBest performing model: {best_model} ({best_accuracy:.4f} accuracy)\")\n",
        "print(f\"Most efficient model: {most_efficient} ({best_efficiency:.4f} efficiency)\")\n",
        "\n",
        "print(f\"\\nKey Insights:\")\n",
        "print(\"- Bidirectional models often perform better but use more parameters\")\n",
        "print(\"- GRU can be more efficient than LSTM with similar performance\")\n",
        "print(\"- Stacked architectures provide hierarchical learning capabilities\")\n",
        "print(\"- Model choice depends on task complexity and computational constraints\")\n",
        "\n",
        "print(f\"\\nLSTM Variations Comparison Complete!\")\n",
        "print(f\"Ready for attention fusion with LSTM architectures!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
