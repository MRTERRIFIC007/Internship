{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# RNN Architectures Part 1: Many-to-One, Many-to-Many, CNN vs RNN\n",
        "\n",
        "## Learning Objectives\n",
        "In this notebook, we'll explore different RNN input-output configurations:\n",
        "- Many-to-One RNN architecture (sequence classification)\n",
        "- Many-to-Many RNN architecture (sequence-to-sequence)\n",
        "- Comparative analysis between CNNs and RNNs\n",
        "- Practical implementations for different use cases\n",
        "\n",
        "## RNN Architecture Types\n",
        "\n",
        "RNNs can be configured in different ways based on the input-output requirements:\n",
        "\n",
        "1. **One-to-One**: Traditional neural network (not truly recurrent)\n",
        "2. **One-to-Many**: Single input → sequence output (image captioning)\n",
        "3. **Many-to-One**: Sequence input → single output (sentiment analysis)\n",
        "4. **Many-to-Many**: Sequence input → sequence output (translation)\n",
        "5. **Many-to-Many (synced)**: Aligned input-output sequences (video classification)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import re\n",
        "import string\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Many-to-One RNN Architecture\n",
        "\n",
        "The Many-to-One configuration is one of the most common RNN patterns:\n",
        "- **Input**: Variable-length sequence\n",
        "- **Output**: Single fixed-size output\n",
        "- **Use Cases**: Sentiment analysis, document classification, sequence classification\n",
        "\n",
        "### Architecture Details:\n",
        "- Process entire sequence through RNN\n",
        "- Only use the final hidden state for prediction\n",
        "- Can use attention mechanisms to consider all time steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementing Many-to-One RNN for Sentiment Analysis\n",
        "def create_many_to_one_rnn(vocab_size, embedding_dim=100, rnn_units=64, max_length=100):\n",
        "    \"\"\"\n",
        "    Create a Many-to-One RNN for text classification\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        keras.layers.LSTM(rnn_units, return_sequences=False, dropout=0.2),\n",
        "        keras.layers.Dense(64, activation='relu'),\n",
        "        keras.layers.Dropout(0.5),\n",
        "        keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create sample text data for sentiment analysis\n",
        "def create_sentiment_data(n_samples=1000):\n",
        "    \"\"\"Create synthetic sentiment data\"\"\"\n",
        "    positive_words = ['good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', 'love', 'perfect']\n",
        "    negative_words = ['bad', 'terrible', 'awful', 'hate', 'horrible', 'disgusting', 'worst', 'pathetic']\n",
        "    neutral_words = ['the', 'and', 'is', 'in', 'to', 'of', 'it', 'that', 'have', 'for']\n",
        "    \n",
        "    texts = []\n",
        "    labels = []\n",
        "    \n",
        "    for _ in range(n_samples):\n",
        "        # Randomly choose sentiment\n",
        "        sentiment = np.random.choice([0, 1])  # 0: negative, 1: positive\n",
        "        \n",
        "        if sentiment == 1:\n",
        "            # Create positive text\n",
        "            words = np.random.choice(positive_words, size=np.random.randint(3, 8)).tolist()\n",
        "            words += np.random.choice(neutral_words, size=np.random.randint(2, 5)).tolist()\n",
        "        else:\n",
        "            # Create negative text\n",
        "            words = np.random.choice(negative_words, size=np.random.randint(3, 8)).tolist()\n",
        "            words += np.random.choice(neutral_words, size=np.random.randint(2, 5)).tolist()\n",
        "        \n",
        "        np.random.shuffle(words)\n",
        "        text = ' '.join(words)\n",
        "        \n",
        "        texts.append(text)\n",
        "        labels.append(sentiment)\n",
        "    \n",
        "    return texts, np.array(labels)\n",
        "\n",
        "# Generate sentiment data\n",
        "print(\"Creating sentiment analysis dataset...\")\n",
        "texts, labels = create_sentiment_data(2000)\n",
        "\n",
        "print(f\"Sample texts:\")\n",
        "for i in range(3):\n",
        "    print(f\"Text: '{texts[i]}' -> Label: {labels[i]} ({'Positive' if labels[i] else 'Negative'})\")\n",
        "\n",
        "# Tokenization and preprocessing\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "max_length = 20\n",
        "padded_sequences = keras.preprocessing.sequence.pad_sequences(\n",
        "    sequences, maxlen=max_length, truncating='post'\n",
        ")\n",
        "\n",
        "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
        "print(f\"Sequence shape: {padded_sequences.shape}\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    padded_sequences, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create and train Many-to-One model\n",
        "print(\"\\nCreating Many-to-One RNN model...\")\n",
        "model_many_to_one = create_many_to_one_rnn(\n",
        "    vocab_size=len(tokenizer.word_index) + 1,\n",
        "    embedding_dim=50,\n",
        "    rnn_units=32,\n",
        "    max_length=max_length\n",
        ")\n",
        "\n",
        "print(\"Model architecture:\")\n",
        "model_many_to_one.summary()\n",
        "\n",
        "print(\"\\nTraining Many-to-One model...\")\n",
        "history = model_many_to_one.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=32,\n",
        "    epochs=10,\n",
        "    validation_data=(X_test, y_test),\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Many-to-Many RNN Architecture\n",
        "\n",
        "Many-to-Many RNNs come in two main variants:\n",
        "1. **Encoder-Decoder**: Input and output sequences can have different lengths\n",
        "2. **Synchronized**: Input and output sequences have the same length\n",
        "\n",
        "### Use Cases:\n",
        "- **Encoder-Decoder**: Machine translation, text summarization\n",
        "- **Synchronized**: Part-of-speech tagging, named entity recognition\n",
        "\n",
        "### Architecture Details:\n",
        "- **Encoder-Decoder**: Two separate RNNs (encoder processes input, decoder generates output)\n",
        "- **Synchronized**: Single RNN with output at each time step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementing Many-to-Many RNN (Synchronized) for Sequence Tagging\n",
        "def create_many_to_many_rnn(vocab_size, tag_size, embedding_dim=50, rnn_units=64, max_length=20):\n",
        "    \"\"\"\n",
        "    Create a Many-to-Many RNN for sequence tagging\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        keras.layers.LSTM(rnn_units, return_sequences=True, dropout=0.2),\n",
        "        keras.layers.TimeDistributed(keras.layers.Dense(tag_size, activation='softmax'))\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create synthetic POS tagging data\n",
        "def create_pos_tagging_data(n_samples=1000):\n",
        "    \"\"\"Create synthetic POS tagging data\"\"\"\n",
        "    words = ['the', 'cat', 'dog', 'runs', 'jumps', 'quickly', 'slowly', 'big', 'small', 'red']\n",
        "    pos_tags = {\n",
        "        'the': 0,      # DET\n",
        "        'cat': 1,      # NOUN\n",
        "        'dog': 1,      # NOUN\n",
        "        'runs': 2,     # VERB\n",
        "        'jumps': 2,    # VERB\n",
        "        'quickly': 3,  # ADV\n",
        "        'slowly': 3,   # ADV\n",
        "        'big': 4,      # ADJ\n",
        "        'small': 4,    # ADJ\n",
        "        'red': 4       # ADJ\n",
        "    }\n",
        "    \n",
        "    sequences = []\n",
        "    tag_sequences = []\n",
        "    \n",
        "    for _ in range(n_samples):\n",
        "        seq_length = np.random.randint(3, 8)\n",
        "        sequence = np.random.choice(words, size=seq_length).tolist()\n",
        "        tags = [pos_tags[word] for word in sequence]\n",
        "        \n",
        "        sequences.append(sequence)\n",
        "        tag_sequences.append(tags)\n",
        "    \n",
        "    return sequences, tag_sequences, pos_tags\n",
        "\n",
        "print(\"Creating POS tagging dataset...\")\n",
        "sequences, tag_sequences, pos_dict = create_pos_tagging_data(1500)\n",
        "\n",
        "print(f\"Sample sequences:\")\n",
        "for i in range(3):\n",
        "    print(f\"Words: {sequences[i]}\")\n",
        "    print(f\"Tags:  {tag_sequences[i]}\")\n",
        "    print()\n",
        "\n",
        "# Create word-to-index mapping\n",
        "word_to_idx = {word: idx for idx, word in enumerate(set([word for seq in sequences for word in seq]))}\n",
        "word_to_idx['<PAD>'] = len(word_to_idx)\n",
        "\n",
        "# Convert to numerical sequences\n",
        "max_seq_length = 10\n",
        "X_sequences = []\n",
        "y_sequences = []\n",
        "\n",
        "for seq, tags in zip(sequences, tag_sequences):\n",
        "    # Convert words to indices\n",
        "    x_seq = [word_to_idx[word] for word in seq]\n",
        "    y_seq = tags\n",
        "    \n",
        "    # Pad sequences\n",
        "    if len(x_seq) < max_seq_length:\n",
        "        x_seq.extend([word_to_idx['<PAD>']] * (max_seq_length - len(x_seq)))\n",
        "        y_seq.extend([0] * (max_seq_length - len(y_seq)))  # Pad with DET tag\n",
        "    else:\n",
        "        x_seq = x_seq[:max_seq_length]\n",
        "        y_seq = y_seq[:max_seq_length]\n",
        "    \n",
        "    X_sequences.append(x_seq)\n",
        "    y_sequences.append(y_seq)\n",
        "\n",
        "X_sequences = np.array(X_sequences)\n",
        "y_sequences = np.array(y_sequences)\n",
        "\n",
        "print(f\"Sequence data shape: {X_sequences.shape}\")\n",
        "print(f\"Tag data shape: {y_sequences.shape}\")\n",
        "\n",
        "# Split data\n",
        "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(\n",
        "    X_sequences, y_sequences, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create and train Many-to-Many model\n",
        "print(\"\\nCreating Many-to-Many RNN model...\")\n",
        "model_many_to_many = create_many_to_many_rnn(\n",
        "    vocab_size=len(word_to_idx),\n",
        "    tag_size=5,  # Number of POS tags\n",
        "    embedding_dim=30,\n",
        "    rnn_units=32,\n",
        "    max_length=max_seq_length\n",
        ")\n",
        "\n",
        "print(\"Model architecture:\")\n",
        "model_many_to_many.summary()\n",
        "\n",
        "print(\"\\nTraining Many-to-Many model...\")\n",
        "history_seq = model_many_to_many.fit(\n",
        "    X_train_seq, y_train_seq,\n",
        "    batch_size=32,\n",
        "    epochs=15,\n",
        "    validation_data=(X_test_seq, y_test_seq),\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. CNN vs RNN Comparison\n",
        "\n",
        "### Key Differences:\n",
        "\n",
        "| Aspect | CNN | RNN |\n",
        "|--------|-----|-----|\n",
        "| **Processing** | Parallel | Sequential |\n",
        "| **Dependencies** | Local patterns | Long-range dependencies |\n",
        "| **Training Speed** | Fast (parallel) | Slower (sequential) |\n",
        "| **Memory** | Fixed receptive field | Variable memory |\n",
        "| **Use Cases** | Images, local text patterns | Sequences, temporal data |\n",
        "\n",
        "### When to Choose:\n",
        "- **CNNs**: When local patterns matter more than order\n",
        "- **RNNs**: When sequential order and long-term dependencies are crucial\n",
        "- **Hybrid**: Combine both for complex tasks (CNN for feature extraction + RNN for sequence modeling)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CNN vs RNN Comparison for Text Classification\n",
        "def create_cnn_model(vocab_size, embedding_dim=50, max_length=20):\n",
        "    \"\"\"Create a CNN model for text classification\"\"\"\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "        keras.layers.GlobalMaxPooling1D(),\n",
        "        keras.layers.Dense(64, activation='relu'),\n",
        "        keras.layers.Dropout(0.5),\n",
        "        keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create CNN model for comparison\n",
        "print(\"Creating CNN model for comparison...\")\n",
        "cnn_model = create_cnn_model(\n",
        "    vocab_size=len(tokenizer.word_index) + 1,\n",
        "    embedding_dim=50,\n",
        "    max_length=max_length\n",
        ")\n",
        "\n",
        "print(\"CNN Model architecture:\")\n",
        "cnn_model.summary()\n",
        "\n",
        "# Train CNN model\n",
        "print(\"\\nTraining CNN model...\")\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "cnn_history = cnn_model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=32,\n",
        "    epochs=10,\n",
        "    validation_data=(X_test, y_test),\n",
        "    verbose=0\n",
        ")\n",
        "cnn_training_time = time.time() - start_time\n",
        "\n",
        "# Compare performance\n",
        "rnn_accuracy = model_many_to_one.evaluate(X_test, y_test, verbose=0)[1]\n",
        "cnn_accuracy = cnn_model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "\n",
        "print(f\"\\nPerformance Comparison:\")\n",
        "print(f\"RNN Accuracy: {rnn_accuracy:.4f}\")\n",
        "print(f\"CNN Accuracy: {cnn_accuracy:.4f}\")\n",
        "print(f\"CNN Training Time: {cnn_training_time:.2f} seconds\")\n",
        "\n",
        "# Model parameter comparison\n",
        "rnn_params = model_many_to_one.count_params()\n",
        "cnn_params = cnn_model.count_params()\n",
        "\n",
        "print(f\"\\nModel Complexity:\")\n",
        "print(f\"RNN Parameters: {rnn_params:,}\")\n",
        "print(f\"CNN Parameters: {cnn_params:,}\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Accuracy comparison\n",
        "models = ['RNN (LSTM)', 'CNN']\n",
        "accuracies = [rnn_accuracy, cnn_accuracy]\n",
        "colors = ['lightcoral', 'lightblue']\n",
        "\n",
        "axes[0, 0].bar(models, accuracies, color=colors)\n",
        "axes[0, 0].set_title('Model Accuracy Comparison')\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].set_ylim(0.5, 1.0)\n",
        "for i, v in enumerate(accuracies):\n",
        "    axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
        "\n",
        "# Parameter comparison\n",
        "param_counts = [rnn_params, cnn_params]\n",
        "axes[0, 1].bar(models, param_counts, color=colors)\n",
        "axes[0, 1].set_title('Model Complexity (Parameters)')\n",
        "axes[0, 1].set_ylabel('Number of Parameters')\n",
        "for i, v in enumerate(param_counts):\n",
        "    axes[0, 1].text(i, v + 1000, f'{v:,}', ha='center')\n",
        "\n",
        "# Training curves\n",
        "axes[1, 0].plot(history.history['val_accuracy'], label='RNN', linewidth=2)\n",
        "axes[1, 0].plot(cnn_history.history['val_accuracy'], label='CNN', linewidth=2)\n",
        "axes[1, 0].set_title('Validation Accuracy During Training')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Validation Accuracy')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss curves\n",
        "axes[1, 1].plot(history.history['val_loss'], label='RNN', linewidth=2)\n",
        "axes[1, 1].plot(cnn_history.history['val_loss'], label='CNN', linewidth=2)\n",
        "axes[1, 1].set_title('Validation Loss During Training')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Validation Loss')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Insights:\")\n",
        "print(\"1. CNNs often train faster due to parallel processing\")\n",
        "print(\"2. RNNs better capture sequential dependencies\")\n",
        "print(\"3. Choice depends on the specific task requirements\")\n",
        "print(\"4. For short texts, CNNs might be sufficient\")\n",
        "print(\"5. For long sequences with complex dependencies, RNNs excel\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
