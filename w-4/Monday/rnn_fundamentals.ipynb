{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# RNN Fundamentals: Understanding Recurrent Neural Networks\n",
        "\n",
        "## Learning Objectives\n",
        "In this notebook, we'll explore the fundamental concepts of Recurrent Neural Networks (RNNs):\n",
        "- Understanding sequential data and temporal dependencies\n",
        "- RNN architecture and recurrent connections\n",
        "- Hidden state propagation through time\n",
        "- Unfolding RNNs through time steps\n",
        "- Mathematical formulation of RNN computations\n",
        "\n",
        "## Introduction to Sequential Data\n",
        "\n",
        "Traditional neural networks process fixed-size inputs and produce fixed-size outputs. However, many real-world problems involve sequential data where:\n",
        "- The order of inputs matters\n",
        "- The length of sequences can vary\n",
        "- Dependencies exist between distant elements\n",
        "\n",
        "Examples include:\n",
        "- Natural language (words in sentences)\n",
        "- Time series data (stock prices, weather)\n",
        "- Audio signals (speech, music)\n",
        "- DNA sequences\n",
        "\n",
        "RNNs are designed to handle such sequential data by maintaining a \"memory\" of previous inputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. RNN Architecture Overview\n",
        "\n",
        "### Basic RNN Structure\n",
        "A Recurrent Neural Network has the following key components:\n",
        "\n",
        "1. **Input Layer**: Receives sequential input data\n",
        "2. **Hidden Layer**: Contains recurrent connections (memory)\n",
        "3. **Output Layer**: Produces predictions\n",
        "\n",
        "### Key Differences from Feedforward Networks:\n",
        "- **Recurrent Connections**: Hidden layer connects to itself\n",
        "- **Shared Parameters**: Same weights are used across all time steps\n",
        "- **Variable Input Length**: Can handle sequences of different lengths\n",
        "\n",
        "### Mathematical Formulation:\n",
        "For a simple RNN at time step t:\n",
        "- **Hidden State**: h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b_h)\n",
        "- **Output**: y_t = W_hy * h_t + b_y\n",
        "\n",
        "Where:\n",
        "- x_t: input at time t\n",
        "- h_t: hidden state at time t\n",
        "- y_t: output at time t\n",
        "- W_hh: hidden-to-hidden weights\n",
        "- W_xh: input-to-hidden weights\n",
        "- W_hy: hidden-to-output weights\n",
        "- b_h, b_y: bias terms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple RNN Implementation from Scratch\n",
        "class SimpleRNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"\n",
        "        Initialize a simple RNN\n",
        "        \n",
        "        Parameters:\n",
        "        input_size: dimension of input features\n",
        "        hidden_size: dimension of hidden state\n",
        "        output_size: dimension of output\n",
        "        \"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        # Initialize weights with small random values\n",
        "        self.W_xh = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "        self.W_hy = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        \n",
        "        # Initialize biases\n",
        "        self.b_h = np.zeros((1, hidden_size))\n",
        "        self.b_y = np.zeros((1, output_size))\n",
        "        \n",
        "        # Initialize hidden state\n",
        "        self.h = np.zeros((1, hidden_size))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the RNN\n",
        "        \n",
        "        Parameters:\n",
        "        x: input sequence (sequence_length, input_size)\n",
        "        \n",
        "        Returns:\n",
        "        outputs: output sequence (sequence_length, output_size)\n",
        "        hidden_states: hidden states (sequence_length, hidden_size)\n",
        "        \"\"\"\n",
        "        sequence_length = x.shape[0]\n",
        "        \n",
        "        outputs = []\n",
        "        hidden_states = []\n",
        "        \n",
        "        # Process each time step\n",
        "        for t in range(sequence_length):\n",
        "            # Update hidden state\n",
        "            self.h = np.tanh(np.dot(x[t:t+1], self.W_xh) + \n",
        "                           np.dot(self.h, self.W_hh) + self.b_h)\n",
        "            \n",
        "            # Compute output\n",
        "            output = np.dot(self.h, self.W_hy) + self.b_y\n",
        "            \n",
        "            outputs.append(output)\n",
        "            hidden_states.append(self.h.copy())\n",
        "        \n",
        "        return np.array(outputs), np.array(hidden_states)\n",
        "    \n",
        "    def reset_hidden_state(self):\n",
        "        \"\"\"Reset hidden state to zeros\"\"\"\n",
        "        self.h = np.zeros((1, self.hidden_size))\n",
        "\n",
        "# Test the simple RNN\n",
        "print(\"Testing Simple RNN Implementation:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Create a simple RNN\n",
        "rnn = SimpleRNN(input_size=3, hidden_size=5, output_size=2)\n",
        "\n",
        "# Create dummy input sequence\n",
        "sequence_length = 4\n",
        "input_sequence = np.random.randn(sequence_length, 3)\n",
        "\n",
        "print(f\"Input sequence shape: {input_sequence.shape}\")\n",
        "\n",
        "# Forward pass\n",
        "outputs, hidden_states = rnn.forward(input_sequence)\n",
        "\n",
        "print(f\"Output sequence shape: {outputs.shape}\")\n",
        "print(f\"Hidden states shape: {hidden_states.shape}\")\n",
        "\n",
        "print(\"\\nInput at each time step:\")\n",
        "for t in range(sequence_length):\n",
        "    print(f\"Time {t}: {input_sequence[t]}\")\n",
        "\n",
        "print(\"\\nHidden states at each time step:\")\n",
        "for t in range(sequence_length):\n",
        "    print(f\"Time {t}: {hidden_states[t].flatten()}\")\n",
        "\n",
        "print(\"\\nOutputs at each time step:\")\n",
        "for t in range(sequence_length):\n",
        "    print(f\"Time {t}: {outputs[t].flatten()}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. RNN Unfolding Through Time\n",
        "\n",
        "The concept of \"unfolding\" helps visualize how RNNs process sequences. Instead of viewing the RNN as a loop, we can \"unfold\" it to show how it processes each time step.\n",
        "\n",
        "### Unfolded RNN Visualization:\n",
        "```\n",
        "x_1 → [RNN] → y_1\n",
        "      ↓\n",
        "x_2 → [RNN] → y_2\n",
        "      ↓\n",
        "x_3 → [RNN] → y_3\n",
        "      ↓\n",
        "...\n",
        "```\n",
        "\n",
        "Each [RNN] block represents the same network with shared parameters, but at different time steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizing RNN Unfolding and Hidden State Evolution\n",
        "def visualize_rnn_unfolding(sequence, hidden_states):\n",
        "    \"\"\"\n",
        "    Visualize how hidden states evolve through time\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "    \n",
        "    # Plot input sequence\n",
        "    ax1.plot(sequence, marker='o', linewidth=2, markersize=8)\n",
        "    ax1.set_title('Input Sequence Over Time', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Time Step')\n",
        "    ax1.set_ylabel('Input Value')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot hidden state evolution\n",
        "    hidden_states_2d = hidden_states.squeeze()\n",
        "    for i in range(hidden_states_2d.shape[1]):\n",
        "        ax2.plot(hidden_states_2d[:, i], marker='o', linewidth=2, \n",
        "                markersize=6, label=f'Hidden Unit {i+1}')\n",
        "    \n",
        "    ax2.set_title('Hidden State Evolution Through Time', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Time Step')\n",
        "    ax2.set_ylabel('Hidden State Value')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create a simple time series for demonstration\n",
        "time_steps = np.arange(10)\n",
        "# Create a sine wave with some noise\n",
        "input_sequence = np.sin(time_steps * 0.5) + 0.1 * np.random.randn(10)\n",
        "\n",
        "# Reshape for RNN (each time step has 1 feature)\n",
        "input_sequence_reshaped = input_sequence.reshape(-1, 1)\n",
        "\n",
        "# Create and test RNN\n",
        "rnn_demo = SimpleRNN(input_size=1, hidden_size=3, output_size=1)\n",
        "outputs, hidden_states = rnn_demo.forward(input_sequence_reshaped)\n",
        "\n",
        "print(\"Demonstrating RNN Unfolding:\")\n",
        "print(f\"Input sequence: {input_sequence}\")\n",
        "print(f\"Hidden states shape: {hidden_states.shape}\")\n",
        "\n",
        "# Visualize the unfolding\n",
        "visualize_rnn_unfolding(input_sequence, hidden_states)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. RNN with TensorFlow/Keras\n",
        "\n",
        "Now let's implement RNNs using TensorFlow/Keras for more practical applications. TensorFlow provides optimized implementations of RNN layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Building RNN with TensorFlow/Keras\n",
        "def create_simple_rnn_model(input_shape, hidden_units=50, output_units=1):\n",
        "    \"\"\"\n",
        "    Create a simple RNN model using Keras\n",
        "    \n",
        "    Parameters:\n",
        "    input_shape: (sequence_length, features)\n",
        "    hidden_units: number of RNN units\n",
        "    output_units: number of output units\n",
        "    \n",
        "    Returns:\n",
        "    model: compiled Keras model\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.SimpleRNN(\n",
        "            hidden_units, \n",
        "            input_shape=input_shape,\n",
        "            return_sequences=False,  # Return only the last output\n",
        "            activation='tanh'\n",
        "        ),\n",
        "        keras.layers.Dense(output_units, activation='linear')\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create sample sequential data for demonstration\n",
        "def generate_sine_sequence(n_samples, sequence_length, n_features):\n",
        "    \"\"\"Generate sine wave sequences for testing\"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "    \n",
        "    for i in range(n_samples):\n",
        "        # Generate sine wave with random phase and frequency\n",
        "        phase = np.random.random() * 2 * np.pi\n",
        "        freq = 0.1 + np.random.random() * 0.4\n",
        "        \n",
        "        # Create sequence\n",
        "        t = np.linspace(0, sequence_length, sequence_length)\n",
        "        sequence = np.sin(freq * t + phase)\n",
        "        \n",
        "        # Add noise\n",
        "        sequence += 0.1 * np.random.randn(sequence_length)\n",
        "        \n",
        "        # Prepare input (all but last) and target (last value)\n",
        "        X.append(sequence[:-1].reshape(-1, 1))\n",
        "        y.append(sequence[-1])\n",
        "    \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Generate training data\n",
        "print(\"Generating training data...\")\n",
        "n_samples = 1000\n",
        "sequence_length = 20\n",
        "n_features = 1\n",
        "\n",
        "X_train, y_train = generate_sine_sequence(n_samples, sequence_length, n_features)\n",
        "X_test, y_test = generate_sine_sequence(200, sequence_length, n_features)\n",
        "\n",
        "print(f\"Training data shape: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"Test data shape: X={X_test.shape}, y={y_test.shape}\")\n",
        "\n",
        "# Create and train the model\n",
        "model = create_simple_rnn_model(\n",
        "    input_shape=(sequence_length-1, n_features),\n",
        "    hidden_units=32,\n",
        "    output_units=1\n",
        ")\n",
        "\n",
        "print(\"\\nModel Architecture:\")\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "print(\"\\nTraining the RNN model...\")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=32,\n",
        "    epochs=20,\n",
        "    validation_data=(X_test, y_test),\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate and visualize results\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training and validation loss\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    \n",
        "    # Plot loss\n",
        "    ax1.plot(history.history['loss'], label='Training Loss')\n",
        "    ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    ax1.set_title('Model Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot MAE\n",
        "    ax2.plot(history.history['mae'], label='Training MAE')\n",
        "    ax2.plot(history.history['val_mae'], label='Validation MAE')\n",
        "    ax2.set_title('Model MAE')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('MAE')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_predictions(model, X_test, y_test, n_examples=5):\n",
        "    \"\"\"Visualize model predictions on test sequences\"\"\"\n",
        "    predictions = model.predict(X_test[:n_examples])\n",
        "    \n",
        "    fig, axes = plt.subplots(n_examples, 1, figsize=(12, 2*n_examples))\n",
        "    if n_examples == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for i in range(n_examples):\n",
        "        sequence = X_test[i].flatten()\n",
        "        true_next = y_test[i]\n",
        "        pred_next = predictions[i][0]\n",
        "        \n",
        "        # Plot the sequence\n",
        "        axes[i].plot(range(len(sequence)), sequence, 'b-o', label='Input Sequence')\n",
        "        axes[i].plot(len(sequence), true_next, 'ro', markersize=10, label=f'True Next: {true_next:.3f}')\n",
        "        axes[i].plot(len(sequence), pred_next, 'go', markersize=10, label=f'Predicted: {pred_next:.3f}')\n",
        "        \n",
        "        axes[i].set_title(f'Sequence {i+1} - Error: {abs(true_next - pred_next):.3f}')\n",
        "        axes[i].legend()\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot training history\n",
        "plot_training_history(history)\n",
        "\n",
        "# Evaluate model\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "print(f\"Test MAE: {test_mae:.4f}\")\n",
        "\n",
        "# Visualize predictions\n",
        "visualize_predictions(model, X_test, y_test, n_examples=3)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Understanding RNN Memory and Information Flow\n",
        "\n",
        "### Key Insights from Our Experiments:\n",
        "\n",
        "1. **Hidden State as Memory**: The hidden state serves as the network's memory, carrying information from previous time steps.\n",
        "\n",
        "2. **Parameter Sharing**: The same weights are used at each time step, allowing the network to generalize across different sequence positions.\n",
        "\n",
        "3. **Sequential Processing**: Unlike feedforward networks, RNNs process inputs sequentially, building up context over time.\n",
        "\n",
        "4. **Variable Length Handling**: RNNs can naturally handle sequences of different lengths.\n",
        "\n",
        "### Important Observations:\n",
        "\n",
        "- **Information Decay**: Information from earlier time steps may decay or get overwritten\n",
        "- **Gradient Flow**: Training RNNs involves backpropagation through time (BPTT)\n",
        "- **Computational Efficiency**: Sequential processing can be computationally intensive\n",
        "\n",
        "## Summary\n",
        "\n",
        "In this notebook, we've covered:\n",
        "- Basic RNN architecture and mathematical formulation\n",
        "- Implementation from scratch using NumPy\n",
        "- TensorFlow/Keras implementation for practical applications\n",
        "- Visualization of hidden state evolution\n",
        "- Understanding of RNN unfolding through time\n",
        "\n",
        "**Next Steps**: In the following notebooks, we'll explore different RNN variants (LSTM, GRU, Bidirectional) and various input-output configurations for different types of sequence modeling tasks.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
