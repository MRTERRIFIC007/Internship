{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# RNN Architectures Part 2: One-to-One and One-to-Many RNNs\n",
        "\n",
        "## Learning Objectives\n",
        "In this notebook, we'll complete our exploration of RNN architectures:\n",
        "- One-to-One RNN patterns (traditional neural network approach)\n",
        "- One-to-Many RNN architecture (text/sequence generation)\n",
        "- Practical implementations and real-world applications\n",
        "- Advanced sequence generation techniques\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "### One-to-One RNN\n",
        "- **Input**: Single fixed-size input\n",
        "- **Output**: Single fixed-size output\n",
        "- **Use Cases**: Traditional classification/regression tasks\n",
        "- **Note**: This is essentially a feedforward network\n",
        "\n",
        "### One-to-Many RNN\n",
        "- **Input**: Single fixed-size input (often an image or context vector)\n",
        "- **Output**: Variable-length sequence\n",
        "- **Use Cases**: Image captioning, music generation, story generation\n",
        "- **Key Feature**: Uses previous output as next input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. One-to-One RNN Implementation\n",
        "\n",
        "While not truly \"recurrent\" in nature, One-to-One networks represent the traditional neural network approach within the RNN framework.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-to-One RNN (Traditional Neural Network)\n",
        "def create_one_to_one_model(input_dim, hidden_units=64, output_units=1):\n",
        "    \"\"\"\n",
        "    Create a One-to-One model (essentially a feedforward network)\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Dense(hidden_units, activation='relu', input_shape=(input_dim,)),\n",
        "        keras.layers.Dropout(0.2),\n",
        "        keras.layers.Dense(hidden_units//2, activation='relu'),\n",
        "        keras.layers.Dropout(0.2),\n",
        "        keras.layers.Dense(output_units, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create sample data for One-to-One classification\n",
        "X_simple = np.random.randn(1000, 10)  # 1000 samples, 10 features\n",
        "y_simple = (X_simple.sum(axis=1) > 0).astype(int)  # Binary classification\n",
        "\n",
        "X_train_simple, X_test_simple, y_train_simple, y_test_simple = train_test_split(\n",
        "    X_simple, y_simple, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create and train One-to-One model\n",
        "print(\"Creating One-to-One model...\")\n",
        "one_to_one_model = create_one_to_one_model(input_dim=10, hidden_units=32)\n",
        "\n",
        "print(\"One-to-One Model architecture:\")\n",
        "one_to_one_model.summary()\n",
        "\n",
        "history_simple = one_to_one_model.fit(\n",
        "    X_train_simple, y_train_simple,\n",
        "    batch_size=32,\n",
        "    epochs=20,\n",
        "    validation_data=(X_test_simple, y_test_simple),\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "accuracy_simple = one_to_one_model.evaluate(X_test_simple, y_test_simple, verbose=0)[1]\n",
        "print(f\"One-to-One Model Accuracy: {accuracy_simple:.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. One-to-Many RNN: Text Generation\n",
        "\n",
        "The One-to-Many architecture is perfect for text generation tasks where we start with a seed or context and generate a sequence of words or characters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-to-Many RNN for Simple Text Generation\n",
        "def create_one_to_many_model(vocab_size, embedding_dim=50, rnn_units=64, sequence_length=10):\n",
        "    \"\"\"\n",
        "    Create a One-to-Many RNN for text generation\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "        keras.layers.LSTM(rnn_units, return_sequences=True),\n",
        "        keras.layers.LSTM(rnn_units),\n",
        "        keras.layers.RepeatVector(sequence_length),\n",
        "        keras.layers.LSTM(rnn_units, return_sequences=True),\n",
        "        keras.layers.TimeDistributed(keras.layers.Dense(vocab_size, activation='softmax'))\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create simple character-level data for text generation\n",
        "def create_character_data():\n",
        "    \"\"\"Create simple character sequences for demonstration\"\"\"\n",
        "    text = \"hello world this is a simple text generation example\"\n",
        "    chars = sorted(list(set(text)))\n",
        "    char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
        "    idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
        "    \n",
        "    sequence_length = 5\n",
        "    X = []\n",
        "    y = []\n",
        "    \n",
        "    for i in range(len(text) - sequence_length):\n",
        "        sequence = text[i:i+sequence_length]\n",
        "        target = text[i+1:i+sequence_length+1]\n",
        "        \n",
        "        X.append([char_to_idx[char] for char in sequence])\n",
        "        y.append([char_to_idx[char] for char in target])\n",
        "    \n",
        "    return np.array(X), np.array(y), char_to_idx, idx_to_char, chars\n",
        "\n",
        "print(\"Creating character-level data...\")\n",
        "X_char, y_char, char_to_idx, idx_to_char, chars = create_character_data()\n",
        "\n",
        "print(f\"Character data shape: X={X_char.shape}, y={y_char.shape}\")\n",
        "print(f\"Vocabulary size: {len(chars)}\")\n",
        "print(f\"Characters: {chars}\")\n",
        "\n",
        "# Simple text generation function\n",
        "def generate_text(model, start_string, char_to_idx, idx_to_char, num_generate=50):\n",
        "    \"\"\"Generate text using the trained model\"\"\"\n",
        "    input_eval = [char_to_idx.get(s, 0) for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    \n",
        "    text_generated = []\n",
        "    \n",
        "    # Low temperature for more predictable results\n",
        "    temperature = 0.5\n",
        "    \n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        \n",
        "        # Use temperature to control randomness\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        \n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx_to_char.get(predicted_id, ''))\n",
        "    \n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "# For demonstration, let's create a simpler sequence prediction model\n",
        "def create_simple_sequence_model(vocab_size, sequence_length=5):\n",
        "    \"\"\"Create a simple sequence prediction model\"\"\"\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Embedding(vocab_size, 32, input_length=sequence_length),\n",
        "        keras.layers.LSTM(64),\n",
        "        keras.layers.Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Prepare data for next character prediction\n",
        "X_simple = X_char[:, :-1]  # All but last character\n",
        "y_simple = X_char[:, -1]   # Last character\n",
        "\n",
        "print(f\"Simple prediction data: X={X_simple.shape}, y={y_simple.shape}\")\n",
        "\n",
        "# Create and train simple model\n",
        "print(\"\\nCreating simple character prediction model...\")\n",
        "simple_gen_model = create_simple_sequence_model(len(chars), sequence_length=4)\n",
        "\n",
        "print(\"Simple generation model architecture:\")\n",
        "simple_gen_model.summary()\n",
        "\n",
        "print(\"\\nTraining simple generation model...\")\n",
        "gen_history = simple_gen_model.fit(\n",
        "    X_simple, y_simple,\n",
        "    batch_size=16,\n",
        "    epochs=50,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(f\"Final training accuracy: {gen_history.history['accuracy'][-1]:.4f}\")\n",
        "\n",
        "# Simple text generation\n",
        "def simple_generate(model, seed, char_to_idx, idx_to_char, length=20):\n",
        "    \"\"\"Simple text generation\"\"\"\n",
        "    result = seed\n",
        "    current = seed\n",
        "    \n",
        "    for _ in range(length):\n",
        "        # Prepare input\n",
        "        if len(current) > 4:\n",
        "            current = current[-4:]\n",
        "        \n",
        "        x_pred = np.array([[char_to_idx.get(c, 0) for c in current]])\n",
        "        \n",
        "        # Predict next character\n",
        "        pred = model.predict(x_pred, verbose=0)\n",
        "        next_idx = np.argmax(pred[0])\n",
        "        next_char = idx_to_char.get(next_idx, '')\n",
        "        \n",
        "        result += next_char\n",
        "        current += next_char\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Generate some text\n",
        "seed = \"hell\"\n",
        "generated = simple_generate(simple_gen_model, seed, char_to_idx, idx_to_char, length=30)\n",
        "print(f\"\\nGenerated text from seed '{seed}': {generated}\")\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(history_simple.history['accuracy'], label='Train')\n",
        "plt.plot(history_simple.history['val_accuracy'], label='Validation')\n",
        "plt.title('One-to-One Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(gen_history.history['accuracy'])\n",
        "plt.title('Text Generation Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(gen_history.history['loss'])\n",
        "plt.title('Text Generation Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Character frequency\n",
        "plt.subplot(2, 2, 4)\n",
        "char_counts = [len([c for c in generated if c == char]) for char in chars]\n",
        "plt.bar(range(len(chars)), char_counts)\n",
        "plt.title('Generated Character Frequency')\n",
        "plt.xlabel('Character Index')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nSummary of RNN Architectures:\")\n",
        "print(\"1. One-to-One: Traditional neural networks for fixed input-output\")\n",
        "print(\"2. One-to-Many: Text/sequence generation from fixed input\")\n",
        "print(\"3. Many-to-One: Sequence classification and sentiment analysis\")\n",
        "print(\"4. Many-to-Many: Sequence transformation and translation\")\n",
        "print(\"5. Each architecture serves specific use cases in sequence modeling\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
