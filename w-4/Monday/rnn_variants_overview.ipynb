{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# RNN Variants: Bidirectional RNNs, LSTMs, and GRUs\n",
        "\n",
        "## Learning Objectives\n",
        "In this notebook, we'll explore advanced RNN architectures that address limitations of vanilla RNNs:\n",
        "- Bidirectional RNNs for capturing future context\n",
        "- Long Short-Term Memory (LSTM) networks\n",
        "- Gated Recurrent Units (GRUs)\n",
        "- Comparative analysis of different architectures\n",
        "- When to use each variant\n",
        "\n",
        "## Introduction to RNN Variants\n",
        "\n",
        "While basic RNNs introduced us to sequence modeling, they have several limitations:\n",
        "1. **Vanishing Gradient Problem**: Difficulty learning long-term dependencies\n",
        "2. **Unidirectional Processing**: Only considers past context\n",
        "3. **Limited Memory**: Simple hidden state can't selectively remember/forget\n",
        "\n",
        "Advanced RNN variants address these issues through improved architectures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Libraries imported successfully!\")\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Bidirectional RNNs\n",
        "\n",
        "### Concept\n",
        "Bidirectional RNNs process sequences in both forward and backward directions, allowing the network to capture both past and future context at each time step.\n",
        "\n",
        "### Architecture:\n",
        "- **Forward RNN**: Processes sequence from start to end\n",
        "- **Backward RNN**: Processes sequence from end to start\n",
        "- **Output**: Concatenation or combination of both directions\n",
        "\n",
        "### Mathematical Formulation:\n",
        "- Forward: h_f(t) = f(x_t, h_f(t-1))\n",
        "- Backward: h_b(t) = f(x_t, h_b(t+1))\n",
        "- Combined: h(t) = [h_f(t); h_b(t)] or g(h_f(t), h_b(t))\n",
        "\n",
        "### Use Cases:\n",
        "- Natural Language Processing (entire sentence context)\n",
        "- Speech Recognition\n",
        "- Any task where future context is available during inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementing Bidirectional RNN\n",
        "def create_bidirectional_rnn(input_shape, hidden_units=50, output_units=1):\n",
        "    \"\"\"\n",
        "    Create a Bidirectional RNN model\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Bidirectional(\n",
        "            keras.layers.SimpleRNN(hidden_units, return_sequences=False),\n",
        "            input_shape=input_shape\n",
        "        ),\n",
        "        keras.layers.Dense(output_units, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create synthetic data to demonstrate bidirectional processing\n",
        "def create_palindrome_data(n_samples=1000, sequence_length=10):\n",
        "    \"\"\"\n",
        "    Create data where we need to identify palindromes\n",
        "    This requires both forward and backward context\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "    \n",
        "    for _ in range(n_samples):\n",
        "        # Create random sequence\n",
        "        sequence = np.random.randint(0, 10, sequence_length)\n",
        "        \n",
        "        # Create palindrome 50% of the time\n",
        "        if np.random.random() < 0.5:\n",
        "            # Make it a palindrome\n",
        "            mid = sequence_length // 2\n",
        "            sequence[mid:] = sequence[:mid][::-1]\n",
        "            label = 1\n",
        "        else:\n",
        "            # Keep it random\n",
        "            label = 0\n",
        "        \n",
        "        X.append(sequence)\n",
        "        y.append(label)\n",
        "    \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Generate palindrome detection data\n",
        "print(\"Creating palindrome detection dataset...\")\n",
        "X_pal, y_pal = create_palindrome_data(n_samples=2000, sequence_length=8)\n",
        "\n",
        "# Normalize the input\n",
        "X_pal = X_pal / 10.0  # Scale to [0, 1]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pal, y_pal, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Reshape for RNN (add feature dimension)\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Training labels distribution: {np.bincount(y_train)}\")\n",
        "\n",
        "# Create and compare models\n",
        "print(\"\\nCreating Unidirectional RNN...\")\n",
        "uni_model = keras.Sequential([\n",
        "    keras.layers.SimpleRNN(32, input_shape=(8, 1), return_sequences=False),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "uni_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"Creating Bidirectional RNN...\")\n",
        "bi_model = create_bidirectional_rnn(input_shape=(8, 1), hidden_units=32)\n",
        "\n",
        "print(\"\\nModel architectures:\")\n",
        "print(\"Unidirectional RNN:\")\n",
        "uni_model.summary()\n",
        "print(\"\\nBidirectional RNN:\")\n",
        "bi_model.summary()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Long Short-Term Memory (LSTM) Networks\n",
        "\n",
        "### The Vanishing Gradient Problem\n",
        "Traditional RNNs suffer from vanishing gradients when learning long-term dependencies. As gradients backpropagate through time, they tend to either:\n",
        "- **Vanish**: Become too small to affect early time steps\n",
        "- **Explode**: Become too large, causing instability\n",
        "\n",
        "### LSTM Solution\n",
        "LSTMs address this through a sophisticated gating mechanism:\n",
        "\n",
        "1. **Forget Gate**: Decides what information to discard from cell state\n",
        "2. **Input Gate**: Decides which values to update in cell state\n",
        "3. **Cell State**: Long-term memory that flows through the network\n",
        "4. **Output Gate**: Decides what parts of cell state to output\n",
        "\n",
        "### Mathematical Formulation:\n",
        "- Forget gate: f_t = σ(W_f · [h_{t-1}, x_t] + b_f)\n",
        "- Input gate: i_t = σ(W_i · [h_{t-1}, x_t] + b_i)\n",
        "- Candidate values: C̃_t = tanh(W_C · [h_{t-1}, x_t] + b_C)\n",
        "- Cell state: C_t = f_t * C_{t-1} + i_t * C̃_t\n",
        "- Output gate: o_t = σ(W_o · [h_{t-1}, x_t] + b_o)\n",
        "- Hidden state: h_t = o_t * tanh(C_t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and compare models\n",
        "print(\"Training Unidirectional RNN...\")\n",
        "uni_history = uni_model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=32,\n",
        "    epochs=20,\n",
        "    validation_data=(X_test, y_test),\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\"Training Bidirectional RNN...\")\n",
        "bi_history = bi_model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=32,\n",
        "    epochs=20,\n",
        "    validation_data=(X_test, y_test),\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Evaluate models\n",
        "uni_accuracy = uni_model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "bi_accuracy = bi_model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "\n",
        "print(f\"\\nModel Performance on Palindrome Detection:\")\n",
        "print(f\"Unidirectional RNN Accuracy: {uni_accuracy:.4f}\")\n",
        "print(f\"Bidirectional RNN Accuracy: {bi_accuracy:.4f}\")\n",
        "print(f\"Improvement: {(bi_accuracy - uni_accuracy):.4f}\")\n",
        "\n",
        "# Visualize training progress\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Accuracy comparison\n",
        "axes[0].plot(uni_history.history['accuracy'], label='Uni RNN - Train')\n",
        "axes[0].plot(uni_history.history['val_accuracy'], label='Uni RNN - Val')\n",
        "axes[0].plot(bi_history.history['accuracy'], label='Bi RNN - Train')\n",
        "axes[0].plot(bi_history.history['val_accuracy'], label='Bi RNN - Val')\n",
        "axes[0].set_title('Model Accuracy Comparison')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss comparison\n",
        "axes[1].plot(uni_history.history['loss'], label='Uni RNN - Train')\n",
        "axes[1].plot(uni_history.history['val_loss'], label='Uni RNN - Val')\n",
        "axes[1].plot(bi_history.history['loss'], label='Bi RNN - Train')\n",
        "axes[1].plot(bi_history.history['val_loss'], label='Bi RNN - Val')\n",
        "axes[1].set_title('Model Loss Comparison')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementing LSTM Networks\n",
        "def create_lstm_model(input_shape, hidden_units=50, output_units=1, task_type='regression'):\n",
        "    \"\"\"\n",
        "    Create an LSTM model for different tasks\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.LSTM(\n",
        "            hidden_units,\n",
        "            input_shape=input_shape,\n",
        "            return_sequences=False\n",
        "        ),\n",
        "        keras.layers.Dense(output_units, \n",
        "                          activation='linear' if task_type=='regression' else 'sigmoid')\n",
        "    ])\n",
        "    \n",
        "    if task_type == 'regression':\n",
        "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    else:\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create data with long-term dependencies for LSTM demonstration\n",
        "def create_long_dependency_data(n_samples=1000, sequence_length=50):\n",
        "    \"\"\"\n",
        "    Create data where early information affects final output\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "    \n",
        "    for _ in range(n_samples):\n",
        "        # Random sequence\n",
        "        sequence = np.random.randn(sequence_length)\n",
        "        \n",
        "        # Important signal at the beginning\n",
        "        signal_position = np.random.randint(0, 5)  # Signal in first 5 positions\n",
        "        signal_value = np.random.choice([-1, 1])\n",
        "        sequence[signal_position] = signal_value * 5  # Strong signal\n",
        "        \n",
        "        # Output depends on early signal\n",
        "        label = 1 if signal_value > 0 else 0\n",
        "        \n",
        "        X.append(sequence)\n",
        "        y.append(label)\n",
        "    \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "print(\"Creating long-term dependency dataset...\")\n",
        "X_long, y_long = create_long_dependency_data(n_samples=2000, sequence_length=30)\n",
        "\n",
        "# Reshape for RNN\n",
        "X_long = X_long.reshape(X_long.shape[0], X_long.shape[1], 1)\n",
        "\n",
        "# Split data\n",
        "X_train_long, X_test_long, y_train_long, y_test_long = train_test_split(\n",
        "    X_long, y_long, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Long dependency data shape: {X_train_long.shape}\")\n",
        "\n",
        "# Compare SimpleRNN vs LSTM on long dependencies\n",
        "print(\"\\nCreating models for long-term dependency comparison...\")\n",
        "\n",
        "simple_rnn = keras.Sequential([\n",
        "    keras.layers.SimpleRNN(32, input_shape=(30, 1)),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "simple_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "lstm_model = create_lstm_model(\n",
        "    input_shape=(30, 1), \n",
        "    hidden_units=32, \n",
        "    output_units=1, \n",
        "    task_type='classification'\n",
        ")\n",
        "\n",
        "print(\"Training SimpleRNN on long dependency task...\")\n",
        "simple_history = simple_rnn.fit(\n",
        "    X_train_long, y_train_long,\n",
        "    batch_size=32, epochs=30,\n",
        "    validation_data=(X_test_long, y_test_long),\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\"Training LSTM on long dependency task...\")\n",
        "lstm_history = lstm_model.fit(\n",
        "    X_train_long, y_train_long,\n",
        "    batch_size=32, epochs=30,\n",
        "    validation_data=(X_test_long, y_test_long),\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Evaluate models\n",
        "simple_acc = simple_rnn.evaluate(X_test_long, y_test_long, verbose=0)[1]\n",
        "lstm_acc = lstm_model.evaluate(X_test_long, y_test_long, verbose=0)[1]\n",
        "\n",
        "print(f\"\\nLong-term Dependency Task Performance:\")\n",
        "print(f\"SimpleRNN Accuracy: {simple_acc:.4f}\")\n",
        "print(f\"LSTM Accuracy: {lstm_acc:.4f}\")\n",
        "print(f\"LSTM Improvement: {(lstm_acc - simple_acc):.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Gated Recurrent Units (GRUs)\n",
        "\n",
        "### Motivation\n",
        "GRUs are a simpler alternative to LSTMs that achieve similar performance with fewer parameters. They were introduced to address the same vanishing gradient problem but with a more efficient architecture.\n",
        "\n",
        "### Key Differences from LSTM:\n",
        "1. **Fewer Gates**: Only reset and update gates (no separate forget/input gates)\n",
        "2. **No Cell State**: Uses hidden state directly\n",
        "3. **Simpler Architecture**: Fewer parameters and computations\n",
        "\n",
        "### Mathematical Formulation:\n",
        "- Reset gate: r_t = σ(W_r · [h_{t-1}, x_t])\n",
        "- Update gate: z_t = σ(W_z · [h_{t-1}, x_t])\n",
        "- Candidate hidden state: h̃_t = tanh(W · [r_t * h_{t-1}, x_t])\n",
        "- Hidden state: h_t = (1 - z_t) * h_{t-1} + z_t * h̃_t\n",
        "\n",
        "### Advantages:\n",
        "- Faster training (fewer parameters)\n",
        "- Similar performance to LSTM on many tasks\n",
        "- Less prone to overfitting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementing GRU and comparing all variants\n",
        "def create_gru_model(input_shape, hidden_units=50, output_units=1, task_type='regression'):\n",
        "    \"\"\"Create a GRU model\"\"\"\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.GRU(\n",
        "            hidden_units,\n",
        "            input_shape=input_shape,\n",
        "            return_sequences=False\n",
        "        ),\n",
        "        keras.layers.Dense(output_units, \n",
        "                          activation='linear' if task_type=='regression' else 'sigmoid')\n",
        "    ])\n",
        "    \n",
        "    if task_type == 'regression':\n",
        "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    else:\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create GRU model for comparison\n",
        "print(\"Creating GRU model...\")\n",
        "gru_model = create_gru_model(\n",
        "    input_shape=(30, 1),\n",
        "    hidden_units=32,\n",
        "    output_units=1,\n",
        "    task_type='classification'\n",
        ")\n",
        "\n",
        "print(\"Training GRU on long dependency task...\")\n",
        "gru_history = gru_model.fit(\n",
        "    X_train_long, y_train_long,\n",
        "    batch_size=32, epochs=30,\n",
        "    validation_data=(X_test_long, y_test_long),\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Evaluate GRU\n",
        "gru_acc = gru_model.evaluate(X_test_long, y_test_long, verbose=0)[1]\n",
        "\n",
        "print(f\"GRU Accuracy: {gru_acc:.4f}\")\n",
        "\n",
        "# Compare model complexities\n",
        "def count_parameters(model):\n",
        "    \"\"\"Count trainable parameters in model\"\"\"\n",
        "    return model.count_params()\n",
        "\n",
        "print(f\"\\nModel Complexity Comparison:\")\n",
        "print(f\"SimpleRNN parameters: {count_parameters(simple_rnn):,}\")\n",
        "print(f\"LSTM parameters: {count_parameters(lstm_model):,}\")\n",
        "print(f\"GRU parameters: {count_parameters(gru_model):,}\")\n",
        "\n",
        "# Create comprehensive comparison plot\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Accuracy comparison\n",
        "models = ['SimpleRNN', 'LSTM', 'GRU']\n",
        "accuracies = [simple_acc, lstm_acc, gru_acc]\n",
        "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
        "\n",
        "axes[0, 0].bar(models, accuracies, color=colors)\n",
        "axes[0, 0].set_title('Model Accuracy Comparison')\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].set_ylim(0.4, 1.0)\n",
        "for i, v in enumerate(accuracies):\n",
        "    axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
        "\n",
        "# Parameter count comparison\n",
        "param_counts = [count_parameters(simple_rnn), count_parameters(lstm_model), count_parameters(gru_model)]\n",
        "axes[0, 1].bar(models, param_counts, color=colors)\n",
        "axes[0, 1].set_title('Model Complexity (Parameters)')\n",
        "axes[0, 1].set_ylabel('Number of Parameters')\n",
        "for i, v in enumerate(param_counts):\n",
        "    axes[0, 1].text(i, v + 50, f'{v:,}', ha='center')\n",
        "\n",
        "# Training accuracy evolution\n",
        "axes[1, 0].plot(simple_history.history['val_accuracy'], label='SimpleRNN', linewidth=2)\n",
        "axes[1, 0].plot(lstm_history.history['val_accuracy'], label='LSTM', linewidth=2)\n",
        "axes[1, 0].plot(gru_history.history['val_accuracy'], label='GRU', linewidth=2)\n",
        "axes[1, 0].set_title('Validation Accuracy During Training')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Validation Accuracy')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Training loss evolution\n",
        "axes[1, 1].plot(simple_history.history['val_loss'], label='SimpleRNN', linewidth=2)\n",
        "axes[1, 1].plot(lstm_history.history['val_loss'], label='LSTM', linewidth=2)\n",
        "axes[1, 1].plot(gru_history.history['val_loss'], label='GRU', linewidth=2)\n",
        "axes[1, 1].set_title('Validation Loss During Training')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Validation Loss')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary and Key Insights\n",
        "\n",
        "### Performance Comparison:\n",
        "From our experiments, we observed:\n",
        "\n",
        "1. **Bidirectional RNNs**: Significantly outperform unidirectional RNNs on tasks requiring future context\n",
        "2. **LSTM vs SimpleRNN**: LSTMs excel at capturing long-term dependencies\n",
        "3. **GRU Performance**: Often matches LSTM performance with fewer parameters\n",
        "4. **Computational Efficiency**: GRU < LSTM < Bidirectional models\n",
        "\n",
        "### When to Use Each Variant:\n",
        "\n",
        "| Model Type | Best For | Considerations |\n",
        "|------------|----------|----------------|\n",
        "| SimpleRNN | Short sequences, simple patterns | Fast, limited memory |\n",
        "| LSTM | Long sequences, complex dependencies | More parameters, slower |\n",
        "| GRU | Good balance of performance/efficiency | Fewer parameters than LSTM |\n",
        "| Bidirectional | Tasks with future context available | 2x computational cost |\n",
        "\n",
        "### Key Learnings:\n",
        "1. **Architecture matters**: Choice of RNN variant significantly impacts performance\n",
        "2. **Task-specific selection**: Different architectures excel at different tasks\n",
        "3. **Trade-offs**: Performance vs computational efficiency\n",
        "4. **Gating mechanisms**: Critical for handling long-term dependencies\n",
        "\n",
        "### Next Steps:\n",
        "In the following notebooks, we'll implement different RNN configurations (many-to-one, many-to-many, etc.) and explore practical applications in text processing and sequence generation.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
