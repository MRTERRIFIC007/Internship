{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Long-term Dependencies Analysis\n",
        "\n",
        "## Learning Objectives\n",
        "In this notebook, we'll conduct a comprehensive analysis of the long-term dependencies problem:\n",
        "- Deep mathematical analysis of vanishing and exploding gradient problems\n",
        "- Empirical studies of dependency length limitations across different architectures\n",
        "- Gradient flow visualization over extended sequences\n",
        "- Performance degradation patterns and measurement techniques\n",
        "- Solution strategies and their effectiveness evaluation\n",
        "- Comparative analysis of RNN, LSTM, and GRU long-term memory capabilities\n",
        "\n",
        "## Introduction to Long-term Dependencies\n",
        "\n",
        "The long-term dependencies problem is one of the fundamental challenges in sequence modeling. When neural networks need to learn relationships between events separated by many time steps, traditional RNNs struggle due to the vanishing gradient problem.\n",
        "\n",
        "### Key Problems:\n",
        "1. **Vanishing Gradients**: Gradients become exponentially smaller as they propagate backward through time\n",
        "2. **Exploding Gradients**: Gradients become exponentially larger, causing training instability\n",
        "3. **Information Decay**: Important information from early time steps gets overwritten or forgotten\n",
        "4. **Limited Context Window**: Effective memory span is much shorter than theoretical capacity\n",
        "\n",
        "### Mathematical Foundation:\n",
        "For a simple RNN with repeated weight matrix W, gradients flow as:\n",
        "**∂L/∂W** ∝ **∏(t=1 to T) ∂h_t/∂h_{t-1}** = **∏(t=1 to T) W·diag(f'(·))**\n",
        "\n",
        "When the largest eigenvalue of this product is:\n",
        "- **< 1**: Vanishing gradients (exponential decay)\n",
        "- **> 1**: Exploding gradients (exponential growth)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Long-term Dependencies Analysis initialized!\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# 1. Long-term Dependency Task Generator\n",
        "class LongTermDependencyTasks:\n",
        "    \"\"\"\n",
        "    Generate various tasks to test long-term dependency capabilities\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size=10):\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "    def copy_task(self, seq_length=100, gap_length=50, num_samples=1000):\n",
        "        \"\"\"\n",
        "        Copy task: remember information from beginning and reproduce after gap\n",
        "        \"\"\"\n",
        "        X = []\n",
        "        y = []\n",
        "        \n",
        "        for _ in range(num_samples):\n",
        "            # Create sequence with information at start, gap, then prediction target\n",
        "            sequence = np.zeros(seq_length)\n",
        "            \n",
        "            # Random information at beginning (length 5)\n",
        "            info_length = 5\n",
        "            sequence[:info_length] = np.random.randint(1, self.vocab_size, info_length)\n",
        "            \n",
        "            # Gap filled with zeros\n",
        "            # sequence[info_length:info_length+gap_length] remains zeros\n",
        "            \n",
        "            # Target: reproduce the information after gap\n",
        "            target = np.zeros(seq_length)\n",
        "            start_reproduce = info_length + gap_length\n",
        "            if start_reproduce + info_length <= seq_length:\n",
        "                target[start_reproduce:start_reproduce+info_length] = sequence[:info_length]\n",
        "            \n",
        "            X.append(sequence)\n",
        "            y.append(target)\n",
        "        \n",
        "        return np.array(X), np.array(y)\n",
        "    \n",
        "    def adding_task(self, seq_length=100, num_samples=1000):\n",
        "        \"\"\"\n",
        "        Adding task: sum two marked numbers in a long sequence\n",
        "        \"\"\"\n",
        "        X = []\n",
        "        y = []\n",
        "        \n",
        "        for _ in range(num_samples):\n",
        "            # Create sequence of random numbers between 0 and 1\n",
        "            sequence = np.random.uniform(0, 1, (seq_length, 2))\n",
        "            \n",
        "            # Mark two positions to add\n",
        "            mark_positions = np.random.choice(seq_length//2, 2, replace=False)\n",
        "            \n",
        "            # Second column is marker (1 for marked positions, 0 otherwise)\n",
        "            sequence[:, 1] = 0\n",
        "            sequence[mark_positions, 1] = 1\n",
        "            \n",
        "            # Target is sum of marked values\n",
        "            target = np.sum(sequence[mark_positions, 0])\n",
        "            \n",
        "            X.append(sequence)\n",
        "            y.append(target)\n",
        "        \n",
        "        return np.array(X), np.array(y)\n",
        "    \n",
        "    def permuted_sequential_mnist(self, permutation_seed=42):\n",
        "        \"\"\"\n",
        "        Permuted Sequential MNIST: classify digit from permuted pixel sequence\n",
        "        \"\"\"\n",
        "        # Load MNIST data\n",
        "        (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "        \n",
        "        # Normalize and reshape\n",
        "        x_train = x_train.astype('float32') / 255.0\n",
        "        x_test = x_test.astype('float32') / 255.0\n",
        "        \n",
        "        # Flatten images to sequences\n",
        "        x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "        \n",
        "        # Create fixed permutation\n",
        "        np.random.seed(permutation_seed)\n",
        "        permutation = np.random.permutation(784)\n",
        "        \n",
        "        # Apply permutation\n",
        "        x_train = x_train[:, permutation]\n",
        "        x_test = x_test[:, permutation]\n",
        "        \n",
        "        # Reshape for RNN input (samples, timesteps, features)\n",
        "        x_train = x_train.reshape(x_train.shape[0], 784, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], 784, 1)\n",
        "        \n",
        "        return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "# 2. Gradient Flow Analyzer\n",
        "class GradientFlowAnalyzer:\n",
        "    \"\"\"\n",
        "    Analyze gradient flow for long-term dependencies\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.gradient_history = {}\n",
        "        \n",
        "    def create_gradient_tracking_model(self, model_type='SimpleRNN', units=50, seq_length=100):\n",
        "        \"\"\"\n",
        "        Create model with gradient tracking capabilities\n",
        "        \"\"\"\n",
        "        if model_type == 'SimpleRNN':\n",
        "            model = keras.Sequential([\n",
        "                layers.SimpleRNN(units, return_sequences=True, input_shape=(seq_length, 1)),\n",
        "                layers.Dense(1)\n",
        "            ])\n",
        "        elif model_type == 'LSTM':\n",
        "            model = keras.Sequential([\n",
        "                layers.LSTM(units, return_sequences=True, input_shape=(seq_length, 1)),\n",
        "                layers.Dense(1)\n",
        "            ])\n",
        "        elif model_type == 'GRU':\n",
        "            model = keras.Sequential([\n",
        "                layers.GRU(units, return_sequences=True, input_shape=(seq_length, 1)),\n",
        "                layers.Dense(1)\n",
        "            ])\n",
        "        \n",
        "        return model\n",
        "    \n",
        "    def analyze_gradient_flow(self, model, X, y, num_batches=5):\n",
        "        \"\"\"\n",
        "        Analyze how gradients flow through the model\n",
        "        \"\"\"\n",
        "        gradient_norms = []\n",
        "        loss_values = []\n",
        "        \n",
        "        for i in range(num_batches):\n",
        "            with tf.GradientTape() as tape:\n",
        "                predictions = model(X[i:i+1], training=True)\n",
        "                loss = tf.reduce_mean(tf.square(predictions - y[i:i+1]))\n",
        "            \n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "            \n",
        "            # Calculate gradient norms for each layer\n",
        "            layer_norms = []\n",
        "            for grad in gradients:\n",
        "                if grad is not None:\n",
        "                    norm = tf.norm(grad).numpy()\n",
        "                    layer_norms.append(norm)\n",
        "                else:\n",
        "                    layer_norms.append(0.0)\n",
        "            \n",
        "            gradient_norms.append(layer_norms)\n",
        "            loss_values.append(loss.numpy())\n",
        "        \n",
        "        return np.array(gradient_norms), np.array(loss_values)\n",
        "    \n",
        "    def eigenvalue_analysis(self, model, seq_length=100):\n",
        "        \"\"\"\n",
        "        Analyze eigenvalues of recurrent weight matrices\n",
        "        \"\"\"\n",
        "        eigenvalues = {}\n",
        "        \n",
        "        for layer in model.layers:\n",
        "            if hasattr(layer, 'recurrent_kernel'):\n",
        "                # Get recurrent weight matrix\n",
        "                W_rec = layer.recurrent_kernel.numpy()\n",
        "                \n",
        "                # Compute eigenvalues\n",
        "                eigvals = np.linalg.eigvals(W_rec)\n",
        "                eigenvalues[layer.name] = {\n",
        "                    'eigenvalues': eigvals,\n",
        "                    'max_eigenvalue': np.max(np.abs(eigvals)),\n",
        "                    'spectral_radius': np.max(np.abs(eigvals))\n",
        "                }\n",
        "        \n",
        "        return eigenvalues\n",
        "\n",
        "# 3. Performance Degradation Analyzer\n",
        "class PerformanceDegradationAnalyzer:\n",
        "    \"\"\"\n",
        "    Analyze how performance degrades with sequence length\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.results = {}\n",
        "    \n",
        "    def test_sequence_lengths(self, task_generator, lengths=[10, 25, 50, 100, 200], \n",
        "                            model_types=['SimpleRNN', 'LSTM', 'GRU']):\n",
        "        \"\"\"\n",
        "        Test performance across different sequence lengths\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        \n",
        "        for model_type in model_types:\n",
        "            results[model_type] = {}\n",
        "            \n",
        "            for length in lengths:\n",
        "                print(f\"Testing {model_type} with sequence length {length}...\")\n",
        "                \n",
        "                # Generate task data\n",
        "                if hasattr(task_generator, 'copy_task'):\n",
        "                    X, y = task_generator.copy_task(seq_length=length, \n",
        "                                                  gap_length=length//2, \n",
        "                                                  num_samples=500)\n",
        "                \n",
        "                # Create and compile model\n",
        "                if model_type == 'SimpleRNN':\n",
        "                    model = keras.Sequential([\n",
        "                        layers.SimpleRNN(32, input_shape=(length, 1)),\n",
        "                        layers.Dense(length, activation='linear')\n",
        "                    ])\n",
        "                elif model_type == 'LSTM':\n",
        "                    model = keras.Sequential([\n",
        "                        layers.LSTM(32, input_shape=(length, 1)),\n",
        "                        layers.Dense(length, activation='linear')\n",
        "                    ])\n",
        "                elif model_type == 'GRU':\n",
        "                    model = keras.Sequential([\n",
        "                        layers.GRU(32, input_shape=(length, 1)),\n",
        "                        layers.Dense(length, activation='linear')\n",
        "                    ])\n",
        "                \n",
        "                model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "                \n",
        "                # Reshape data for RNN\n",
        "                X_reshaped = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "                \n",
        "                # Split data\n",
        "                split_idx = int(0.8 * len(X_reshaped))\n",
        "                X_train, X_val = X_reshaped[:split_idx], X_reshaped[split_idx:]\n",
        "                y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "                \n",
        "                # Train model\n",
        "                history = model.fit(X_train, y_train, \n",
        "                                  validation_data=(X_val, y_val),\n",
        "                                  epochs=20, batch_size=32, verbose=0)\n",
        "                \n",
        "                # Store results\n",
        "                final_loss = history.history['val_loss'][-1]\n",
        "                final_mae = history.history['val_mae'][-1]\n",
        "                \n",
        "                results[model_type][length] = {\n",
        "                    'val_loss': final_loss,\n",
        "                    'val_mae': final_mae,\n",
        "                    'history': history.history\n",
        "                }\n",
        "        \n",
        "        self.results = results\n",
        "        return results\n",
        "\n",
        "# Initialize analyzers\n",
        "task_generator = LongTermDependencyTasks(vocab_size=8)\n",
        "gradient_analyzer = GradientFlowAnalyzer()\n",
        "degradation_analyzer = PerformanceDegradationAnalyzer()\n",
        "\n",
        "print(\"Long-term dependency analyzers initialized!\")\n",
        "print(\"Ready to test sequence modeling capabilities across different architectures!\")\n",
        "\n",
        "# Generate sample tasks for initial analysis\n",
        "print(\"\\nGenerating sample long-term dependency tasks...\")\n",
        "\n",
        "# Copy task with varying gap lengths\n",
        "copy_X_short, copy_y_short = task_generator.copy_task(seq_length=50, gap_length=20, num_samples=100)\n",
        "copy_X_long, copy_y_long = task_generator.copy_task(seq_length=150, gap_length=100, num_samples=100)\n",
        "\n",
        "# Adding task\n",
        "add_X, add_y = task_generator.adding_task(seq_length=100, num_samples=100)\n",
        "\n",
        "print(f\"Copy task (short): X shape = {copy_X_short.shape}, y shape = {copy_y_short.shape}\")\n",
        "print(f\"Copy task (long): X shape = {copy_X_long.shape}, y shape = {copy_y_long.shape}\")\n",
        "print(f\"Adding task: X shape = {add_X.shape}, y shape = {add_y.shape}\")\n",
        "\n",
        "# Visualize task examples\n",
        "plt.figure(figsize=(16, 10))\n",
        "\n",
        "# Copy task visualization (short)\n",
        "plt.subplot(2, 3, 1)\n",
        "sample_idx = 0\n",
        "plt.plot(copy_X_short[sample_idx], 'b-', label='Input', alpha=0.7)\n",
        "plt.plot(copy_y_short[sample_idx], 'r--', label='Target', alpha=0.7)\n",
        "plt.title('Copy Task (Short Gap)')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "\n",
        "# Copy task visualization (long)\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.plot(copy_X_long[sample_idx], 'b-', label='Input', alpha=0.7)\n",
        "plt.plot(copy_y_long[sample_idx], 'r--', label='Target', alpha=0.7)\n",
        "plt.title('Copy Task (Long Gap)')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "\n",
        "# Adding task visualization\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.plot(add_X[sample_idx, :, 0], 'b-', label='Values', alpha=0.7)\n",
        "plt.plot(add_X[sample_idx, :, 1] * 5, 'ro', label='Markers', alpha=0.7, markersize=3)\n",
        "plt.title(f'Adding Task (Target: {add_y[sample_idx]:.3f})')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "\n",
        "# Task complexity analysis\n",
        "plt.subplot(2, 3, 4)\n",
        "tasks = ['Copy Short', 'Copy Long', 'Adding']\n",
        "gaps = [20, 100, 50]  # Effective gap lengths\n",
        "plt.bar(tasks, gaps, alpha=0.7, color='skyblue')\n",
        "plt.title('Task Complexity (Gap Length)')\n",
        "plt.ylabel('Gap Length')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Information requirements\n",
        "plt.subplot(2, 3, 5)\n",
        "info_bits = [5, 5, 2]  # Bits of information to remember\n",
        "plt.bar(tasks, info_bits, alpha=0.7, color='lightcoral')\n",
        "plt.title('Information to Remember (bits)')\n",
        "plt.ylabel('Information Content')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Expected difficulty\n",
        "plt.subplot(2, 3, 6)\n",
        "difficulty = [2, 4, 3]  # Subjective difficulty ranking\n",
        "colors = ['green', 'red', 'orange']\n",
        "bars = plt.bar(tasks, difficulty, alpha=0.7, color=colors)\n",
        "plt.title('Expected Difficulty')\n",
        "plt.ylabel('Difficulty Level')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "for bar, diff in zip(bars, difficulty):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "             f'{diff}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nLong-term Dependency Tasks Generated!\")\n",
        "print(f\"Ready to analyze gradient flow and performance degradation!\")\n",
        "print(f\"Tasks cover a range of dependency lengths and complexity levels!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Long-term Dependencies Analysis\n",
        "\n",
        "## Learning Objectives\n",
        "In this notebook, we'll conduct a comprehensive analysis of the long-term dependencies problem:\n",
        "- Deep mathematical analysis of vanishing and exploding gradient problems\n",
        "- Empirical studies of dependency length limitations across different architectures\n",
        "- Gradient flow visualization over extended sequences\n",
        "- Performance degradation patterns and measurement techniques\n",
        "- Solution strategies and their effectiveness evaluation\n",
        "- Comparative analysis of RNN, LSTM, and GRU long-term memory capabilities\n",
        "\n",
        "## Introduction to Long-term Dependencies\n",
        "\n",
        "The long-term dependencies problem is one of the fundamental challenges in sequence modeling. When neural networks need to learn relationships between events separated by many time steps, traditional RNNs struggle due to the vanishing gradient problem.\n",
        "\n",
        "### Key Problems:\n",
        "1. **Vanishing Gradients**: Gradients become exponentially smaller as they propagate backward through time\n",
        "2. **Exploding Gradients**: Gradients become exponentially larger, causing training instability\n",
        "3. **Information Decay**: Important information from early time steps gets overwritten or forgotten\n",
        "4. **Limited Context Window**: Effective memory span is much shorter than theoretical capacity\n",
        "\n",
        "### Mathematical Foundation:\n",
        "For a simple RNN with repeated weight matrix W, gradients flow as:\n",
        "**∂L/∂W** ∝ **∏(t=1 to T) ∂h_t/∂h_{t-1}** = **∏(t=1 to T) W·diag(f'(·))**\n",
        "\n",
        "When the largest eigenvalue of this product is:\n",
        "- **< 1**: Vanishing gradients (exponential decay)\n",
        "- **> 1**: Exploding gradients (exponential growth)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
