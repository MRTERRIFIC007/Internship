{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Advanced LSTM Text Generation\n",
        "\n",
        "## Learning Objectives\n",
        "In this notebook, we'll implement sophisticated text generation systems using LSTM architectures:\n",
        "- Character-level and word-level generation strategies\n",
        "- Temperature-based sampling and nucleus sampling techniques\n",
        "- Beam search and advanced decoding algorithms\n",
        "- Style transfer and conditional text generation\n",
        "- Multi-layer LSTM architectures for enhanced generation\n",
        "- Quality evaluation and optimization techniques\n",
        "\n",
        "## Introduction to Advanced Text Generation\n",
        "\n",
        "Text generation is one of the most fascinating applications of RNNs. Modern LSTM-based generators can produce coherent, creative text across various domains by learning complex patterns in language structure and semantics.\n",
        "\n",
        "### Key Components:\n",
        "1. **Text Preprocessing**: Tokenization, vocabulary building, sequence preparation\n",
        "2. **Model Architecture**: Multi-layer LSTMs with dropout and regularization\n",
        "3. **Sampling Strategies**: Temperature, top-k, nucleus (top-p) sampling\n",
        "4. **Decoding Techniques**: Greedy search, beam search, diverse beam search\n",
        "5. **Conditioning**: Style control, topic guidance, prompt-based generation\n",
        "\n",
        "### Generation Quality Factors:\n",
        "- **Coherence**: Maintaining logical flow and consistency\n",
        "- **Creativity**: Generating novel and interesting content\n",
        "- **Control**: Following style, topic, or format constraints\n",
        "- **Efficiency**: Generating text quickly for real-time applications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Advanced LSTM Text Generation initialized!\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# 1. Advanced Text Preprocessor\n",
        "class AdvancedTextPreprocessor:\n",
        "    \"\"\"\n",
        "    Sophisticated text preprocessing for generation tasks\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, level='char', max_vocab_size=10000):\n",
        "        self.level = level  # 'char' or 'word'\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        self.vocab = {}\n",
        "        self.reverse_vocab = {}\n",
        "        self.vocab_size = 0\n",
        "        \n",
        "    def create_sample_texts(self):\n",
        "        \"\"\"\n",
        "        Create diverse sample texts for training\n",
        "        \"\"\"\n",
        "        texts = [\n",
        "            # Literary style\n",
        "            \"The morning sun cast golden rays through the ancient oak trees, illuminating the dewdrops that sparkled like diamonds on the emerald grass. A gentle breeze whispered secrets of forgotten times.\",\n",
        "            \n",
        "            # Technical style\n",
        "            \"Neural networks utilize backpropagation algorithms to optimize weight parameters through gradient descent. The loss function measures prediction accuracy and guides the learning process.\",\n",
        "            \n",
        "            # Conversational style\n",
        "            \"Hey there! How's your day going? I just discovered this amazing coffee shop downtown. You should definitely check it out when you have time. The atmosphere is incredible!\",\n",
        "            \n",
        "            # Narrative style\n",
        "            \"Sarah walked through the empty hallway, her footsteps echoing in the silence. The old building held memories of countless students who had passed through these same corridors decades ago.\",\n",
        "            \n",
        "            # Scientific style\n",
        "            \"The experimental results demonstrate a significant correlation between temperature variations and molecular movement patterns. Statistical analysis reveals confidence intervals within acceptable parameters.\",\n",
        "            \n",
        "            # Poetic style\n",
        "            \"Moonbeams dance on silver streams, while midnight owls share ancient dreams. The world sleeps beneath starlit skies, as time itself gently flies.\",\n",
        "            \n",
        "            # Dialogue style\n",
        "            \"I can't believe you said that!\" exclaimed Maria. \"What did you expect me to do?\" replied James, shrugging his shoulders. \"Sometimes honesty is the best policy.\",\n",
        "            \n",
        "            # Descriptive style\n",
        "            \"The vintage bookstore contained thousands of leather-bound volumes, their pages yellowed with age. Dust motes floated lazily in the afternoon sunlight streaming through tall windows.\"\n",
        "        ]\n",
        "        \n",
        "        return texts\n",
        "    \n",
        "    def build_vocabulary(self, texts):\n",
        "        \"\"\"\n",
        "        Build vocabulary from texts\n",
        "        \"\"\"\n",
        "        if self.level == 'char':\n",
        "            # Character-level vocabulary\n",
        "            all_chars = ''.join(texts)\n",
        "            char_counts = Counter(all_chars)\n",
        "            \n",
        "            # Keep most common characters\n",
        "            most_common = char_counts.most_common(self.max_vocab_size - 2)  # -2 for special tokens\n",
        "            \n",
        "            # Create vocab mapping\n",
        "            self.vocab = {'<UNK>': 0, '<PAD>': 1}\n",
        "            for i, (char, _) in enumerate(most_common):\n",
        "                self.vocab[char] = i + 2\n",
        "                \n",
        "        elif self.level == 'word':\n",
        "            # Word-level vocabulary\n",
        "            all_words = []\n",
        "            for text in texts:\n",
        "                # Simple tokenization\n",
        "                words = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text.lower())\n",
        "                all_words.extend(words)\n",
        "            \n",
        "            word_counts = Counter(all_words)\n",
        "            most_common = word_counts.most_common(self.max_vocab_size - 2)\n",
        "            \n",
        "            # Create vocab mapping\n",
        "            self.vocab = {'<UNK>': 0, '<PAD>': 1}\n",
        "            for i, (word, _) in enumerate(most_common):\n",
        "                self.vocab[word] = i + 2\n",
        "        \n",
        "        # Create reverse mapping\n",
        "        self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        \n",
        "        print(f\"Vocabulary size ({self.level}-level): {self.vocab_size}\")\n",
        "        \n",
        "    def encode_text(self, text):\n",
        "        \"\"\"\n",
        "        Encode text to sequence of integers\n",
        "        \"\"\"\n",
        "        if self.level == 'char':\n",
        "            return [self.vocab.get(char, 0) for char in text]\n",
        "        elif self.level == 'word':\n",
        "            words = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text.lower())\n",
        "            return [self.vocab.get(word, 0) for word in words]\n",
        "    \n",
        "    def decode_sequence(self, sequence):\n",
        "        \"\"\"\n",
        "        Decode sequence of integers back to text\n",
        "        \"\"\"\n",
        "        if self.level == 'char':\n",
        "            return ''.join([self.reverse_vocab.get(idx, '<UNK>') for idx in sequence])\n",
        "        elif self.level == 'word':\n",
        "            words = [self.reverse_vocab.get(idx, '<UNK>') for idx in sequence]\n",
        "            return ' '.join(words)\n",
        "    \n",
        "    def create_training_sequences(self, texts, sequence_length=50):\n",
        "        \"\"\"\n",
        "        Create training sequences from texts\n",
        "        \"\"\"\n",
        "        sequences = []\n",
        "        targets = []\n",
        "        \n",
        "        for text in texts:\n",
        "            encoded = self.encode_text(text)\n",
        "            \n",
        "            # Create overlapping sequences\n",
        "            for i in range(len(encoded) - sequence_length):\n",
        "                seq = encoded[i:i + sequence_length]\n",
        "                target = encoded[i + 1:i + sequence_length + 1]\n",
        "                \n",
        "                sequences.append(seq)\n",
        "                targets.append(target)\n",
        "        \n",
        "        return np.array(sequences), np.array(targets)\n",
        "\n",
        "# 2. Advanced Sampling Strategies\n",
        "class AdvancedSampler:\n",
        "    \"\"\"\n",
        "    Implement sophisticated sampling strategies for text generation\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def temperature_sampling(self, logits, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Sample using temperature scaling\n",
        "        \"\"\"\n",
        "        if temperature == 0:\n",
        "            return np.argmax(logits)\n",
        "        \n",
        "        # Apply temperature\n",
        "        logits = logits / temperature\n",
        "        \n",
        "        # Convert to probabilities\n",
        "        exp_logits = np.exp(logits - np.max(logits))  # Numerical stability\n",
        "        probabilities = exp_logits / np.sum(exp_logits)\n",
        "        \n",
        "        # Sample from distribution\n",
        "        return np.random.choice(len(probabilities), p=probabilities)\n",
        "    \n",
        "    def top_k_sampling(self, logits, k=40, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Sample from top-k most probable tokens\n",
        "        \"\"\"\n",
        "        # Get top-k indices\n",
        "        top_k_indices = np.argpartition(logits, -k)[-k:]\n",
        "        top_k_logits = logits[top_k_indices]\n",
        "        \n",
        "        # Apply temperature\n",
        "        if temperature != 1.0:\n",
        "            top_k_logits = top_k_logits / temperature\n",
        "        \n",
        "        # Convert to probabilities\n",
        "        exp_logits = np.exp(top_k_logits - np.max(top_k_logits))\n",
        "        probabilities = exp_logits / np.sum(exp_logits)\n",
        "        \n",
        "        # Sample and map back to original indices\n",
        "        sampled_idx = np.random.choice(len(probabilities), p=probabilities)\n",
        "        return top_k_indices[sampled_idx]\n",
        "    \n",
        "    def nucleus_sampling(self, logits, p=0.9, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Nucleus (top-p) sampling\n",
        "        \"\"\"\n",
        "        # Apply temperature\n",
        "        if temperature != 1.0:\n",
        "            logits = logits / temperature\n",
        "        \n",
        "        # Convert to probabilities\n",
        "        exp_logits = np.exp(logits - np.max(logits))\n",
        "        probabilities = exp_logits / np.sum(exp_logits)\n",
        "        \n",
        "        # Sort probabilities in descending order\n",
        "        sorted_indices = np.argsort(probabilities)[::-1]\n",
        "        sorted_probs = probabilities[sorted_indices]\n",
        "        \n",
        "        # Find nucleus (cumulative probability >= p)\n",
        "        cumulative_probs = np.cumsum(sorted_probs)\n",
        "        nucleus_size = np.searchsorted(cumulative_probs, p) + 1\n",
        "        \n",
        "        # Sample from nucleus\n",
        "        nucleus_indices = sorted_indices[:nucleus_size]\n",
        "        nucleus_probs = sorted_probs[:nucleus_size]\n",
        "        nucleus_probs = nucleus_probs / np.sum(nucleus_probs)  # Renormalize\n",
        "        \n",
        "        sampled_idx = np.random.choice(len(nucleus_probs), p=nucleus_probs)\n",
        "        return nucleus_indices[sampled_idx]\n",
        "\n",
        "# 3. Advanced LSTM Text Generator\n",
        "class AdvancedLSTMGenerator:\n",
        "    \"\"\"\n",
        "    Sophisticated LSTM-based text generator\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim=256, lstm_units=512, num_layers=2):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.lstm_units = lstm_units\n",
        "        self.num_layers = num_layers\n",
        "        self.model = None\n",
        "        self.sampler = AdvancedSampler()\n",
        "        \n",
        "    def build_model(self, sequence_length):\n",
        "        \"\"\"\n",
        "        Build advanced multi-layer LSTM model\n",
        "        \"\"\"\n",
        "        model = keras.Sequential([\n",
        "            layers.Embedding(self.vocab_size, self.embedding_dim, \n",
        "                           input_length=sequence_length),\n",
        "            layers.Dropout(0.2)\n",
        "        ])\n",
        "        \n",
        "        # Add multiple LSTM layers\n",
        "        for i in range(self.num_layers):\n",
        "            return_sequences = (i < self.num_layers - 1)  # All but last layer return sequences\n",
        "            \n",
        "            model.add(layers.LSTM(\n",
        "                self.lstm_units,\n",
        "                return_sequences=return_sequences,\n",
        "                dropout=0.3,\n",
        "                recurrent_dropout=0.3\n",
        "            ))\n",
        "            \n",
        "            if return_sequences:\n",
        "                model.add(layers.Dropout(0.3))\n",
        "        \n",
        "        # Output layer\n",
        "        model.add(layers.Dense(self.vocab_size, activation='softmax'))\n",
        "        \n",
        "        self.model = model\n",
        "        return model\n",
        "    \n",
        "    def train_model(self, X, y, epochs=50, batch_size=64):\n",
        "        \"\"\"\n",
        "        Train the text generation model\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not built. Call build_model() first.\")\n",
        "        \n",
        "        # Compile model\n",
        "        self.model.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        \n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1\n",
        "            ),\n",
        "            keras.callbacks.EarlyStopping(\n",
        "                monitor='loss', patience=10, restore_best_weights=True\n",
        "            )\n",
        "        ]\n",
        "        \n",
        "        # Train model\n",
        "        history = self.model.fit(\n",
        "            X, y,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "        \n",
        "        return history\n",
        "    \n",
        "    def generate_text(self, seed_text, preprocessor, length=200, \n",
        "                     sampling_strategy='temperature', **sampling_kwargs):\n",
        "        \"\"\"\n",
        "        Generate text using various sampling strategies\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained. Train model first.\")\n",
        "        \n",
        "        # Encode seed text\n",
        "        encoded_seed = preprocessor.encode_text(seed_text)\n",
        "        sequence_length = self.model.input_shape[1]\n",
        "        \n",
        "        # Pad or truncate seed to sequence length\n",
        "        if len(encoded_seed) < sequence_length:\n",
        "            encoded_seed = [1] * (sequence_length - len(encoded_seed)) + encoded_seed  # Pad with PAD token\n",
        "        else:\n",
        "            encoded_seed = encoded_seed[-sequence_length:]\n",
        "        \n",
        "        generated = encoded_seed.copy()\n",
        "        \n",
        "        # Generate text\n",
        "        for _ in range(length):\n",
        "            # Prepare input\n",
        "            input_seq = np.array([generated[-sequence_length:]])\n",
        "            \n",
        "            # Predict next token probabilities\n",
        "            predictions = self.model.predict(input_seq, verbose=0)[0]\n",
        "            \n",
        "            # Sample next token based on strategy\n",
        "            if sampling_strategy == 'temperature':\n",
        "                temperature = sampling_kwargs.get('temperature', 1.0)\n",
        "                next_token = self.sampler.temperature_sampling(predictions, temperature)\n",
        "            elif sampling_strategy == 'top_k':\n",
        "                k = sampling_kwargs.get('k', 40)\n",
        "                temperature = sampling_kwargs.get('temperature', 1.0)\n",
        "                next_token = self.sampler.top_k_sampling(predictions, k, temperature)\n",
        "            elif sampling_strategy == 'nucleus':\n",
        "                p = sampling_kwargs.get('p', 0.9)\n",
        "                temperature = sampling_kwargs.get('temperature', 1.0)\n",
        "                next_token = self.sampler.nucleus_sampling(predictions, p, temperature)\n",
        "            else:  # Greedy\n",
        "                next_token = np.argmax(predictions)\n",
        "            \n",
        "            generated.append(next_token)\n",
        "        \n",
        "        # Decode generated sequence (excluding seed)\n",
        "        generated_tokens = generated[len(encoded_seed):]\n",
        "        return preprocessor.decode_sequence(generated_tokens)\n",
        "\n",
        "# Initialize components\n",
        "print(\"Initializing advanced text generation components...\")\n",
        "\n",
        "# Create preprocessors for both character and word level\n",
        "char_preprocessor = AdvancedTextPreprocessor(level='char', max_vocab_size=100)\n",
        "word_preprocessor = AdvancedTextPreprocessor(level='word', max_vocab_size=2000)\n",
        "\n",
        "# Generate sample texts\n",
        "sample_texts = char_preprocessor.create_sample_texts()\n",
        "print(f\"Created {len(sample_texts)} sample texts covering different styles\")\n",
        "\n",
        "# Build vocabularies\n",
        "print(\"\\nBuilding vocabularies...\")\n",
        "char_preprocessor.build_vocabulary(sample_texts)\n",
        "word_preprocessor.build_vocabulary(sample_texts)\n",
        "\n",
        "# Create training sequences\n",
        "sequence_length = 50\n",
        "char_X, char_y = char_preprocessor.create_training_sequences(sample_texts, sequence_length)\n",
        "word_X, word_y = word_preprocessor.create_training_sequences(sample_texts, sequence_length//2)  # Shorter for words\n",
        "\n",
        "print(f\"\\nCharacter-level sequences: {char_X.shape}\")\n",
        "print(f\"Word-level sequences: {word_X.shape}\")\n",
        "\n",
        "# Visualize sample generation strategies\n",
        "print(\"\\nDemonstrating sampling strategies...\")\n",
        "\n",
        "# Create a simple test case\n",
        "test_logits = np.array([1.0, 2.0, 0.5, 3.0, 1.5, 0.8, 2.5])  # Example logits\n",
        "sampler = AdvancedSampler()\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Temperature sampling comparison\n",
        "temperatures = [0.5, 1.0, 1.5, 2.0]\n",
        "temp_samples = []\n",
        "\n",
        "for temp in temperatures:\n",
        "    samples = [sampler.temperature_sampling(test_logits, temp) for _ in range(100)]\n",
        "    temp_samples.append(samples)\n",
        "\n",
        "plt.subplot(2, 3, 1)\n",
        "for i, (temp, samples) in enumerate(zip(temperatures, temp_samples)):\n",
        "    counts = np.bincount(samples, minlength=len(test_logits))\n",
        "    plt.bar(np.arange(len(test_logits)) + i*0.2, counts, width=0.2, \n",
        "           label=f'T={temp}', alpha=0.7)\n",
        "plt.title('Temperature Sampling Comparison')\n",
        "plt.xlabel('Token Index')\n",
        "plt.ylabel('Sample Count')\n",
        "plt.legend()\n",
        "\n",
        "# Top-k sampling\n",
        "plt.subplot(2, 3, 2)\n",
        "k_values = [1, 2, 3, 5]\n",
        "for k in k_values:\n",
        "    samples = [sampler.top_k_sampling(test_logits, k) for _ in range(100)]\n",
        "    counts = np.bincount(samples, minlength=len(test_logits))\n",
        "    plt.bar(np.arange(len(test_logits)), counts, alpha=0.7, label=f'k={k}')\n",
        "plt.title('Top-k Sampling (k variations)')\n",
        "plt.xlabel('Token Index')\n",
        "plt.ylabel('Sample Count')\n",
        "plt.legend()\n",
        "\n",
        "# Nucleus sampling\n",
        "plt.subplot(2, 3, 3)\n",
        "p_values = [0.5, 0.7, 0.9, 0.95]\n",
        "for p in p_values:\n",
        "    samples = [sampler.nucleus_sampling(test_logits, p) for _ in range(100)]\n",
        "    counts = np.bincount(samples, minlength=len(test_logits))\n",
        "    plt.bar(np.arange(len(test_logits)), counts, alpha=0.7, label=f'p={p}')\n",
        "plt.title('Nucleus Sampling (p variations)')\n",
        "plt.xlabel('Token Index')\n",
        "plt.ylabel('Sample Count')\n",
        "plt.legend()\n",
        "\n",
        "# Logits distribution\n",
        "plt.subplot(2, 3, 4)\n",
        "softmax_probs = np.exp(test_logits) / np.sum(np.exp(test_logits))\n",
        "plt.bar(range(len(test_logits)), test_logits, alpha=0.7, label='Logits')\n",
        "plt.title('Original Logits')\n",
        "plt.xlabel('Token Index')\n",
        "plt.ylabel('Logit Value')\n",
        "\n",
        "# Probability distribution\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.bar(range(len(test_logits)), softmax_probs, alpha=0.7, color='orange')\n",
        "plt.title('Softmax Probabilities')\n",
        "plt.xlabel('Token Index')\n",
        "plt.ylabel('Probability')\n",
        "\n",
        "# Sampling strategy comparison\n",
        "plt.subplot(2, 3, 6)\n",
        "greedy_choice = np.argmax(test_logits)\n",
        "temp_choice = sampler.temperature_sampling(test_logits, 1.0)\n",
        "topk_choice = sampler.top_k_sampling(test_logits, 3)\n",
        "nucleus_choice = sampler.nucleus_sampling(test_logits, 0.9)\n",
        "\n",
        "strategies = ['Greedy', 'Temperature', 'Top-k', 'Nucleus']\n",
        "choices = [greedy_choice, temp_choice, topk_choice, nucleus_choice]\n",
        "\n",
        "plt.bar(strategies, choices, alpha=0.7, color=['red', 'blue', 'green', 'purple'])\n",
        "plt.title('Sample Choices by Strategy')\n",
        "plt.ylabel('Selected Token Index')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nSampling Strategy Analysis:\")\n",
        "print(f\"Greedy choice: {greedy_choice}\")\n",
        "print(f\"Temperature choice: {temp_choice}\")\n",
        "print(f\"Top-k choice: {topk_choice}\")\n",
        "print(f\"Nucleus choice: {nucleus_choice}\")\n",
        "\n",
        "print(f\"\\nAdvanced LSTM Text Generation Setup Complete!\")\n",
        "print(f\"Ready to build and train sophisticated generation models!\")\n",
        "print(f\"Character vocab size: {char_preprocessor.vocab_size}\")\n",
        "print(f\"Word vocab size: {word_preprocessor.vocab_size}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
