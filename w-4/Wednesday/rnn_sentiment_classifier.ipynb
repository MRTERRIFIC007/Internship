{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# RNN-based Sentiment Classification\n",
        "\n",
        "## Learning Objectives\n",
        "In this notebook, we'll implement and train RNN models for sentiment analysis:\n",
        "- Design RNN architectures specifically for text classification\n",
        "- Implement embedding layers for text representation\n",
        "- Build and compare different RNN variants (SimpleRNN, LSTM, GRU)\n",
        "- Explore bidirectional RNNs for sentiment analysis\n",
        "- Optimize training strategies and hyperparameters\n",
        "- Evaluate and interpret model performance\n",
        "\n",
        "## Introduction to RNN-based Text Classification\n",
        "\n",
        "RNNs are particularly well-suited for sentiment analysis because they can:\n",
        "- Process variable-length sequences naturally\n",
        "- Capture sequential dependencies in text\n",
        "- Learn contextual representations of words\n",
        "- Handle the temporal nature of language\n",
        "\n",
        "Today we'll build progressively more sophisticated RNN models and compare their performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Libraries imported successfully!\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreate sentiment dataset from fundamentals notebook\n",
        "def create_sentiment_dataset():\n",
        "    \"\"\"Create the same sentiment dataset for consistency\"\"\"\n",
        "    positive_texts = [\n",
        "        \"I absolutely love this product! It exceeded all my expectations.\",\n",
        "        \"Amazing quality and fast delivery. Highly recommend!\",\n",
        "        \"This is the best purchase I've made in years. Perfect!\",\n",
        "        \"Outstanding customer service and excellent product quality.\",\n",
        "        \"I'm so happy with this purchase. Will definitely buy again!\",\n",
        "        \"Fantastic! Everything arrived on time and in perfect condition.\",\n",
        "        \"Great value for money. The product works perfectly.\",\n",
        "        \"Superb quality and amazing features. Love it!\",\n",
        "        \"Excellent product! Exactly what I was looking for.\",\n",
        "        \"Perfect fit and great design. Very satisfied!\",\n",
        "        \"This product is incredible! Exceeds expectations.\",\n",
        "        \"Beautiful design and excellent functionality. Recommended!\",\n",
        "        \"Top quality! Fast shipping and great packaging.\",\n",
        "        \"Love the features and ease of use. Perfect!\",\n",
        "        \"Outstanding value and performance. Very happy!\",\n",
        "        \"Brilliant product! Works exactly as described.\",\n",
        "        \"Impressive quality and attention to detail.\",\n",
        "        \"Wonderful experience from start to finish!\",\n",
        "        \"This is exactly what I needed. Perfect solution!\",\n",
        "        \"Excellent craftsmanship and great customer support.\",\n",
        "        \"Amazing results! Better than expected.\",\n",
        "        \"High quality materials and excellent build.\",\n",
        "        \"Perfect for my needs. Great functionality!\",\n",
        "        \"Superb performance and reliable operation.\",\n",
        "        \"Love the innovation and thoughtful design.\"\n",
        "    ]\n",
        "    \n",
        "    negative_texts = [\n",
        "        \"Terrible product! Complete waste of money.\",\n",
        "        \"Poor quality and disappointing performance. Avoid!\",\n",
        "        \"This is the worst purchase I've ever made.\",\n",
        "        \"Awful customer service and defective product.\",\n",
        "        \"I hate this product. Nothing works as advertised.\",\n",
        "        \"Horrible experience. Product broke immediately.\",\n",
        "        \"Overpriced and poor quality. Very disappointed.\",\n",
        "        \"Useless product with terrible design flaws.\",\n",
        "        \"Don't buy this! It's a complete scam.\",\n",
        "        \"Worst quality I've ever seen. Returned immediately.\",\n",
        "        \"Broken on arrival and no customer support.\",\n",
        "        \"This product is garbage. Save your money!\",\n",
        "        \"Terrible build quality and false advertising.\",\n",
        "        \"Complete disappointment. Nothing works properly.\",\n",
        "        \"Poor materials and shoddy construction.\",\n",
        "        \"Defective product with misleading description.\",\n",
        "        \"Nightmare experience with this purchase.\",\n",
        "        \"Cheap quality and overpriced. Avoid at all costs!\",\n",
        "        \"Broken after one day. Terrible reliability.\",\n",
        "        \"Worst customer service experience ever.\",\n",
        "        \"Product failed completely. Total waste.\",\n",
        "        \"Inferior quality and poor performance.\",\n",
        "        \"Disappointing results and bad value.\",\n",
        "        \"Unreliable and poorly designed product.\",\n",
        "        \"Frustrated with poor quality and service.\"\n",
        "    ]\n",
        "    \n",
        "    neutral_texts = [\n",
        "        \"The product is okay. Nothing special but works.\",\n",
        "        \"Average quality for the price. Could be better.\",\n",
        "        \"It's fine. Does what it's supposed to do.\",\n",
        "        \"Decent product with standard features.\",\n",
        "        \"Regular quality item. Nothing extraordinary.\",\n",
        "        \"Standard product with basic functionality.\",\n",
        "        \"It works as described. No surprises.\",\n",
        "        \"Average performance and typical quality.\",\n",
        "        \"Basic product that meets minimum requirements.\",\n",
        "        \"Normal quality for this price range.\",\n",
        "        \"The product is acceptable but not outstanding.\",\n",
        "        \"Standard features and regular performance.\",\n",
        "        \"It's an ordinary product with basic design.\",\n",
        "        \"Typical quality and standard delivery.\",\n",
        "        \"Regular item with expected functionality.\",\n",
        "        \"Average build quality and normal features.\",\n",
        "        \"Standard product that works adequately.\",\n",
        "        \"Basic design with typical performance.\",\n",
        "        \"It's a regular product with normal quality.\",\n",
        "        \"Standard functionality and average materials.\",\n",
        "        \"Ordinary product with expected features.\",\n",
        "        \"Normal quality and standard performance.\",\n",
        "        \"Basic item that meets requirements.\",\n",
        "        \"Average product with typical characteristics.\",\n",
        "        \"Standard quality and regular functionality.\"\n",
        "    ]\n",
        "    \n",
        "    mixed_texts = [\n",
        "        \"Good product but expensive for what you get.\",\n",
        "        \"Fast delivery but product quality could be better.\",\n",
        "        \"Great customer service but the product is average.\",\n",
        "        \"Love the design but functionality is limited.\",\n",
        "        \"Excellent packaging but disappointing content.\",\n",
        "        \"Quick shipping but poor build quality.\",\n",
        "        \"Nice features but overpriced for the value.\",\n",
        "        \"Good concept but poor execution.\",\n",
        "        \"Beautiful appearance but lacks durability.\",\n",
        "        \"Helpful support but defective product.\"\n",
        "    ]\n",
        "    \n",
        "    texts = positive_texts + negative_texts + neutral_texts + mixed_texts\n",
        "    labels = (['positive'] * len(positive_texts) + \n",
        "              ['negative'] * len(negative_texts) + \n",
        "              ['neutral'] * (len(neutral_texts) + len(mixed_texts)))\n",
        "    \n",
        "    df = pd.DataFrame({'text': texts, 'sentiment': labels})\n",
        "    return df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Create dataset\n",
        "sentiment_df = create_sentiment_dataset()\n",
        "print(f\"Dataset created with {len(sentiment_df)} samples\")\n",
        "print(f\"Class distribution:\\n{sentiment_df['sentiment'].value_counts()}\")\n",
        "\n",
        "# Basic preprocessing for RNN\n",
        "def preprocess_for_rnn(text):\n",
        "    \"\"\"Minimal preprocessing for RNN - preserve structure\"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^\\w\\s\\.\\!\\?]', '', text)  # Keep basic punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "# Preprocess texts\n",
        "processed_texts = [preprocess_for_rnn(text) for text in sentiment_df['text']]\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(sentiment_df['sentiment'])\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "print(f\"\\nLabel encoding:\")\n",
        "for i, label in enumerate(label_encoder.classes_):\n",
        "    print(f\"{i}: {label}\")\n",
        "\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "# Split the data\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "    processed_texts, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels\n",
        ")\n",
        "\n",
        "print(f\"\\nData split:\")\n",
        "print(f\"Training samples: {len(X_train_text)}\")\n",
        "print(f\"Test samples: {len(X_test_text)}\")\n",
        "\n",
        "# Text tokenization and sequence preparation\n",
        "MAX_VOCAB_SIZE = 5000\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "\n",
        "# Create tokenizer\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train_text)\n",
        "\n",
        "# Convert texts to sequences\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train_text)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test_text)\n",
        "\n",
        "# Pad sequences\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "\n",
        "# Get actual vocabulary size\n",
        "vocab_size = min(len(tokenizer.word_index) + 1, MAX_VOCAB_SIZE)\n",
        "\n",
        "print(f\"\\nSequence preparation:\")\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Max sequence length: {MAX_SEQUENCE_LENGTH}\")\n",
        "print(f\"Training sequences shape: {X_train_padded.shape}\")\n",
        "print(f\"Test sequences shape: {X_test_padded.shape}\")\n",
        "\n",
        "# Analyze sequence lengths\n",
        "original_lengths = [len(seq) for seq in X_train_sequences]\n",
        "print(f\"\\nSequence length statistics:\")\n",
        "print(f\"Mean: {np.mean(original_lengths):.1f}\")\n",
        "print(f\"Median: {np.median(original_lengths):.1f}\")\n",
        "print(f\"Max: {np.max(original_lengths)}\")\n",
        "print(f\"Min: {np.min(original_lengths)}\")\n",
        "\n",
        "# Visualize data preparation\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Sequence length distribution\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.hist(original_lengths, bins=20, alpha=0.7, edgecolor='black')\n",
        "plt.axvline(MAX_SEQUENCE_LENGTH, color='red', linestyle='--', label=f'Max Length: {MAX_SEQUENCE_LENGTH}')\n",
        "plt.xlabel('Sequence Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Original Sequence Length Distribution')\n",
        "plt.legend()\n",
        "\n",
        "# Vocabulary frequency\n",
        "plt.subplot(2, 3, 2)\n",
        "word_freq = Counter([word for seq in X_train_sequences for word in seq])\n",
        "top_words = word_freq.most_common(20)\n",
        "words, freqs = zip(*top_words)\n",
        "plt.bar(range(len(words)), freqs)\n",
        "plt.xlabel('Word Index')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 20 Word Frequencies')\n",
        "plt.xticks(range(0, len(words), 2))\n",
        "\n",
        "# Class distribution in training set\n",
        "plt.subplot(2, 3, 3)\n",
        "train_class_dist = np.bincount(y_train)\n",
        "test_class_dist = np.bincount(y_test)\n",
        "class_names = label_encoder.classes_\n",
        "\n",
        "x = np.arange(len(class_names))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, train_class_dist, width, label='Train', alpha=0.7)\n",
        "plt.bar(x + width/2, test_class_dist, width, label='Test', alpha=0.7)\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Class Distribution: Train vs Test')\n",
        "plt.xticks(x, class_names, rotation=45)\n",
        "plt.legend()\n",
        "\n",
        "# Padding visualization\n",
        "plt.subplot(2, 3, 4)\n",
        "# Show padding effect on a few sequences\n",
        "sample_indices = [0, 1, 2, 3, 4]\n",
        "for i, idx in enumerate(sample_indices):\n",
        "    seq = X_train_padded[idx]\n",
        "    non_zero = np.count_nonzero(seq)\n",
        "    padding = len(seq) - non_zero\n",
        "    plt.bar(i, non_zero, label='Text' if i == 0 else \"\", color='skyblue')\n",
        "    plt.bar(i, padding, bottom=non_zero, label='Padding' if i == 0 else \"\", color='lightcoral')\n",
        "\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Sequence Length')\n",
        "plt.title('Padding Effect on Sample Sequences')\n",
        "plt.legend()\n",
        "\n",
        "# Word coverage analysis\n",
        "plt.subplot(2, 3, 5)\n",
        "# Calculate how many words are covered by vocabulary\n",
        "all_words = [word for text in X_train_text for word in text.split()]\n",
        "unique_words = set(all_words)\n",
        "covered_words = set([word for word in unique_words if word in tokenizer.word_index and tokenizer.word_index[word] < MAX_VOCAB_SIZE])\n",
        "\n",
        "coverage = len(covered_words) / len(unique_words) * 100\n",
        "oov_rate = 100 - coverage\n",
        "\n",
        "plt.pie([coverage, oov_rate], labels=['Covered', 'OOV'], autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'])\n",
        "plt.title(f'Vocabulary Coverage\\n({len(covered_words)}/{len(unique_words)} words)')\n",
        "\n",
        "# Sequence padding statistics\n",
        "plt.subplot(2, 3, 6)\n",
        "padding_stats = []\n",
        "labels = []\n",
        "for i in range(len(X_train_padded)):\n",
        "    seq = X_train_padded[i]\n",
        "    padding_count = np.sum(seq == 0)\n",
        "    padding_stats.append(padding_count)\n",
        "\n",
        "plt.hist(padding_stats, bins=20, alpha=0.7, edgecolor='black')\n",
        "plt.xlabel('Number of Padding Tokens')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Padding Distribution in Sequences')\n",
        "plt.axvline(np.mean(padding_stats), color='red', linestyle='--', label=f'Mean: {np.mean(padding_stats):.1f}')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nData preparation complete!\")\n",
        "print(f\"Ready to build RNN models for sentiment classification.\")\n",
        "print(f\"Target performance to beat: {1/num_classes:.3f} (random baseline)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Building RNN Architectures for Sentiment Classification\n",
        "\n",
        "We'll create multiple RNN architectures and compare their performance:\n",
        "1. SimpleRNN baseline\n",
        "2. LSTM model\n",
        "3. GRU model\n",
        "4. Bidirectional RNN variants\n",
        "5. Stacked architectures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RNN Architecture Factory\n",
        "def create_rnn_models(vocab_size, max_length, num_classes, embedding_dim=100, rnn_units=64):\n",
        "    \"\"\"\n",
        "    Create different RNN architectures for sentiment classification\n",
        "    \"\"\"\n",
        "    models = {}\n",
        "    \n",
        "    # 1. Simple RNN\n",
        "    models['SimpleRNN'] = keras.Sequential([\n",
        "        layers.Embedding(vocab_size, embedding_dim, input_length=max_length, mask_zero=True),\n",
        "        layers.SimpleRNN(rnn_units, dropout=0.2, recurrent_dropout=0.2),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='SimpleRNN')\n",
        "    \n",
        "    # 2. LSTM\n",
        "    models['LSTM'] = keras.Sequential([\n",
        "        layers.Embedding(vocab_size, embedding_dim, input_length=max_length, mask_zero=True),\n",
        "        layers.LSTM(rnn_units, dropout=0.2, recurrent_dropout=0.2),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='LSTM')\n",
        "    \n",
        "    # 3. GRU\n",
        "    models['GRU'] = keras.Sequential([\n",
        "        layers.Embedding(vocab_size, embedding_dim, input_length=max_length, mask_zero=True),\n",
        "        layers.GRU(rnn_units, dropout=0.2, recurrent_dropout=0.2),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='GRU')\n",
        "    \n",
        "    # 4. Bidirectional LSTM\n",
        "    models['BiLSTM'] = keras.Sequential([\n",
        "        layers.Embedding(vocab_size, embedding_dim, input_length=max_length, mask_zero=True),\n",
        "        layers.Bidirectional(layers.LSTM(rnn_units//2, dropout=0.2, recurrent_dropout=0.2)),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='BiLSTM')\n",
        "    \n",
        "    # 5. Bidirectional GRU\n",
        "    models['BiGRU'] = keras.Sequential([\n",
        "        layers.Embedding(vocab_size, embedding_dim, input_length=max_length, mask_zero=True),\n",
        "        layers.Bidirectional(layers.GRU(rnn_units//2, dropout=0.2, recurrent_dropout=0.2)),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='BiGRU')\n",
        "    \n",
        "    # 6. Stacked LSTM\n",
        "    models['StackedLSTM'] = keras.Sequential([\n",
        "        layers.Embedding(vocab_size, embedding_dim, input_length=max_length, mask_zero=True),\n",
        "        layers.LSTM(rnn_units, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
        "        layers.LSTM(rnn_units//2, dropout=0.2, recurrent_dropout=0.2),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='StackedLSTM')\n",
        "    \n",
        "    return models\n",
        "\n",
        "# Create all models\n",
        "rnn_models = create_rnn_models(\n",
        "    vocab_size=vocab_size,\n",
        "    max_length=MAX_SEQUENCE_LENGTH,\n",
        "    num_classes=num_classes,\n",
        "    embedding_dim=64,  # Smaller embedding for our vocabulary\n",
        "    rnn_units=64\n",
        ")\n",
        "\n",
        "# Display model architectures\n",
        "print(\"RNN Model Architectures:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for name, model in rnn_models.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"Parameters: {model.count_params():,}\")\n",
        "    model.summary()\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Compare model complexities\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Parameter count comparison\n",
        "plt.subplot(2, 3, 1)\n",
        "model_names = list(rnn_models.keys())\n",
        "param_counts = [model.count_params() for model in rnn_models.values()]\n",
        "colors = ['skyblue', 'lightcoral', 'lightgreen', 'orange', 'pink', 'yellow']\n",
        "\n",
        "bars = plt.bar(model_names, param_counts, color=colors)\n",
        "plt.title('Model Complexity (Parameters)')\n",
        "plt.ylabel('Number of Parameters')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Add value labels\n",
        "for bar, count in zip(bars, param_counts):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(param_counts)*0.01,\n",
        "             f'{count:,}', ha='center', va='bottom', rotation=90)\n",
        "\n",
        "# Memory estimation\n",
        "plt.subplot(2, 3, 2)\n",
        "memory_mb = [params * 4 / (1024 * 1024) for params in param_counts]  # 4 bytes per float32 parameter\n",
        "plt.bar(model_names, memory_mb, color=colors)\n",
        "plt.title('Estimated Memory Usage')\n",
        "plt.ylabel('Memory (MB)')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Layer count analysis\n",
        "plt.subplot(2, 3, 3)\n",
        "layer_counts = []\n",
        "rnn_layer_counts = []\n",
        "\n",
        "for model in rnn_models.values():\n",
        "    total_layers = len(model.layers)\n",
        "    rnn_layers = sum(1 for layer in model.layers if any(rnn_type in str(type(layer)) \n",
        "                    for rnn_type in ['LSTM', 'GRU', 'SimpleRNN', 'Bidirectional']))\n",
        "    layer_counts.append(total_layers)\n",
        "    rnn_layer_counts.append(rnn_layers)\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, layer_counts, width, label='Total Layers', alpha=0.7)\n",
        "plt.bar(x + width/2, rnn_layer_counts, width, label='RNN Layers', alpha=0.7)\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Layer Count')\n",
        "plt.title('Layer Analysis')\n",
        "plt.xticks(x, model_names, rotation=45)\n",
        "plt.legend()\n",
        "\n",
        "# Theoretical complexity\n",
        "plt.subplot(2, 3, 4)\n",
        "# Relative computational complexity (simplified)\n",
        "complexity_scores = [1.0, 4.0, 3.0, 8.0, 6.0, 8.0]  # Relative to SimpleRNN\n",
        "plt.bar(model_names, complexity_scores, color=colors)\n",
        "plt.title('Relative Computational Complexity')\n",
        "plt.ylabel('Complexity Score')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Training time estimation (theoretical)\n",
        "plt.subplot(2, 3, 5)\n",
        "# Based on parameter count and complexity\n",
        "training_time_est = [p * c / 1000 for p, c in zip(param_counts, complexity_scores)]\n",
        "plt.bar(model_names, training_time_est, color=colors)\n",
        "plt.title('Estimated Training Time (Relative)')\n",
        "plt.ylabel('Relative Time')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Architecture comparison table\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.axis('off')\n",
        "table_data = []\n",
        "for i, name in enumerate(model_names):\n",
        "    table_data.append([\n",
        "        name,\n",
        "        f\"{param_counts[i]:,}\",\n",
        "        f\"{layer_counts[i]}\",\n",
        "        f\"{rnn_layer_counts[i]}\",\n",
        "        f\"{complexity_scores[i]:.1f}\"\n",
        "    ])\n",
        "\n",
        "table = plt.table(cellText=table_data,\n",
        "                 colLabels=['Model', 'Parameters', 'Total\\nLayers', 'RNN\\nLayers', 'Complexity'],\n",
        "                 cellLoc='center',\n",
        "                 loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(9)\n",
        "table.scale(1.2, 1.5)\n",
        "plt.title('Model Comparison Summary')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nModel Creation Complete!\")\n",
        "print(f\"Created {len(rnn_models)} different RNN architectures\")\n",
        "print(f\"Ready for training and evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training and Evaluation Pipeline\n",
        "def train_and_evaluate_models(models, X_train, y_train, X_test, y_test, epochs=20, batch_size=32):\n",
        "    \"\"\"\n",
        "    Train and evaluate multiple RNN models\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    histories = {}\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        \n",
        "        # Create callbacks\n",
        "        early_stopping = keras.callbacks.EarlyStopping(\n",
        "            patience=5, restore_best_weights=True, monitor='val_accuracy'\n",
        "        )\n",
        "        \n",
        "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "            patience=3, factor=0.5, min_lr=1e-6, monitor='val_accuracy'\n",
        "        )\n",
        "        \n",
        "        # Train model\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=0.2,\n",
        "            callbacks=[early_stopping, reduce_lr],\n",
        "            verbose=1\n",
        "        )\n",
        "        \n",
        "        # Evaluate on test set\n",
        "        test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "        \n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test, verbose=0)\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "        \n",
        "        # Store results\n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'test_accuracy': test_accuracy,\n",
        "            'test_loss': test_loss,\n",
        "            'y_pred': y_pred_classes,\n",
        "            'y_pred_proba': y_pred,\n",
        "            'history': history\n",
        "        }\n",
        "        histories[name] = history.history\n",
        "        \n",
        "        print(f\"Final test accuracy: {test_accuracy:.4f}\")\n",
        "    \n",
        "    return results, histories\n",
        "\n",
        "# Train selected models (subset for demonstration)\n",
        "selected_models = {\n",
        "    'SimpleRNN': rnn_models['SimpleRNN'],\n",
        "    'LSTM': rnn_models['LSTM'],\n",
        "    'BiLSTM': rnn_models['BiLSTM']\n",
        "}\n",
        "\n",
        "print(\"Training RNN models for sentiment classification...\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "# Train models\n",
        "model_results, training_histories = train_and_evaluate_models(\n",
        "    selected_models, \n",
        "    X_train_padded, y_train, \n",
        "    X_test_padded, y_test,\n",
        "    epochs=15,  # Reduced for demonstration\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "# Analyze results\n",
        "print(f\"\\nTraining Results Summary:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "best_accuracy = 0\n",
        "best_model = \"\"\n",
        "\n",
        "for name, result in model_results.items():\n",
        "    accuracy = result['test_accuracy']\n",
        "    loss = result['test_loss']\n",
        "    print(f\"{name:12s}: Accuracy={accuracy:.4f}, Loss={loss:.4f}\")\n",
        "    \n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_model = name\n",
        "\n",
        "print(f\"\\nBest performing model: {best_model} with {best_accuracy:.4f} accuracy\")\n",
        "\n",
        "# Detailed analysis of best model\n",
        "best_result = model_results[best_model]\n",
        "print(f\"\\nDetailed Results for {best_model}:\")\n",
        "print(classification_report(y_test, best_result['y_pred'], \n",
        "                          target_names=label_encoder.classes_))\n",
        "\n",
        "# Visualization of results\n",
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "# Training history - Loss\n",
        "plt.subplot(3, 4, 1)\n",
        "for name, history in training_histories.items():\n",
        "    plt.plot(history['loss'], label=f'{name} (train)', linestyle='-')\n",
        "    plt.plot(history['val_loss'], label=f'{name} (val)', linestyle='--')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Training history - Accuracy\n",
        "plt.subplot(3, 4, 2)\n",
        "for name, history in training_histories.items():\n",
        "    plt.plot(history['accuracy'], label=f'{name} (train)', linestyle='-')\n",
        "    plt.plot(history['val_accuracy'], label=f'{name} (val)', linestyle='--')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Model performance comparison\n",
        "plt.subplot(3, 4, 3)\n",
        "model_names = list(model_results.keys())\n",
        "accuracies = [result['test_accuracy'] for result in model_results.values()]\n",
        "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
        "\n",
        "bars = plt.bar(model_names, accuracies, color=colors)\n",
        "plt.title('Test Accuracy Comparison')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Add value labels\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "             f'{acc:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# Confusion matrix for best model\n",
        "plt.subplot(3, 4, 4)\n",
        "cm = confusion_matrix(y_test, best_result['y_pred'])\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.title(f'Confusion Matrix - {best_model}')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "\n",
        "# Training efficiency comparison\n",
        "plt.subplot(3, 4, 5)\n",
        "training_epochs = [len(history['loss']) for history in training_histories.values()]\n",
        "plt.bar(model_names, training_epochs, color=colors)\n",
        "plt.title('Training Epochs (Early Stopping)')\n",
        "plt.ylabel('Epochs')\n",
        "\n",
        "# Learning curves comparison\n",
        "plt.subplot(3, 4, 6)\n",
        "for name, history in training_histories.items():\n",
        "    final_train_acc = history['accuracy'][-1]\n",
        "    final_val_acc = history['val_accuracy'][-1]\n",
        "    overfitting = final_train_acc - final_val_acc\n",
        "    plt.bar(name, overfitting, color=colors[list(training_histories.keys()).index(name)])\n",
        "\n",
        "plt.title('Overfitting Analysis (Train - Val Acc)')\n",
        "plt.ylabel('Overfitting Score')\n",
        "plt.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Class-wise performance\n",
        "plt.subplot(3, 4, 7)\n",
        "class_accuracies = []\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    class_mask = y_test == i\n",
        "    if np.sum(class_mask) > 0:\n",
        "        class_acc = np.mean(best_result['y_pred'][class_mask] == y_test[class_mask])\n",
        "        class_accuracies.append(class_acc)\n",
        "    else:\n",
        "        class_accuracies.append(0)\n",
        "\n",
        "plt.bar(label_encoder.classes_, class_accuracies, color=['lightgreen', 'lightcoral', 'lightblue'])\n",
        "plt.title(f'Class-wise Accuracy - {best_model}')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Prediction confidence analysis\n",
        "plt.subplot(3, 4, 8)\n",
        "prediction_confidences = np.max(best_result['y_pred_proba'], axis=1)\n",
        "correct_predictions = best_result['y_pred'] == y_test\n",
        "\n",
        "plt.hist(prediction_confidences[correct_predictions], alpha=0.7, label='Correct', bins=20)\n",
        "plt.hist(prediction_confidences[~correct_predictions], alpha=0.7, label='Incorrect', bins=20)\n",
        "plt.title('Prediction Confidence Distribution')\n",
        "plt.xlabel('Confidence')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "\n",
        "# Loss comparison\n",
        "plt.subplot(3, 4, 9)\n",
        "losses = [result['test_loss'] for result in model_results.values()]\n",
        "plt.bar(model_names, losses, color=colors)\n",
        "plt.title('Test Loss Comparison')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "# Parameter efficiency\n",
        "plt.subplot(3, 4, 10)\n",
        "param_counts = [selected_models[name].count_params() for name in model_names]\n",
        "efficiency_scores = [acc / (params / 1000) for acc, params in zip(accuracies, param_counts)]\n",
        "plt.bar(model_names, efficiency_scores, color=colors)\n",
        "plt.title('Parameter Efficiency (Acc/1K Params)')\n",
        "plt.ylabel('Efficiency Score')\n",
        "\n",
        "# Convergence analysis\n",
        "plt.subplot(3, 4, 11)\n",
        "convergence_epochs = []\n",
        "for name, history in training_histories.items():\n",
        "    val_accs = history['val_accuracy']\n",
        "    # Find epoch where model reaches 90% of final accuracy\n",
        "    final_acc = val_accs[-1]\n",
        "    target_acc = 0.9 * final_acc\n",
        "    converged_epoch = next((i for i, acc in enumerate(val_accs) if acc >= target_acc), len(val_accs))\n",
        "    convergence_epochs.append(converged_epoch + 1)\n",
        "\n",
        "plt.bar(model_names, convergence_epochs, color=colors)\n",
        "plt.title('Convergence Speed (Epochs to 90% Final Acc)')\n",
        "plt.ylabel('Epochs')\n",
        "\n",
        "# Performance summary table\n",
        "plt.subplot(3, 4, 12)\n",
        "plt.axis('off')\n",
        "summary_data = []\n",
        "for i, name in enumerate(model_names):\n",
        "    result = model_results[name]\n",
        "    summary_data.append([\n",
        "        name,\n",
        "        f\"{result['test_accuracy']:.3f}\",\n",
        "        f\"{result['test_loss']:.3f}\",\n",
        "        f\"{training_epochs[i]}\",\n",
        "        f\"{param_counts[i]:,}\"\n",
        "    ])\n",
        "\n",
        "table = plt.table(cellText=summary_data,\n",
        "                 colLabels=['Model', 'Accuracy', 'Loss', 'Epochs', 'Parameters'],\n",
        "                 cellLoc='center',\n",
        "                 loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(9)\n",
        "table.scale(1.2, 1.5)\n",
        "plt.title('Performance Summary')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Sample predictions analysis\n",
        "print(f\"\\nSample Predictions from Best Model ({best_model}):\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Show some examples\n",
        "sample_indices = np.random.choice(len(X_test_text), 5, replace=False)\n",
        "\n",
        "for i, idx in enumerate(sample_indices):\n",
        "    text = X_test_text[idx]\n",
        "    true_label = label_encoder.classes_[y_test[idx]]\n",
        "    pred_label = label_encoder.classes_[best_result['y_pred'][idx]]\n",
        "    confidence = np.max(best_result['y_pred_proba'][idx])\n",
        "    \n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Text: '{text[:80]}...'\")\n",
        "    print(f\"True: {true_label}, Predicted: {pred_label}\")\n",
        "    print(f\"Confidence: {confidence:.3f}\")\n",
        "    print(f\"Correct: {'✓' if true_label == pred_label else '✗'}\")\n",
        "\n",
        "print(f\"\\nRNN-based Sentiment Classification Complete!\")\n",
        "print(f\"Best model achieved {best_accuracy:.4f} accuracy\")\n",
        "print(f\"Ready for advanced techniques and attention mechanisms!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
