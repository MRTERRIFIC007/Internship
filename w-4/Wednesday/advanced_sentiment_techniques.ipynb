{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Advanced Sentiment Analysis Techniques\n",
        "\n",
        "## Learning Objectives\n",
        "In this notebook, we'll explore advanced techniques for sentiment analysis:\n",
        "- Attention mechanisms in RNNs for sentiment analysis\n",
        "- Hierarchical RNN architectures for document-level sentiment\n",
        "- Multi-class and fine-grained sentiment classification\n",
        "- Aspect-based sentiment analysis\n",
        "- Model interpretability and attention visualization\n",
        "- Transfer learning for sentiment analysis\n",
        "\n",
        "## Introduction to Advanced Techniques\n",
        "\n",
        "While basic RNNs can capture sequential information, advanced techniques help us:\n",
        "- Focus on important parts of the text (attention)\n",
        "- Handle longer documents effectively (hierarchical models)\n",
        "- Understand specific aspects of sentiment (aspect-based analysis)\n",
        "- Interpret model decisions (attention visualization)\n",
        "\n",
        "These techniques are crucial for real-world sentiment analysis applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Advanced sentiment analysis techniques ready!\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Attention Mechanism Implementation\n",
        "class AttentionLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom attention layer for RNN-based sentiment analysis\n",
        "    \"\"\"\n",
        "    def __init__(self, units=64, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        # Attention weights\n",
        "        self.W_a = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True,\n",
        "            name='attention_weights'\n",
        "        )\n",
        "        self.b_a = self.add_weight(\n",
        "            shape=(self.units,),\n",
        "            initializer='zeros',\n",
        "            trainable=True,\n",
        "            name='attention_bias'\n",
        "        )\n",
        "        self.v_a = self.add_weight(\n",
        "            shape=(self.units, 1),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True,\n",
        "            name='attention_context'\n",
        "        )\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        # inputs shape: (batch_size, time_steps, features)\n",
        "        \n",
        "        # Calculate attention scores\n",
        "        score = tf.nn.tanh(tf.tensordot(inputs, self.W_a, axes=1) + self.b_a)\n",
        "        attention_weights = tf.nn.softmax(tf.tensordot(score, self.v_a, axes=1), axis=1)\n",
        "        \n",
        "        # Apply attention weights\n",
        "        context_vector = attention_weights * inputs\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        return context_vector, attention_weights\n",
        "    \n",
        "    def get_config(self):\n",
        "        config = super(AttentionLayer, self).get_config()\n",
        "        config.update({'units': self.units})\n",
        "        return config\n",
        "\n",
        "# 2. Hierarchical Attention Network\n",
        "def create_hierarchical_attention_model(vocab_size, max_length, num_classes, embedding_dim=64):\n",
        "    \"\"\"\n",
        "    Create a hierarchical attention network for document-level sentiment\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=(max_length,))\n",
        "    \n",
        "    # Embedding layer\n",
        "    embedding = layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs)\n",
        "    \n",
        "    # Word-level encoder\n",
        "    word_encoder = layers.Bidirectional(\n",
        "        layers.LSTM(32, return_sequences=True, dropout=0.2)\n",
        "    )(embedding)\n",
        "    \n",
        "    # Word-level attention\n",
        "    word_context, word_attention = AttentionLayer(32)(word_encoder)\n",
        "    \n",
        "    # Sentence-level processing (simplified for our use case)\n",
        "    sentence_encoder = layers.Dense(64, activation='relu')(word_context)\n",
        "    sentence_encoder = layers.Dropout(0.3)(sentence_encoder)\n",
        "    \n",
        "    # Final classification\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(sentence_encoder)\n",
        "    \n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name='HierarchicalAttention')\n",
        "    return model\n",
        "\n",
        "# 3. Attention-based LSTM with interpretability\n",
        "def create_attention_lstm(vocab_size, max_length, num_classes, embedding_dim=64, lstm_units=64):\n",
        "    \"\"\"\n",
        "    Create LSTM with attention mechanism for interpretability\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=(max_length,))\n",
        "    \n",
        "    # Embedding\n",
        "    embedding = layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs)\n",
        "    \n",
        "    # LSTM encoder\n",
        "    lstm_output = layers.LSTM(lstm_units, return_sequences=True, dropout=0.2)(embedding)\n",
        "    \n",
        "    # Attention mechanism\n",
        "    attention_output, attention_weights = AttentionLayer(lstm_units)(lstm_output)\n",
        "    \n",
        "    # Classification head\n",
        "    dense = layers.Dense(32, activation='relu')(attention_output)\n",
        "    dropout = layers.Dropout(0.3)(dense)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(dropout)\n",
        "    \n",
        "    # Create model that also returns attention weights for interpretability\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name='AttentionLSTM')\n",
        "    \n",
        "    # Create interpretation model\n",
        "    interpretation_model = keras.Model(\n",
        "        inputs=inputs, \n",
        "        outputs=[outputs, attention_weights], \n",
        "        name='AttentionLSTM_Interpretable'\n",
        "    )\n",
        "    \n",
        "    return model, interpretation_model\n",
        "\n",
        "# Create sample data for demonstration\n",
        "def create_extended_sentiment_data():\n",
        "    \"\"\"\n",
        "    Create extended sentiment dataset with more nuanced examples\n",
        "    \"\"\"\n",
        "    positive_texts = [\n",
        "        \"This product is absolutely fantastic! I love everything about it.\",\n",
        "        \"Outstanding quality and exceptional customer service. Highly recommended!\",\n",
        "        \"Perfect solution for my needs. Couldn't be happier with this purchase.\",\n",
        "        \"Amazing value for money. The features are incredible and it works flawlessly.\",\n",
        "        \"Exceptional product quality. Fast shipping and excellent packaging.\",\n",
        "        \"Love the innovative design and thoughtful features. Simply perfect!\",\n",
        "        \"Outstanding performance and reliability. Exceeds all expectations.\",\n",
        "        \"Brilliant product with amazing attention to detail. Highly satisfied!\"\n",
        "    ]\n",
        "    \n",
        "    negative_texts = [\n",
        "        \"Terrible product quality. Complete waste of money and disappointing experience.\",\n",
        "        \"Poor construction and unreliable performance. Avoid this product at all costs.\",\n",
        "        \"Awful customer service and defective product. Very frustrated with this purchase.\",\n",
        "        \"Overpriced and poor quality. Nothing works as advertised. Total disappointment.\",\n",
        "        \"Horrible experience from start to finish. Product broke immediately.\",\n",
        "        \"Worst purchase ever made. Cheap materials and terrible design flaws.\",\n",
        "        \"Completely unreliable and poorly designed. Save your money and look elsewhere.\",\n",
        "        \"Frustrating experience with poor quality and misleading product description.\"\n",
        "    ]\n",
        "    \n",
        "    neutral_texts = [\n",
        "        \"Average product with standard features. Does what it's supposed to do.\",\n",
        "        \"Decent quality for the price range. Nothing extraordinary but functional.\",\n",
        "        \"Standard product with typical performance. Meets basic requirements adequately.\",\n",
        "        \"Regular quality item with expected functionality. No major complaints.\",\n",
        "        \"Ordinary product with basic design. Works as described without surprises.\",\n",
        "        \"Acceptable quality and performance. Could be better but serves its purpose.\",\n",
        "        \"Standard features and regular build quality. Typical for this price range.\",\n",
        "        \"Basic product that meets minimum requirements. Average in most aspects.\"\n",
        "    ]\n",
        "    \n",
        "    # Create fine-grained sentiment examples\n",
        "    very_positive = [\n",
        "        \"Absolutely incredible! This is the best product I've ever purchased. Mind-blowing quality!\",\n",
        "        \"Phenomenal experience! Everything is perfect and beyond my wildest expectations!\"\n",
        "    ]\n",
        "    \n",
        "    very_negative = [\n",
        "        \"Absolutely horrible! This is the worst product imaginable. Complete disaster!\",\n",
        "        \"Catastrophically bad! Nothing works and it's a complete waste of money!\"\n",
        "    ]\n",
        "    \n",
        "    texts = positive_texts + negative_texts + neutral_texts + very_positive + very_negative\n",
        "    labels = (['positive'] * len(positive_texts) + \n",
        "              ['negative'] * len(negative_texts) + \n",
        "              ['neutral'] * len(neutral_texts) + \n",
        "              ['very_positive'] * len(very_positive) +\n",
        "              ['very_negative'] * len(very_negative))\n",
        "    \n",
        "    return pd.DataFrame({'text': texts, 'sentiment': labels})\n",
        "\n",
        "# Create extended dataset\n",
        "extended_df = create_extended_sentiment_data()\n",
        "print(f\"Extended dataset created with {len(extended_df)} samples\")\n",
        "print(f\"Classes: {extended_df['sentiment'].unique()}\")\n",
        "print(f\"Class distribution:\\n{extended_df['sentiment'].value_counts()}\")\n",
        "\n",
        "# Demonstrate attention mechanism concepts\n",
        "print(f\"\\nAttention Mechanism Concepts:\")\n",
        "print(\"=\" * 40)\n",
        "print(\"1. Attention helps models focus on relevant words\")\n",
        "print(\"2. Different words get different importance weights\")\n",
        "print(\"3. Attention weights sum to 1.0 across sequence\")\n",
        "print(\"4. Higher weights indicate more important words for prediction\")\n",
        "print(\"5. Attention provides interpretability to RNN decisions\")\n",
        "\n",
        "# Visualize attention concept\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Simulated attention weights for different examples\n",
        "examples = [\n",
        "    \"This product is absolutely fantastic and amazing\",\n",
        "    \"Poor quality and terrible customer service experience\", \n",
        "    \"Average product with standard basic features only\",\n",
        "    \"Incredible quality but expensive for the value\"\n",
        "]\n",
        "\n",
        "sentiments = [\"Positive\", \"Negative\", \"Neutral\", \"Mixed\"]\n",
        "colors = ['green', 'red', 'blue', 'orange']\n",
        "\n",
        "for i, (example, sentiment, color) in enumerate(zip(examples, sentiments, colors)):\n",
        "    ax = axes[i//2, i%2]\n",
        "    words = example.split()\n",
        "    \n",
        "    # Simulate attention weights (in real model, these come from attention layer)\n",
        "    if sentiment == \"Positive\":\n",
        "        weights = [0.05, 0.1, 0.05, 0.4, 0.3, 0.05, 0.05]\n",
        "    elif sentiment == \"Negative\":\n",
        "        weights = [0.35, 0.1, 0.05, 0.4, 0.05, 0.025, 0.025]\n",
        "    elif sentiment == \"Neutral\":\n",
        "        weights = [0.15, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15]\n",
        "    else:  # Mixed\n",
        "        weights = [0.3, 0.1, 0.05, 0.3, 0.05, 0.1, 0.1]\n",
        "    \n",
        "    # Ensure weights sum to 1\n",
        "    weights = np.array(weights)\n",
        "    weights = weights / weights.sum()\n",
        "    \n",
        "    # Create attention visualization\n",
        "    bars = ax.bar(range(len(words)), weights, color=color, alpha=0.7)\n",
        "    ax.set_xticks(range(len(words)))\n",
        "    ax.set_xticklabels(words, rotation=45, ha='right')\n",
        "    ax.set_ylabel('Attention Weight')\n",
        "    ax.set_title(f'{sentiment} Sentiment\\n\"{example}\"')\n",
        "    ax.set_ylim(0, max(weights) * 1.2)\n",
        "    \n",
        "    # Add weight labels\n",
        "    for bar, weight in zip(bars, weights):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{weight:.2f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nAdvanced sentiment analysis techniques initialized!\")\n",
        "print(f\"Ready to implement attention mechanisms and hierarchical models!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
