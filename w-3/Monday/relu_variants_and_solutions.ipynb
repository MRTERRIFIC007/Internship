{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ReLU Variants and Solutions\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the dying ReLU problem and its causes\n",
        "- Implement and compare Leaky ReLU solutions\n",
        "- Explore Parametric ReLU (PReLU) and its benefits\n",
        "- Develop best practices for ReLU usage in deep networks\n",
        "\n",
        "## Session 4: 16:00-17:30 - Solving the ReLU Problem\n",
        "\n",
        "ReLU revolutionized deep learning, but it's not perfect. Let's explore its limitations and the elegant solutions that have emerged to address them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential imports for ReLU analysis\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"ðŸš€ Ready to solve the ReLU problem!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. The Dying ReLU Problem\n",
        "\n",
        "**What is it?**\n",
        "The dying ReLU problem occurs when neurons get stuck in a state where they always output zero, effectively becoming \"dead\" and contributing nothing to learning.\n",
        "\n",
        "**Why does it happen?**\n",
        "- ReLU: f(x) = max(0, x)\n",
        "- Derivative: f'(x) = 1 if x > 0, else 0\n",
        "- If a neuron's input becomes negative and stays negative, its gradient becomes zero\n",
        "- With zero gradients, the neuron can never recover!\n",
        "\n",
        "**Mathematical Example:**\n",
        "If weights W and bias b result in Wx + b < 0 for all training examples, then:\n",
        "- Output = 0 (always)\n",
        "- Gradient = 0 (always)\n",
        "- Weight updates = 0 (neuron is dead!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implement ReLU and its variants\n",
        "def relu_activation(x):\n",
        "    \"\"\"Standard ReLU: f(x) = max(0, x)\"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    \"\"\"ReLU derivative: f'(x) = 1 if x > 0, else 0\"\"\"\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    \"\"\"Leaky ReLU: f(x) = x if x > 0, else Î±x\"\"\"\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "def leaky_relu_derivative(x, alpha=0.01):\n",
        "    \"\"\"Leaky ReLU derivative: f'(x) = 1 if x > 0, else Î±\"\"\"\n",
        "    return np.where(x > 0, 1.0, alpha)\n",
        "\n",
        "def parametric_relu(x, alpha):\n",
        "    \"\"\"Parametric ReLU: f(x) = x if x > 0, else Î±x (Î± is learnable)\"\"\"\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "def parametric_relu_derivative(x, alpha):\n",
        "    \"\"\"PReLU derivative: f'(x) = 1 if x > 0, else Î±\"\"\"\n",
        "    return np.where(x > 0, 1.0, alpha)\n",
        "\n",
        "def elu_activation(x, alpha=1.0):\n",
        "    \"\"\"ELU: f(x) = x if x > 0, else Î±(e^x - 1)\"\"\"\n",
        "    return np.where(x > 0, x, alpha * (np.exp(np.clip(x, -500, 500)) - 1))\n",
        "\n",
        "def elu_derivative(x, alpha=1.0):\n",
        "    \"\"\"ELU derivative: f'(x) = 1 if x > 0, else Î±*e^x\"\"\"\n",
        "    return np.where(x > 0, 1.0, alpha * np.exp(np.clip(x, -500, 500)))\n",
        "\n",
        "# Demonstrate the dying ReLU problem\n",
        "def simulate_dying_relu():\n",
        "    \"\"\"\n",
        "    Simulate how neurons can 'die' with ReLU\n",
        "    \"\"\"\n",
        "    # Create a scenario where ReLU neurons die\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Initial weights and biases that push neurons negative\n",
        "    weights = np.random.normal(-2, 0.5, (100, 10))  # Negative bias\n",
        "    biases = np.random.normal(-1, 0.2, 100)\n",
        "    \n",
        "    # Sample inputs\n",
        "    inputs = np.random.normal(0, 1, (1000, 10))\n",
        "    \n",
        "    dead_neurons = []\n",
        "    \n",
        "    for epoch in range(10):\n",
        "        # Forward pass\n",
        "        z = inputs @ weights.T + biases\n",
        "        activations = relu_activation(z)\n",
        "        \n",
        "        # Count dead neurons (always output 0)\n",
        "        dead_count = np.sum(np.all(activations == 0, axis=0))\n",
        "        dead_neurons.append(dead_count)\n",
        "        \n",
        "        # Simulate some learning (but dead neurons won't recover)\n",
        "        # In real training, dead neurons get zero gradients\n",
        "        gradients = relu_derivative(z)\n",
        "        alive_mask = np.any(gradients > 0, axis=0)\n",
        "        \n",
        "        # Only alive neurons get updated\n",
        "        weights[alive_mask] += np.random.normal(0, 0.01, (np.sum(alive_mask), 10))\n",
        "    \n",
        "    return dead_neurons\n",
        "\n",
        "# Run dying ReLU simulation\n",
        "dead_neuron_counts = simulate_dying_relu()\n",
        "\n",
        "print(\"ðŸ’€ Dying ReLU Simulation:\")\n",
        "print(\"=\" * 40)\n",
        "for epoch, count in enumerate(dead_neuron_counts):\n",
        "    print(f\"Epoch {epoch}: {count}/100 neurons are dead\")\n",
        "\n",
        "# Visualize ReLU variants\n",
        "x = np.linspace(-3, 3, 1000)\n",
        "\n",
        "# Calculate activations for different variants\n",
        "relu_vals = relu_activation(x)\n",
        "leaky_vals = leaky_relu(x, alpha=0.1)\n",
        "prelu_vals = parametric_relu(x, alpha=0.2)\n",
        "elu_vals = elu_activation(x, alpha=1.0)\n",
        "\n",
        "# Calculate derivatives\n",
        "relu_deriv = relu_derivative(x)\n",
        "leaky_deriv = leaky_relu_derivative(x, alpha=0.1)\n",
        "prelu_deriv = parametric_relu_derivative(x, alpha=0.2)\n",
        "elu_deriv = elu_derivative(x, alpha=1.0)\n",
        "\n",
        "# Create visualization\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('ReLU Variants: Functions and Derivatives', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot activation functions\n",
        "ax1.plot(x, relu_vals, label='ReLU', linewidth=2, color='red')\n",
        "ax1.plot(x, leaky_vals, label='Leaky ReLU (Î±=0.1)', linewidth=2, color='blue')\n",
        "ax1.plot(x, prelu_vals, label='PReLU (Î±=0.2)', linewidth=2, color='green')\n",
        "ax1.plot(x, elu_vals, label='ELU (Î±=1.0)', linewidth=2, color='purple')\n",
        "ax1.set_title('Activation Functions')\n",
        "ax1.set_xlabel('Input (x)')\n",
        "ax1.set_ylabel('Output f(x)')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "ax1.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
        "\n",
        "# Plot derivatives\n",
        "ax2.plot(x, relu_deriv, label=\"ReLU f'(x)\", linewidth=2, color='red')\n",
        "ax2.plot(x, leaky_deriv, label=\"Leaky ReLU f'(x)\", linewidth=2, color='blue')\n",
        "ax2.plot(x, prelu_deriv, label=\"PReLU f'(x)\", linewidth=2, color='green')\n",
        "ax2.plot(x, elu_deriv, label=\"ELU f'(x)\", linewidth=2, color='purple')\n",
        "ax2.set_title('Derivatives')\n",
        "ax2.set_xlabel('Input (x)')\n",
        "ax2.set_ylabel(\"Derivative f'(x)\")\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "ax2.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
        "\n",
        "# Plot dying ReLU progression\n",
        "ax3.plot(range(len(dead_neuron_counts)), dead_neuron_counts, 'ro-', linewidth=2)\n",
        "ax3.set_title('Dying ReLU Problem Progression')\n",
        "ax3.set_xlabel('Training Epoch')\n",
        "ax3.set_ylabel('Number of Dead Neurons')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Compare negative region behavior\n",
        "x_neg = np.linspace(-2, 0, 100)\n",
        "ax4.plot(x_neg, relu_activation(x_neg), label='ReLU (dies)', linewidth=3, color='red')\n",
        "ax4.plot(x_neg, leaky_relu(x_neg, 0.1), label='Leaky ReLU (survives)', linewidth=3, color='blue')\n",
        "ax4.plot(x_neg, elu_activation(x_neg), label='ELU (smooth)', linewidth=3, color='purple')\n",
        "ax4.set_title('Negative Input Behavior')\n",
        "ax4.set_xlabel('Negative Input')\n",
        "ax4.set_ylabel('Output')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Key Insights:\")\n",
        "print(\"â€¢ ReLU has zero gradient for negative inputs â†’ neurons can die\")\n",
        "print(\"â€¢ Leaky ReLU provides small gradient for negative inputs\")\n",
        "print(\"â€¢ PReLU learns the optimal slope for negative region\")\n",
        "print(\"â€¢ ELU provides smooth transition and non-zero gradients\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Practical Implementation in TensorFlow\n",
        "\n",
        "Let's implement these ReLU variants in TensorFlow and compare their performance on a real task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a challenging dataset that might cause dying ReLU\n",
        "X, y = make_classification(\n",
        "    n_samples=2000,\n",
        "    n_features=20,\n",
        "    n_informative=15,\n",
        "    n_redundant=5,\n",
        "    n_classes=2,\n",
        "    random_state=42,\n",
        "    flip_y=0.1  # Add some noise\n",
        ")\n",
        "\n",
        "# Split and scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Custom PReLU layer\n",
        "class PReLU(keras.layers.Layer):\n",
        "    def __init__(self, alpha_initializer='zeros', **kwargs):\n",
        "        super(PReLU, self).__init__(**kwargs)\n",
        "        self.alpha_initializer = alpha_initializer\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.alpha = self.add_weight(\n",
        "            shape=(input_shape[-1],),\n",
        "            initializer=self.alpha_initializer,\n",
        "            trainable=True,\n",
        "            name='alpha'\n",
        "        )\n",
        "        super(PReLU, self).build(input_shape)\n",
        "    \n",
        "    def call(self, x):\n",
        "        return tf.where(x >= 0, x, self.alpha * x)\n",
        "\n",
        "# Create models with different ReLU variants\n",
        "def create_relu_model(activation_type, input_dim):\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Dense(64, input_dim=input_dim),\n",
        "    ])\n",
        "    \n",
        "    if activation_type == 'relu':\n",
        "        model.add(keras.layers.ReLU())\n",
        "    elif activation_type == 'leaky_relu':\n",
        "        model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "    elif activation_type == 'prelu':\n",
        "        model.add(PReLU())\n",
        "    elif activation_type == 'elu':\n",
        "        model.add(keras.layers.ELU())\n",
        "    \n",
        "    model.add(keras.layers.Dense(32))\n",
        "    \n",
        "    if activation_type == 'relu':\n",
        "        model.add(keras.layers.ReLU())\n",
        "    elif activation_type == 'leaky_relu':\n",
        "        model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "    elif activation_type == 'prelu':\n",
        "        model.add(PReLU())\n",
        "    elif activation_type == 'elu':\n",
        "        model.add(keras.layers.ELU())\n",
        "    \n",
        "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Train models with different ReLU variants\n",
        "relu_variants = ['relu', 'leaky_relu', 'prelu', 'elu']\n",
        "relu_results = {}\n",
        "\n",
        "print(\"ðŸ”¬ Comparing ReLU Variants:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for variant in relu_variants:\n",
        "    print(f\"Training with {variant}...\")\n",
        "    \n",
        "    model = create_relu_model(variant, X_train_scaled.shape[1])\n",
        "    \n",
        "    # Monitor dead neurons during training\n",
        "    class DeadNeuronCallback(keras.callbacks.Callback):\n",
        "        def __init__(self):\n",
        "            self.dead_neurons = []\n",
        "        \n",
        "        def on_epoch_end(self, epoch, logs=None):\n",
        "            # Get activations from first hidden layer\n",
        "            layer_output = keras.Model(\n",
        "                inputs=self.model.input,\n",
        "                outputs=self.model.layers[1].output  # After first activation\n",
        "            )\n",
        "            activations = layer_output.predict(X_train_scaled[:100], verbose=0)\n",
        "            \n",
        "            # Count neurons that are always zero\n",
        "            dead_count = np.sum(np.all(activations <= 0, axis=0))\n",
        "            self.dead_neurons.append(dead_count)\n",
        "    \n",
        "    dead_callback = DeadNeuronCallback()\n",
        "    \n",
        "    history = model.fit(\n",
        "        X_train_scaled, y_train,\n",
        "        validation_data=(X_test_scaled, y_test),\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        callbacks=[dead_callback],\n",
        "        verbose=0\n",
        "    )\n",
        "    \n",
        "    # Evaluate\n",
        "    train_loss, train_acc = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
        "    test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "    \n",
        "    relu_results[variant] = {\n",
        "        'train_accuracy': train_acc,\n",
        "        'test_accuracy': test_acc,\n",
        "        'dead_neurons': dead_callback.dead_neurons,\n",
        "        'history': history\n",
        "    }\n",
        "    \n",
        "    print(f\"  {variant:12} | Test Accuracy: {test_acc:.4f} | Final Dead Neurons: {dead_callback.dead_neurons[-1]}\")\n",
        "\n",
        "# Visualize results\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('ReLU Variants Performance Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Training curves\n",
        "for variant, results in relu_results.items():\n",
        "    history = results['history']\n",
        "    ax1.plot(history.history['val_accuracy'], label=f'{variant}', alpha=0.8)\n",
        "\n",
        "ax1.set_title('Validation Accuracy During Training')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Validation Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Dead neuron progression\n",
        "for variant, results in relu_results.items():\n",
        "    dead_neurons = results['dead_neurons']\n",
        "    ax2.plot(dead_neurons, label=f'{variant}', alpha=0.8)\n",
        "\n",
        "ax2.set_title('Dead Neurons During Training')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Number of Dead Neurons')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Final accuracy comparison\n",
        "variants = list(relu_results.keys())\n",
        "test_accs = [relu_results[v]['test_accuracy'] for v in variants]\n",
        "colors = ['red', 'blue', 'green', 'purple']\n",
        "\n",
        "bars = ax3.bar(variants, test_accs, color=colors, alpha=0.7)\n",
        "ax3.set_title('Final Test Accuracy')\n",
        "ax3.set_ylabel('Test Accuracy')\n",
        "ax3.set_ylim([0, 1])\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, test_accs):\n",
        "    height = bar.get_height()\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{acc:.4f}', ha='center', va='bottom')\n",
        "\n",
        "# Loss comparison\n",
        "train_losses = [relu_results[v]['history'].history['loss'][-1] for v in variants]\n",
        "val_losses = [relu_results[v]['history'].history['val_loss'][-1] for v in variants]\n",
        "\n",
        "x_pos = np.arange(len(variants))\n",
        "width = 0.35\n",
        "\n",
        "ax4.bar(x_pos - width/2, train_losses, width, label='Train Loss', alpha=0.7)\n",
        "ax4.bar(x_pos + width/2, val_losses, width, label='Val Loss', alpha=0.7)\n",
        "ax4.set_title('Final Loss Comparison')\n",
        "ax4.set_xlabel('ReLU Variant')\n",
        "ax4.set_ylabel('Loss')\n",
        "ax4.set_xticks(x_pos)\n",
        "ax4.set_xticklabels(variants)\n",
        "ax4.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸŽ¯ Key Findings:\")\n",
        "print(\"â€¢ Standard ReLU may suffer from dead neurons\")\n",
        "print(\"â€¢ Leaky ReLU prevents neuron death with minimal computation overhead\")\n",
        "print(\"â€¢ PReLU learns optimal negative slope but adds parameters\")\n",
        "print(\"â€¢ ELU provides smooth gradients but requires more computation\")\n",
        "print(\"\\nðŸ’¡ Practical Recommendation:\")\n",
        "print(\"Leaky ReLU offers the best balance of performance and simplicity!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Best Practices and Final Thoughts\n",
        "\n",
        "### When to Use Each ReLU Variant:\n",
        "\n",
        "1. **Standard ReLU**: Good default choice, fast computation, works well with proper initialization\n",
        "2. **Leaky ReLU**: Best general-purpose upgrade to ReLU, prevents dying neurons\n",
        "3. **PReLU**: When you have enough data and want to learn optimal negative slopes\n",
        "4. **ELU**: When you need smooth gradients and can afford extra computation\n",
        "\n",
        "### Personal Reflection:\n",
        "Understanding the dying ReLU problem has been enlightening. It's fascinating how such a simple mathematical issue (zero gradients for negative inputs) can completely derail the learning process. The elegant solutions developed by the community - from Leaky ReLU's simple fix to ELU's smooth curves - showcase the iterative nature of deep learning research.\n",
        "\n",
        "**Key Insight**: Sometimes the best solutions are the simplest ones. Leaky ReLU's tiny modification (0.01x instead of 0 for negative inputs) solves a major problem with minimal overhead.\n",
        "\n",
        "**Tomorrow's Connection**: These activation functions will be crucial when we build CNNs for computer vision. Understanding their gradient properties will help us design better architectures for image processing tasks.\n",
        "\n",
        "---\n",
        "\n",
        "**Session 4 Complete! ðŸŽ‰**\n",
        "*Ready for tomorrow's journey into computer vision and CNNs!*\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
