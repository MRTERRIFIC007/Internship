{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Gradients in Machine Learning\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand how activation functions impact gradient flow\n",
        "- Explore the vanishing gradient problem in depth\n",
        "- Analyze the exploding gradient problem\n",
        "- Implement mathematical solutions and visualizations\n",
        "\n",
        "## Session 3: 14:30-16:00 - The Mathematics Behind Learning\n",
        "\n",
        "Gradients are the backbone of neural network training. Understanding how they flow through networks with different activation functions is crucial for designing effective deep learning architectures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential imports for gradient analysis\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import seaborn as sns\n",
        "from scipy import optimize\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"üöÄ Ready to explore gradient dynamics!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Understanding Gradient Flow\n",
        "\n",
        "Gradients tell us how to update weights to minimize loss. In deep networks, gradients must flow backward through many layers via the chain rule:\n",
        "\n",
        "‚àÇL/‚àÇw‚ÇÅ = ‚àÇL/‚àÇa_n √ó ‚àÇa_n/‚àÇa_{n-1} √ó ... √ó ‚àÇa‚ÇÇ/‚àÇa‚ÇÅ √ó ‚àÇa‚ÇÅ/‚àÇw‚ÇÅ\n",
        "\n",
        "Where:\n",
        "- L is the loss function\n",
        "- a_i is the activation of layer i\n",
        "- w‚ÇÅ are the weights in the first layer\n",
        "\n",
        "The key insight: **Each term ‚àÇa_i/‚àÇa_{i-1} depends on the derivative of the activation function!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate gradient flow through a deep network\n",
        "def simulate_gradient_flow(activation_func, derivative_func, num_layers=10, input_gradient=1.0):\n",
        "    \"\"\"\n",
        "    Simulate how gradients flow through a deep network\n",
        "    \n",
        "    Args:\n",
        "        activation_func: Activation function\n",
        "        derivative_func: Derivative of activation function\n",
        "        num_layers: Number of layers to simulate\n",
        "        input_gradient: Initial gradient value\n",
        "    \n",
        "    Returns:\n",
        "        Array of gradient magnitudes at each layer\n",
        "    \"\"\"\n",
        "    gradients = [input_gradient]\n",
        "    current_gradient = input_gradient\n",
        "    \n",
        "    # Simulate typical activations in each layer\n",
        "    layer_activations = np.random.normal(0, 1, num_layers)\n",
        "    \n",
        "    for i in range(num_layers):\n",
        "        # Gradient of activation function at this layer\n",
        "        activation_derivative = derivative_func(layer_activations[i])\n",
        "        \n",
        "        # Apply chain rule: multiply by local gradient\n",
        "        current_gradient *= activation_derivative\n",
        "        gradients.append(current_gradient)\n",
        "    \n",
        "    return np.array(gradients)\n",
        "\n",
        "# Define common activation functions and their derivatives\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def sigmoid_activation(x):\n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    s = sigmoid_activation(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def tanh_activation(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x)**2\n",
        "\n",
        "# Simulate gradient flow for different activation functions\n",
        "activation_functions = {\n",
        "    'ReLU': (relu, relu_derivative),\n",
        "    'Sigmoid': (sigmoid_activation, sigmoid_derivative),\n",
        "    'Tanh': (tanh_activation, tanh_derivative)\n",
        "}\n",
        "\n",
        "print(\"üîç Gradient Flow Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Run simulation multiple times and average\n",
        "num_simulations = 100\n",
        "num_layers = 15\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "all_results = {}\n",
        "for name, (func, deriv) in activation_functions.items():\n",
        "    layer_gradients = []\n",
        "    \n",
        "    for sim in range(num_simulations):\n",
        "        gradients = simulate_gradient_flow(func, deriv, num_layers)\n",
        "        layer_gradients.append(gradients)\n",
        "    \n",
        "    # Average across simulations\n",
        "    avg_gradients = np.mean(layer_gradients, axis=0)\n",
        "    std_gradients = np.std(layer_gradients, axis=0)\n",
        "    all_results[name] = (avg_gradients, std_gradients)\n",
        "    \n",
        "    # Plot gradient magnitude vs layer depth\n",
        "    layers = range(len(avg_gradients))\n",
        "    ax1.plot(layers, np.abs(avg_gradients), label=f'{name}', linewidth=2, alpha=0.8)\n",
        "    ax1.fill_between(layers, \n",
        "                     np.abs(avg_gradients) - std_gradients, \n",
        "                     np.abs(avg_gradients) + std_gradients, \n",
        "                     alpha=0.2)\n",
        "\n",
        "ax1.set_xlabel('Layer Depth')\n",
        "ax1.set_ylabel('Gradient Magnitude')\n",
        "ax1.set_title('Gradient Flow Through Deep Networks')\n",
        "ax1.set_yscale('log')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot gradient ratio (current/initial)\n",
        "for name, (avg_gradients, std_gradients) in all_results.items():\n",
        "    gradient_ratios = avg_gradients / avg_gradients[0]\n",
        "    layers = range(len(gradient_ratios))\n",
        "    ax2.plot(layers, gradient_ratios, label=f'{name}', linewidth=2, alpha=0.8)\n",
        "\n",
        "ax2.set_xlabel('Layer Depth')\n",
        "ax2.set_ylabel('Gradient Ratio (Current/Initial)')\n",
        "ax2.set_title('Gradient Preservation Through Layers')\n",
        "ax2.axhline(y=1, color='red', linestyle='--', alpha=0.5, label='No change')\n",
        "ax2.axhline(y=0.1, color='orange', linestyle='--', alpha=0.5, label='90% loss')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print numerical results\n",
        "print(\"\\nGradient Analysis Results:\")\n",
        "for name, (avg_gradients, _) in all_results.items():\n",
        "    final_gradient = avg_gradients[-1]\n",
        "    initial_gradient = avg_gradients[0]\n",
        "    ratio = final_gradient / initial_gradient\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Initial gradient: {initial_gradient:.4f}\")\n",
        "    print(f\"  Final gradient: {final_gradient:.4f}\")\n",
        "    print(f\"  Ratio (final/initial): {ratio:.6f}\")\n",
        "    \n",
        "    if abs(ratio) < 0.01:\n",
        "        print(f\"  ‚ö†Ô∏è  Vanishing gradient problem detected!\")\n",
        "    elif abs(ratio) > 100:\n",
        "        print(f\"  üí• Exploding gradient problem detected!\")\n",
        "    else:\n",
        "        print(f\"  ‚úÖ Gradient flow is stable\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. The Vanishing Gradient Problem\n",
        "\n",
        "**The Problem:**\n",
        "As gradients propagate backward through many layers, they can become exponentially smaller, effectively stopping learning in early layers.\n",
        "\n",
        "**Mathematical Cause:**\n",
        "If activation derivatives are consistently < 1, then:\n",
        "‚àÇL/‚àÇw‚ÇÅ = ‚àÇL/‚àÇa_n √ó (‚àè·µ¢ ‚àÇa·µ¢/‚àÇa·µ¢‚Çã‚ÇÅ) √ó ‚àÇa‚ÇÅ/‚àÇw‚ÇÅ\n",
        "\n",
        "If each ‚àÇa·µ¢/‚àÇa·µ¢‚Çã‚ÇÅ ‚âà 0.25 (typical for sigmoid), then after 10 layers:\n",
        "Gradient ‚âà 0.25¬π‚Å∞ ‚âà 0.000001 √ó original\n",
        "\n",
        "**Common Causes:**\n",
        "- Sigmoid/Tanh activations (derivatives max at 0.25)\n",
        "- Deep networks (many multiplications)\n",
        "- Poor weight initialization\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Solutions and Best Practices\n",
        "\n",
        "### Modern Solutions to Gradient Problems:\n",
        "\n",
        "1. **Better Activations**: ReLU family (Leaky ReLU, ELU, Swish)\n",
        "2. **Normalization**: Batch Normalization, Layer Normalization\n",
        "3. **Skip Connections**: ResNet-style connections\n",
        "4. **Better Initialization**: Xavier/He initialization\n",
        "5. **Gradient Clipping**: For exploding gradients\n",
        "\n",
        "### Personal Insight:\n",
        "Understanding gradient flow has been crucial for my development as a deep learning practitioner. The elegance of backpropagation combined with the practical challenges of deep networks creates a fascinating engineering problem. Modern architectures like ResNet and Transformer aren't just clever tricks - they're principled solutions to fundamental mathematical constraints.\n",
        "\n",
        "**Key Takeaway**: Successful deep learning isn't just about having the right architecture; it's about ensuring effective gradient flow throughout the network.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
