{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Activation Functions Fundamentals\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the role of activation functions in neural networks\n",
        "- Implement and visualize identity, threshold/step, and Swish functions\n",
        "- Analyze mathematical properties and derivatives\n",
        "- Compare performance characteristics\n",
        "\n",
        "## Session 1: 10:00-11:30 - Deep Dive into Core Activation Functions\n",
        "\n",
        "Today we're exploring the mathematical foundations that make neural networks capable of learning complex patterns. Without activation functions, neural networks would be just linear transformations!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential imports for our activation function exploration\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"ðŸš€ Environment setup complete!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Identity Activation Function\n",
        "\n",
        "The identity function is the simplest activation function: f(x) = x\n",
        "\n",
        "**Mathematical Properties:**\n",
        "- Function: f(x) = x\n",
        "- Derivative: f'(x) = 1\n",
        "- Range: (-âˆž, +âˆž)\n",
        "- Linear function\n",
        "\n",
        "**Use Cases:**\n",
        "- Output layer in regression problems\n",
        "- When you want to preserve the input signal unchanged\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def identity_activation(x):\n",
        "    \"\"\"\n",
        "    Identity activation function: f(x) = x\n",
        "    \n",
        "    Args:\n",
        "        x: Input values (can be numpy array or scalar)\n",
        "    \n",
        "    Returns:\n",
        "        Same as input (identity transformation)\n",
        "    \"\"\"\n",
        "    return x\n",
        "\n",
        "def identity_derivative(x):\n",
        "    \"\"\"\n",
        "    Derivative of identity function: f'(x) = 1\n",
        "    \n",
        "    Args:\n",
        "        x: Input values\n",
        "    \n",
        "    Returns:\n",
        "        Array of ones with same shape as input\n",
        "    \"\"\"\n",
        "    return np.ones_like(x)\n",
        "\n",
        "# Test the identity function\n",
        "x_test = np.array([-5, -2, 0, 2, 5])\n",
        "y_identity = identity_activation(x_test)\n",
        "dy_identity = identity_derivative(x_test)\n",
        "\n",
        "print(\"Identity Function Test:\")\n",
        "print(f\"Input: {x_test}\")\n",
        "print(f\"Output: {y_identity}\")\n",
        "print(f\"Derivative: {dy_identity}\")\n",
        "print(f\"âœ… Identity function working correctly: {np.array_equal(x_test, y_identity)}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Threshold/Step Activation Function\n",
        "\n",
        "The step function outputs discrete values based on a threshold.\n",
        "\n",
        "**Mathematical Properties:**\n",
        "- Function: f(x) = 1 if x â‰¥ threshold, else 0\n",
        "- Derivative: 0 everywhere (undefined at threshold)\n",
        "- Range: {0, 1}\n",
        "- Non-differentiable at threshold\n",
        "\n",
        "**Historical Significance:**\n",
        "- Used in early perceptrons\n",
        "- Binary classification output\n",
        "- Not suitable for backpropagation due to zero gradients\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def step_activation(x, threshold=0.0):\n",
        "    \"\"\"\n",
        "    Step/Threshold activation function\n",
        "    \n",
        "    Args:\n",
        "        x: Input values\n",
        "        threshold: Threshold value (default: 0.0)\n",
        "    \n",
        "    Returns:\n",
        "        1 if x >= threshold, 0 otherwise\n",
        "    \"\"\"\n",
        "    return np.where(x >= threshold, 1.0, 0.0)\n",
        "\n",
        "def step_derivative(x, threshold=0.0):\n",
        "    \"\"\"\n",
        "    Derivative of step function (mostly zero, undefined at threshold)\n",
        "    \n",
        "    Args:\n",
        "        x: Input values\n",
        "        threshold: Threshold value\n",
        "    \n",
        "    Returns:\n",
        "        Zero everywhere (simplified for practical use)\n",
        "    \"\"\"\n",
        "    return np.zeros_like(x)\n",
        "\n",
        "# Test step function with different thresholds\n",
        "x_range = np.linspace(-5, 5, 11)\n",
        "thresholds = [0.0, 1.0, -1.0]\n",
        "\n",
        "print(\"Step Function Analysis:\")\n",
        "print(f\"Input range: {x_range}\")\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_step = step_activation(x_range, threshold)\n",
        "    print(f\"\\nThreshold = {threshold}:\")\n",
        "    print(f\"Output: {y_step}\")\n",
        "    print(f\"Number of activated neurons: {np.sum(y_step)}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Swish Activation Function\n",
        "\n",
        "Swish is a modern activation function: f(x) = x * sigmoid(Î²x)\n",
        "\n",
        "**Mathematical Properties:**\n",
        "- Function: f(x) = x * Ïƒ(Î²x) = x / (1 + e^(-Î²x))\n",
        "- Self-gated (output depends on input)\n",
        "- Smooth and differentiable\n",
        "- Î² is a learnable parameter (often Î²=1)\n",
        "\n",
        "**Advantages:**\n",
        "- Often outperforms ReLU in deep networks\n",
        "- Smooth function helps with optimization\n",
        "- Self-gating property\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Sigmoid function: Ïƒ(x) = 1 / (1 + e^(-x))\n",
        "    \n",
        "    Args:\n",
        "        x: Input values\n",
        "    \n",
        "    Returns:\n",
        "        Sigmoid transformed values\n",
        "    \"\"\"\n",
        "    # Clip x to prevent overflow\n",
        "    x_clipped = np.clip(x, -500, 500)\n",
        "    return 1.0 / (1.0 + np.exp(-x_clipped))\n",
        "\n",
        "def swish_activation(x, beta=1.0):\n",
        "    \"\"\"\n",
        "    Swish activation function: f(x) = x * sigmoid(Î²x)\n",
        "    \n",
        "    Args:\n",
        "        x: Input values\n",
        "        beta: Scaling parameter (default: 1.0)\n",
        "    \n",
        "    Returns:\n",
        "        Swish transformed values\n",
        "    \"\"\"\n",
        "    return x * sigmoid(beta * x)\n",
        "\n",
        "def swish_derivative(x, beta=1.0):\n",
        "    \"\"\"\n",
        "    Derivative of Swish function\n",
        "    f'(x) = sigmoid(Î²x) + x * sigmoid(Î²x) * (1 - sigmoid(Î²x)) * Î²\n",
        "    \n",
        "    Args:\n",
        "        x: Input values\n",
        "        beta: Scaling parameter\n",
        "    \n",
        "    Returns:\n",
        "        Derivative values\n",
        "    \"\"\"\n",
        "    sig = sigmoid(beta * x)\n",
        "    return sig + x * sig * (1 - sig) * beta\n",
        "\n",
        "# Test Swish with different beta values\n",
        "x_test = np.linspace(-5, 5, 100)\n",
        "beta_values = [0.5, 1.0, 2.0]\n",
        "\n",
        "print(\"Swish Function Analysis:\")\n",
        "print(f\"Testing with input range: [{x_test.min():.1f}, {x_test.max():.1f}]\")\n",
        "\n",
        "for beta in beta_values:\n",
        "    y_swish = swish_activation(x_test, beta)\n",
        "    dy_swish = swish_derivative(x_test, beta)\n",
        "    \n",
        "    print(f\"\\nBeta = {beta}:\")\n",
        "    print(f\"Output range: [{y_swish.min():.3f}, {y_swish.max():.3f}]\")\n",
        "    print(f\"Max derivative: {dy_swish.max():.3f}\")\n",
        "    print(f\"Min derivative: {dy_swish.min():.3f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Comprehensive Visualization\n",
        "\n",
        "Let's visualize all three activation functions and their derivatives to understand their behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualization\n",
        "x = np.linspace(-5, 5, 1000)\n",
        "\n",
        "# Calculate activation functions\n",
        "y_identity = identity_activation(x)\n",
        "y_step = step_activation(x, threshold=0)\n",
        "y_swish = swish_activation(x, beta=1.0)\n",
        "\n",
        "# Calculate derivatives\n",
        "dy_identity = identity_derivative(x)\n",
        "dy_step = step_derivative(x)\n",
        "dy_swish = swish_derivative(x, beta=1.0)\n",
        "\n",
        "# Create subplots\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Activation Functions Fundamentals: Functions and Derivatives', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot activation functions\n",
        "ax1.plot(x, y_identity, label='Identity: f(x) = x', linewidth=2, color='blue')\n",
        "ax1.plot(x, y_step, label='Step: f(x) = 1 if xâ‰¥0 else 0', linewidth=2, color='red')\n",
        "ax1.plot(x, y_swish, label='Swish: f(x) = x * Ïƒ(x)', linewidth=2, color='green')\n",
        "ax1.set_xlabel('Input (x)')\n",
        "ax1.set_ylabel('Output f(x)')\n",
        "ax1.set_title('Activation Functions Comparison')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "ax1.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
        "\n",
        "# Plot derivatives\n",
        "ax2.plot(x, dy_identity, label=\"Identity: f'(x) = 1\", linewidth=2, color='blue')\n",
        "ax2.plot(x, dy_step, label=\"Step: f'(x) = 0\", linewidth=2, color='red')\n",
        "ax2.plot(x, dy_swish, label=\"Swish: f'(x)\", linewidth=2, color='green')\n",
        "ax2.set_xlabel('Input (x)')\n",
        "ax2.set_ylabel(\"Derivative f'(x)\")\n",
        "ax2.set_title('Derivatives Comparison')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "ax2.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
        "\n",
        "# Swish with different beta values\n",
        "beta_values = [0.5, 1.0, 2.0, 5.0]\n",
        "colors = ['purple', 'green', 'orange', 'brown']\n",
        "for beta, color in zip(beta_values, colors):\n",
        "    y_swish_beta = swish_activation(x, beta)\n",
        "    ax3.plot(x, y_swish_beta, label=f'Î² = {beta}', linewidth=2, color=color)\n",
        "\n",
        "ax3.set_xlabel('Input (x)')\n",
        "ax3.set_ylabel('Swish(x, Î²)')\n",
        "ax3.set_title('Swish Function with Different Î² Values')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "ax3.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "ax3.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
        "\n",
        "# Step function with different thresholds\n",
        "thresholds = [-2, -1, 0, 1, 2]\n",
        "colors_step = ['red', 'orange', 'blue', 'green', 'purple']\n",
        "for threshold, color in zip(thresholds, colors_step):\n",
        "    y_step_thresh = step_activation(x, threshold)\n",
        "    ax4.plot(x, y_step_thresh, label=f'threshold = {threshold}', linewidth=2, color=color)\n",
        "\n",
        "ax4.set_xlabel('Input (x)')\n",
        "ax4.set_ylabel('Step(x, threshold)')\n",
        "ax4.set_title('Step Function with Different Thresholds')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "ax4.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "ax4.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ðŸ“Š Visualization complete! Key observations:\")\n",
        "print(\"â€¢ Identity: Linear, constant gradient (good for regression output)\")\n",
        "print(\"â€¢ Step: Discrete output, zero gradient (historical significance only)\")\n",
        "print(\"â€¢ Swish: Smooth, self-gated, better than ReLU in many cases\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Key Insights and Learning Reflection\n",
        "\n",
        "### What I've Learned Today:\n",
        "\n",
        "1. **Identity Function**: Simple but crucial for regression tasks. The constant gradient of 1 means perfect gradient flow, but lacks non-linearity.\n",
        "\n",
        "2. **Step Function**: Historically important but practically unusable in modern deep learning due to zero gradients everywhere.\n",
        "\n",
        "3. **Swish Function**: A modern, smooth activation that often outperforms ReLU. The self-gating property (x times sigmoid) makes it adaptive to input.\n",
        "\n",
        "### Mathematical Insights:\n",
        "- Gradient flow is crucial for learning\n",
        "- Smooth functions generally train better\n",
        "- Non-linearity enables complex pattern learning\n",
        "\n",
        "### Practical Takeaways:\n",
        "- Choose activation functions based on your specific task\n",
        "- Consider gradient properties when designing deep networks\n",
        "- Modern activations like Swish can provide better performance than traditional ones\n",
        "\n",
        "### Personal Note:\n",
        "The mathematical beauty of activation functions is fascinating. Each function has its place in the neural network ecosystem! Understanding these fundamentals will be crucial as we dive deeper into computer vision and more complex architectures later this week.\n",
        "\n",
        "**Next up:** Linear vs Non-linear Activation Functions - where we'll see why non-linearity is absolutely essential for deep learning!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
