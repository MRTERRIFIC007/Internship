# Week 3 - Monday: Activation Functions & Gradients in Deep Learning

## Daily Schedule & Learning Objectives

### Session 1: 10:00-11:30 - Activation Functions Fundamentals

**Topics Covered:**

- Identity activation function
- Threshold/Step functions
- Swish activation function
- Mathematical formulations and properties

### Session 2: 11:30-12:30 & 13:00-14:30 - Linear vs Non-linear Activation Functions

**Topics Covered:**

- Softmax activation function
- Role in enabling backpropagation
- Weight update mechanisms
- Comparison of linear and non-linear activations

### Session 3: 14:30-16:00 - Gradients in Machine Learning

**Topics Covered:**

- Impact of activation functions on gradients
- Vanishing gradient problem
- Exploding gradient problem
- Mathematical analysis and solutions

### Session 4: 16:00-17:30 - The Dying ReLU Problem

**Topics Covered:**

- Understanding the dying ReLU problem
- Leaky ReLU solution
- Parametric ReLU (PReLU)
- Best practices for ReLU usage

## Notebooks Overview

1. **`activation_functions_fundamentals.ipynb`** - Comprehensive exploration of different activation functions
2. **`linear_vs_nonlinear_activations.ipynb`** - Comparative analysis and backpropagation implementation
3. **`gradients_and_optimization.ipynb`** - Gradient problems and their solutions
4. **`relu_variants_and_solutions.ipynb`** - ReLU problem analysis and modern solutions

## Key Learning Outcomes

By the end of this day, you should understand:

- How different activation functions affect neural network behavior
- The mathematical foundations of gradient-based optimization
- Common problems in deep learning (vanishing/exploding gradients, dying ReLU)
- Modern solutions and best practices for activation function selection

## Mathematical Concepts Covered

- Derivative calculations for different activation functions
- Chain rule application in backpropagation
- Gradient flow analysis through deep networks
- Numerical stability considerations

## Practical Skills Developed

- Implementation of custom activation functions
- Visualization of gradient behavior
- Debugging gradient-related issues
- Performance comparison of different activation strategies

## Prerequisites

- Understanding of basic neural networks (Week 2)
- Calculus fundamentals (derivatives, chain rule)
- NumPy and matplotlib proficiency
- TensorFlow/Keras basics

## Resources & References

- Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville
- "Understanding the difficulty of training deep feedforward neural networks" - Glorot & Bengio
- TensorFlow documentation on activation functions
- PyTorch activation function implementations

## Personal Learning Notes

This week marks a significant step into deep learning theory. The mathematical rigor required here is much higher than previous weeks, but understanding these fundamentals is crucial for building effective deep neural networks. The practical implications of choosing the right activation function cannot be overstated.

---

_Week 3, Day 1 - Deep Learning Fundamentals_
_Focus: Mathematical foundations and practical implementation_
