{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Linear vs Non-linear Activation Functions\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the fundamental difference between linear and non-linear activations\n",
        "- Implement and analyze the Softmax activation function\n",
        "- Explore how activation functions enable backpropagation\n",
        "- Understand weight update mechanisms in neural networks\n",
        "\n",
        "## Session 2: 11:30-12:30 & 13:00-14:30 - Deep Dive into Linearity\n",
        "\n",
        "Why can't we just use linear functions everywhere? This session will answer that crucial question and show why non-linearity is the secret sauce of deep learning!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Essential imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.datasets import make_classification, make_circles, make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"ðŸš€ Environment ready for linear vs non-linear exploration!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. The Fundamental Problem with Linear Functions\n",
        "\n",
        "**Mathematical Truth:** A composition of linear functions is still linear!\n",
        "\n",
        "If we have multiple layers with only linear activations:\n",
        "- Layer 1: fâ‚(x) = Wâ‚x + bâ‚\n",
        "- Layer 2: fâ‚‚(x) = Wâ‚‚x + bâ‚‚\n",
        "- Combined: fâ‚‚(fâ‚(x)) = Wâ‚‚(Wâ‚x + bâ‚) + bâ‚‚ = (Wâ‚‚Wâ‚)x + (Wâ‚‚bâ‚ + bâ‚‚)\n",
        "\n",
        "This is equivalent to a single linear transformation! No matter how many layers we stack, we get the same representational power as a single layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstration: Linear composition is still linear\n",
        "def demonstrate_linear_composition():\n",
        "    \"\"\"\n",
        "    Prove that composing linear functions results in a linear function\n",
        "    \"\"\"\n",
        "    # Define two linear transformations\n",
        "    W1 = np.array([[2, 1], [0, 3]])\n",
        "    b1 = np.array([1, 2])\n",
        "    \n",
        "    W2 = np.array([[1, -1], [2, 0]])\n",
        "    b2 = np.array([0, 1])\n",
        "    \n",
        "    # Test input\n",
        "    x = np.array([1, 2])\n",
        "    \n",
        "    # Apply transformations sequentially\n",
        "    layer1_output = W1 @ x + b1\n",
        "    layer2_output = W2 @ layer1_output + b2\n",
        "    \n",
        "    print(\"ðŸ” Linear Composition Demonstration:\")\n",
        "    print(f\"Input x: {x}\")\n",
        "    print(f\"Layer 1 output: {layer1_output}\")\n",
        "    print(f\"Layer 2 output: {layer2_output}\")\n",
        "    \n",
        "    # Equivalent single transformation\n",
        "    W_combined = W2 @ W1\n",
        "    b_combined = W2 @ b1 + b2\n",
        "    single_output = W_combined @ x + b_combined\n",
        "    \n",
        "    print(f\"\\nEquivalent single transformation:\")\n",
        "    print(f\"Combined W: \\n{W_combined}\")\n",
        "    print(f\"Combined b: {b_combined}\")\n",
        "    print(f\"Single transformation output: {single_output}\")\n",
        "    \n",
        "    # Verify they're the same\n",
        "    print(f\"\\nâœ… Outputs are identical: {np.allclose(layer2_output, single_output)}\")\n",
        "    \n",
        "    return W_combined, b_combined\n",
        "\n",
        "W_combined, b_combined = demonstrate_linear_composition()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. The Power of Non-linearity\n",
        "\n",
        "**Key Insight:** Non-linear activation functions allow neural networks to learn complex patterns and approximate any continuous function (Universal Approximation Theorem).\n",
        "\n",
        "**Visual Intuition:**\n",
        "- Linear functions can only separate data with straight lines/planes\n",
        "- Non-linear functions can create curved decision boundaries\n",
        "- This enables learning of complex patterns like XOR, spirals, and real-world data distributions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets to demonstrate linear vs non-linear capabilities\n",
        "def create_demonstration_datasets():\n",
        "    \"\"\"\n",
        "    Create various datasets to show the power of non-linear activations\n",
        "    \"\"\"\n",
        "    # 1. Linearly separable data\n",
        "    X_linear, y_linear = make_classification(\n",
        "        n_samples=300, n_features=2, n_redundant=0, n_informative=2,\n",
        "        random_state=42, n_clusters_per_class=1, class_sep=2\n",
        "    )\n",
        "    \n",
        "    # 2. Non-linearly separable data (circles)\n",
        "    X_circles, y_circles = make_circles(n_samples=300, noise=0.1, factor=0.3, random_state=42)\n",
        "    \n",
        "    # 3. Non-linearly separable data (moons)\n",
        "    X_moons, y_moons = make_moons(n_samples=300, noise=0.15, random_state=42)\n",
        "    \n",
        "    # 4. XOR-like data (classic non-linear problem)\n",
        "    np.random.seed(42)\n",
        "    X_xor = np.random.randn(200, 2)\n",
        "    y_xor = ((X_xor[:, 0] > 0) & (X_xor[:, 1] > 0)) | ((X_xor[:, 0] < 0) & (X_xor[:, 1] < 0))\n",
        "    y_xor = y_xor.astype(int)\n",
        "    \n",
        "    return {\n",
        "        'linear': (X_linear, y_linear),\n",
        "        'circles': (X_circles, y_circles),\n",
        "        'moons': (X_moons, y_moons),\n",
        "        'xor': (X_xor, y_xor)\n",
        "    }\n",
        "\n",
        "# Create all datasets\n",
        "datasets = create_demonstration_datasets()\n",
        "\n",
        "# Visualize the datasets\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "fig.suptitle('Datasets: Linear vs Non-linear Separability', fontsize=16, fontweight='bold')\n",
        "\n",
        "titles = ['Linearly Separable', 'Circles (Non-linear)', 'Moons (Non-linear)', 'XOR (Non-linear)']\n",
        "dataset_names = ['linear', 'circles', 'moons', 'xor']\n",
        "\n",
        "for idx, (ax, title, name) in enumerate(zip(axes.flat, titles, dataset_names)):\n",
        "    X, y = datasets[name]\n",
        "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', alpha=0.7, s=30)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.colorbar(scatter, ax=ax)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ðŸ“Š Datasets created successfully!\")\n",
        "print(\"Notice how some datasets cannot be separated by a straight line...\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Softmax Activation Function\n",
        "\n",
        "The Softmax function is crucial for multi-class classification. It converts raw scores (logits) into probabilities.\n",
        "\n",
        "**Mathematical Definition:**\n",
        "For a vector z with K classes:\n",
        "Ïƒ(z_i) = e^(z_i) / Î£(j=1 to K) e^(z_j)\n",
        "\n",
        "**Key Properties:**\n",
        "- Output values sum to 1 (probability distribution)\n",
        "- Emphasizes the largest input (winner-takes-most)\n",
        "- Differentiable everywhere\n",
        "- Used in the output layer for classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Stable softmax implementation\n",
        "    \n",
        "    Args:\n",
        "        x: Input array (can be 1D or 2D)\n",
        "    \n",
        "    Returns:\n",
        "        Softmax probabilities\n",
        "    \"\"\"\n",
        "    # Subtract max for numerical stability\n",
        "    if x.ndim == 1:\n",
        "        x_stable = x - np.max(x)\n",
        "        exp_x = np.exp(x_stable)\n",
        "        return exp_x / np.sum(exp_x)\n",
        "    else:\n",
        "        x_stable = x - np.max(x, axis=1, keepdims=True)\n",
        "        exp_x = np.exp(x_stable)\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def softmax_derivative(x):\n",
        "    \"\"\"\n",
        "    Derivative of softmax function\n",
        "    \n",
        "    Args:\n",
        "        x: Input array\n",
        "    \n",
        "    Returns:\n",
        "        Jacobian matrix of softmax\n",
        "    \"\"\"\n",
        "    s = softmax(x)\n",
        "    return np.diag(s) - np.outer(s, s)\n",
        "\n",
        "# Test softmax with different inputs\n",
        "test_cases = [\n",
        "    np.array([1.0, 2.0, 3.0]),\n",
        "    np.array([0.0, 0.0, 0.0]),\n",
        "    np.array([1000.0, 1001.0, 1002.0]),  # Test numerical stability\n",
        "    np.array([-1000.0, -999.0, -998.0])  # Test negative large values\n",
        "]\n",
        "\n",
        "print(\"ðŸ§® Softmax Function Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, test_input in enumerate(test_cases):\n",
        "    softmax_output = softmax(test_input)\n",
        "    derivative_matrix = softmax_derivative(test_input)\n",
        "    \n",
        "    print(f\"\\nTest Case {i + 1}:\")\n",
        "    print(f\"Input: {test_input}\")\n",
        "    print(f\"Softmax output: {softmax_output}\")\n",
        "    print(f\"Sum of outputs: {np.sum(softmax_output):.6f}\")\n",
        "    print(f\"Max probability: {np.max(softmax_output):.6f}\")\n",
        "    print(f\"Predicted class: {np.argmax(softmax_output)}\")\n",
        "\n",
        "# Visualize softmax behavior\n",
        "x_range = np.linspace(-5, 5, 100)\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# 2D softmax visualization\n",
        "for offset in [-2, 0, 2]:\n",
        "    inputs = np.column_stack([x_range, np.full_like(x_range, offset)])\n",
        "    softmax_vals = softmax(inputs)\n",
        "    ax1.plot(x_range, softmax_vals[:, 0], label=f'Class 1 (offset={offset})', alpha=0.7)\n",
        "    ax1.plot(x_range, softmax_vals[:, 1], label=f'Class 2 (offset={offset})', linestyle='--', alpha=0.7)\n",
        "\n",
        "ax1.set_title('Softmax Behavior with Different Offsets')\n",
        "ax1.set_xlabel('Input Value for Class 1')\n",
        "ax1.set_ylabel('Probability')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Temperature effect on softmax\n",
        "def softmax_with_temperature(x, temperature=1.0):\n",
        "    return softmax(x / temperature)\n",
        "\n",
        "x_test = np.array([1.0, 2.0, 3.0, 4.0])\n",
        "temperatures = [0.5, 1.0, 2.0, 5.0]\n",
        "\n",
        "for temp in temperatures:\n",
        "    soft_temp = softmax_with_temperature(x_test, temp)\n",
        "    ax2.plot(range(len(x_test)), soft_temp, 'o-', label=f'T={temp}', alpha=0.7)\n",
        "\n",
        "ax2.set_title('Temperature Effect on Softmax')\n",
        "ax2.set_xlabel('Class Index')\n",
        "ax2.set_ylabel('Probability')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“‹ Key Observations:\")\n",
        "print(\"â€¢ Softmax outputs always sum to 1\")\n",
        "print(\"â€¢ Higher temperature â†’ more uniform distribution\")\n",
        "print(\"â€¢ Lower temperature â†’ more peaked distribution\")\n",
        "print(\"â€¢ Numerical stability is crucial for large inputs\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Practical Comparison: Linear vs Non-linear Networks\n",
        "\n",
        "Let's train networks with different activation functions on our datasets to see the dramatic difference in performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create models with different activation functions\n",
        "def create_model(activation, input_dim, hidden_units=32):\n",
        "    \"\"\"\n",
        "    Create a neural network with specified activation function\n",
        "    \"\"\"\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Dense(hidden_units, input_dim=input_dim),\n",
        "        keras.layers.Activation(activation),\n",
        "        keras.layers.Dense(hidden_units),\n",
        "        keras.layers.Activation(activation),\n",
        "        keras.layers.Dense(1, activation='sigmoid')  # Binary classification\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Train and evaluate models on different datasets\n",
        "def train_and_evaluate(X, y, dataset_name):\n",
        "    \"\"\"\n",
        "    Train models with different activations and compare performance\n",
        "    \"\"\"\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    \n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Test different activation functions\n",
        "    activations = ['linear', 'relu', 'tanh', 'sigmoid']\n",
        "    results = {}\n",
        "    \n",
        "    print(f\"\\nðŸ§ª Training on {dataset_name} dataset...\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for activation in activations:\n",
        "        print(f\"Training with {activation} activation...\")\n",
        "        \n",
        "        # Create and train model\n",
        "        model = create_model(activation, X_train_scaled.shape[1])\n",
        "        \n",
        "        # Train with early stopping to prevent overfitting\n",
        "        early_stopping = keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss', patience=20, restore_best_weights=True\n",
        "        )\n",
        "        \n",
        "        history = model.fit(\n",
        "            X_train_scaled, y_train,\n",
        "            validation_split=0.2,\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        # Evaluate\n",
        "        train_loss, train_acc = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
        "        test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "        \n",
        "        results[activation] = {\n",
        "            'train_accuracy': train_acc,\n",
        "            'test_accuracy': test_acc,\n",
        "            'train_loss': train_loss,\n",
        "            'test_loss': test_loss,\n",
        "            'history': history,\n",
        "            'model': model\n",
        "        }\n",
        "        \n",
        "        print(f\"  {activation:8} | Train: {train_acc:.4f} | Test: {test_acc:.4f}\")\n",
        "    \n",
        "    return results, X_test_scaled, y_test\n",
        "\n",
        "# Run experiments on all datasets\n",
        "all_results = {}\n",
        "for dataset_name, (X, y) in datasets.items():\n",
        "    results, X_test, y_test = train_and_evaluate(X, y, dataset_name)\n",
        "    all_results[dataset_name] = {\n",
        "        'results': results,\n",
        "        'X_test': X_test,\n",
        "        'y_test': y_test\n",
        "    }\n",
        "\n",
        "print(\"\\nðŸ† Final Results Summary:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Dataset':<12} | {'Linear':<8} | {'ReLU':<8} | {'Tanh':<8} | {'Sigmoid':<8}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for dataset_name, data in all_results.items():\n",
        "    results = data['results']\n",
        "    line = f\"{dataset_name:<12} |\"\n",
        "    for activation in ['linear', 'relu', 'tanh', 'sigmoid']:\n",
        "        acc = results[activation]['test_accuracy']\n",
        "        line += f\" {acc:6.4f} |\"\n",
        "    print(line)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Key Insights and Reflection\n",
        "\n",
        "### Critical Findings:\n",
        "\n",
        "1. **Linear Limitations**: Networks with only linear activations fail completely on non-linearly separable data (circles, moons, XOR)\n",
        "\n",
        "2. **Non-linear Power**: ReLU, Tanh, and Sigmoid all enable learning of complex patterns, but with different characteristics\n",
        "\n",
        "3. **Softmax Excellence**: Perfect for multi-class classification, converting raw scores to interpretable probabilities\n",
        "\n",
        "4. **Practical Impact**: The choice of activation function can make the difference between 50% (random) and 95%+ accuracy\n",
        "\n",
        "### Personal Learning Note:\n",
        "Today's experiments clearly demonstrate why the AI revolution happened when it did. The realization that stacking linear layers is pointless, and the introduction of effective non-linear activations like ReLU, was a crucial breakthrough. The mathematical elegance of these simple functions enabling such complex behavior is truly remarkable!\n",
        "\n",
        "**Tomorrow's Preview**: We'll dive into CNNs - where these activation functions meet spatial data processing for computer vision applications.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
